{"config":{"lang":["de","en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PyADM1ODE_Kalibrierung","text":"<p>Hinweis: Dies ist ein Framework f\u00fcr die Parameterkalibrierung, das nur zusammen mit PyADM1ODE funktioniert.</p> <p>Fortgeschrittenes Framework zur Parameterkalibrierung f\u00fcr PyADM1ODE Biogasanlagenmodelle</p> <p>Automatisierte Kalibrierung und Rekalibrierung von Parametern des Anaerobic Digestion Model No. 1 (ADM1) unter Verwendung von realen Messdaten mit mehreren Optimierungsalgorithmen, umfassender Validierung und Online-Anpassungsf\u00e4higkeiten.</p>"},{"location":"#ubersicht","title":"\u00dcbersicht","text":"<p>PyADM1ODE_calibration bietet ein vollst\u00e4ndiges Kalibrierungs-Framework f\u00fcr PyADM1ODE Biogasanlagenmodelle:</p> <ul> <li>Erstkalibrierung: Batch-Optimierung aus historischen Messdaten</li> <li>Online-Rekalibrierung: Echtzeit-Parameteranpassung w\u00e4hrend des Anlagenbetriebs</li> <li>Mehrere Optimierungsalgorithmen: Differential Evolution, Nelder-Mead, L-BFGS-B, Particle Swarm</li> <li>Multi-Objective Optimierung: Ausgleich mehrerer Ausg\u00e4nge (CH\u2084, pH, VFA) mit gewichteten Zielvorgaben</li> <li>Umfassende Validierung: G\u00fctekriterien, Residualanalyse, Kreuzvalidierung</li> <li>Parameter-Identifizierbarkeit: Sensitivit\u00e4tsanalyse und Korrelationserkennung</li> <li>Datenmanagement: CSV/Datenbank-Import, Validierung, Ausrei\u00dfererkennung, L\u00fcckenf\u00fcllung</li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#pyadm1ode_calibration.calibration.Calibrator","title":"<code>pyadm1ode_calibration.calibration.Calibrator</code>","text":"<p>Orchestration layer for calibration.</p> Source code in <code>pyadm1ode_calibration/calibration/__init__.py</code> <pre><code>class Calibrator:\n    \"\"\"Orchestration layer for calibration.\"\"\"\n\n    def __init__(self, plant, verbose: bool = True):\n        self.plant = plant\n        self.verbose = verbose\n        self.initial_calibrator = InitialCalibrator(plant, verbose)\n        self.online_calibrator = OnlineCalibrator(plant, verbose)\n\n    def run_initial_calibration(self, measurements, parameters, **kwargs):\n        return self.initial_calibrator.calibrate(measurements, parameters, **kwargs)\n\n    def run_online_calibration(self, measurements, parameters, **kwargs):\n        return self.online_calibrator.calibrate(measurements, parameters, **kwargs)\n\n    def apply_calibration(self, result: CalibrationResult):\n        self.online_calibrator.apply_calibration(result)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.InitialCalibrator","title":"<code>pyadm1ode_calibration.calibration.InitialCalibrator</code>","text":"<p>               Bases: <code>BaseCalibrator</code></p> <p>Initial calibrator for ADM1 parameters from historical data.</p> Source code in <code>pyadm1ode_calibration/calibration/methods/initial.py</code> <pre><code>class InitialCalibrator(BaseCalibrator):\n    \"\"\"Initial calibrator for ADM1 parameters from historical data.\"\"\"\n\n    def __init__(self, plant: Any, verbose: bool = True):\n        super().__init__(plant, verbose)\n        self.parameter_bounds = create_default_bounds()\n        self.validator = CalibrationValidator(plant, verbose=False)\n        self.sensitivity_analyzer = SensitivityAnalyzer(plant, self.simulator, verbose)\n        self.identifiability_analyzer = IdentifiabilityAnalyzer(plant, self.sensitivity_analyzer, verbose)\n        self._optimization_history: List[Dict[str, Any]] = []\n        self._best_objective_value: float = float(\"inf\")\n        self._original_parameters: Dict[str, float] = self._get_current_parameters()\n\n    def calibrate(\n        self,\n        measurements: MeasurementData,\n        parameters: List[str],\n        bounds: Optional[Dict[str, Tuple[float, float]]] = None,\n        method: str = \"differential_evolution\",\n        objectives: Optional[List[str]] = None,\n        weights: Optional[Dict[str, float]] = None,\n        validation_split: float = 0.2,\n        max_iterations: int = 100,\n        population_size: int = 15,\n        tolerance: float = 1e-4,\n        sensitivity_analysis: bool = True,\n        use_constraints: bool = False,\n        **kwargs: Any,\n    ) -&gt; CalibrationResult:\n        start_time = time.time()\n        if objectives is None:\n            objectives = [\"Q_ch4\"]\n\n        # Split data\n        train_data, val_data = self._split_data(measurements, validation_split)\n\n        initial_params = self.parameter_bounds.get_default_values(parameters)\n        param_bounds = self._setup_bounds(parameters, bounds)\n\n        # Create objective function\n        def simulator_wrapper(params: Dict[str, float]) -&gt; Dict[str, np.ndarray]:\n            return self.simulator.simulate_with_parameters(params, train_data)\n\n        measurements_dict: Dict[str, np.ndarray] = {}\n        for obj in objectives:\n            try:\n                measurements_dict[obj] = train_data.get_measurement(obj).values\n            except Exception:\n                continue\n\n        objective_func: Callable[[np.ndarray], float]\n        if weights is None:\n            objective_func = WeightedSumObjective(\n                simulator=simulator_wrapper,\n                measurements_dict=measurements_dict,\n                objectives=objectives,\n                parameter_names=parameters,\n                error_metric=\"rmse\",\n                normalize=True,\n            )\n        else:\n            objective_func = MultiObjectiveFunction(\n                simulator=simulator_wrapper,\n                measurements_dict=measurements_dict,\n                objectives=objectives,\n                weights=weights,\n                parameter_names=parameters,\n                error_metric=\"rmse\",\n                normalize=True,\n            )\n\n        # Constraints\n        obj_func_final: Callable[[np.ndarray], float]\n        if use_constraints:\n            constraints = ParameterConstraints()\n            for param, (lb, ub) in param_bounds.items():\n                constraints.add_box_constraint(param, lb, ub, hard=True)\n\n            def penalized_objective(x: np.ndarray) -&gt; float:\n                params = {name: val for name, val in zip(parameters, x)}\n                return objective_func(x) + constraints.calculate_penalty(params)\n\n            obj_func_final = penalized_objective\n        else:\n            obj_func_final = objective_func\n\n        optimizer_kwargs = {**kwargs}\n        optimizer_kwargs[\"tolerance\"] = tolerance\n        if method in [\"differential_evolution\", \"de\"]:\n            optimizer_kwargs[\"population_size\"] = population_size\n\n        optimizer = create_optimizer(\n            method=method,\n            bounds=param_bounds,\n            max_iterations=max_iterations,\n            verbose=self.verbose,\n            **optimizer_kwargs,\n        )\n\n        initial_guess = (\n            np.array([initial_params[p] for p in parameters]) if method in [\"nelder_mead\", \"nm\", \"lbfgsb\", \"powell\"] else None\n        )\n        opt_result = optimizer.optimize(obj_func_final, initial_guess=initial_guess)\n\n        # Validation\n        validation_metrics: Dict[str, float] = {}\n        if len(val_data) &gt; 0:\n            val_result = self.validator.validate(\n                parameters=opt_result.parameter_dict, measurements=val_data, objectives=objectives\n            )\n            for obj, metrics in val_result.items():\n                validation_metrics.update(\n                    {f\"{obj}_rmse\": float(metrics.rmse), f\"{obj}_r2\": float(metrics.r2), f\"{obj}_nse\": float(metrics.nse)}\n                )\n\n        # Sensitivity\n        sensitivity_results: Dict[str, float] = {}\n        if sensitivity_analysis and opt_result.success:\n            sens = self.sensitivity_analyzer.analyze(opt_result.parameter_dict, train_data, objectives)\n            sensitivity_results = {p: float(max(abs(s) for s in r.sensitivity_indices.values())) for p, r in sens.items()}\n\n        return CalibrationResult(\n            success=opt_result.success,\n            parameters=opt_result.parameter_dict,\n            initial_parameters=initial_params,\n            objective_value=float(opt_result.fun),\n            n_iterations=int(opt_result.nit),\n            execution_time=time.time() - start_time,\n            method=method,\n            message=str(opt_result.message) if hasattr(opt_result, \"message\") else \"Optimization completed\",\n            validation_metrics=validation_metrics,\n            sensitivity=sensitivity_results,\n            history=opt_result.history,\n        )\n\n    def sensitivity_analysis(\n        self, parameters: Dict[str, float], measurements: MeasurementData, objectives: Optional[List[str]] = None\n    ) -&gt; Dict[str, SensitivityResult]:\n        return self.sensitivity_analyzer.analyze(parameters, measurements, objectives)\n\n    def identifiability_analysis(\n        self, parameters: Dict[str, float], measurements: MeasurementData\n    ) -&gt; Dict[str, IdentifiabilityResult]:\n        return self.identifiability_analyzer.analyze(parameters, measurements)\n\n    def _split_data(self, measurements: MeasurementData, split_ratio: float) -&gt; Tuple[MeasurementData, MeasurementData]:\n        n_train = int(len(measurements) * (1 - split_ratio))\n        return (\n            MeasurementData(measurements.data.iloc[:n_train].copy(), metadata=measurements.metadata.copy()),\n            MeasurementData(measurements.data.iloc[n_train:].copy(), metadata=measurements.metadata.copy()),\n        )\n\n    def _setup_bounds(\n        self, parameters: List[str], custom_bounds: Optional[Dict[str, Tuple[float, float]]]\n    ) -&gt; Dict[str, Tuple[float, float]]:\n        bounds: Dict[str, Tuple[float, float]] = {}\n        for param in parameters:\n            if custom_bounds and param in custom_bounds:\n                bounds[param] = custom_bounds[param]\n            else:\n                b = self.parameter_bounds.get_bounds_tuple(param)\n                if b:\n                    bounds[param] = b\n                else:\n                    default = self.parameter_bounds.get_default_values([param])[param]\n                    bounds[param] = (default * 0.5, default * 1.5)\n        return bounds\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.OnlineCalibrator","title":"<code>pyadm1ode_calibration.calibration.OnlineCalibrator</code>","text":"<p>               Bases: <code>BaseCalibrator</code></p> <p>Online calibrator for real-time parameter adjustment.</p> <p>Performs fast, bounded re-calibration when model predictions deviate from measurements.</p> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>class OnlineCalibrator(BaseCalibrator):\n    \"\"\"Online calibrator for real-time parameter adjustment.\n\n    Performs fast, bounded re-calibration when model predictions deviate from\n    measurements.\n    \"\"\"\n\n    def __init__(self, plant: Any, verbose: bool = True, parameter_bounds: Optional[ParameterBounds] = None):\n        \"\"\"Initialize online calibrator.\n\n        Args:\n            plant: BiogasPlant instance\n            verbose: Enable progress output\n            parameter_bounds: Custom parameter bounds manager\n        \"\"\"\n        super().__init__(plant, verbose)\n        self.parameter_bounds: ParameterBounds = parameter_bounds or create_default_bounds()\n        self.validator: CalibrationValidator = CalibrationValidator(plant, verbose=False)\n        self.trigger: OnlineCalibrationTrigger = OnlineCalibrationTrigger()\n        self.state: OnlineState = OnlineState()\n\n    def calibrate(\n        self,\n        measurements: MeasurementData,\n        parameters: Optional[List[str]] = None,\n        current_parameters: Optional[Dict[str, float]] = None,\n        variance_threshold: float = 0.15,\n        max_parameter_change: float = 0.20,\n        time_window: int = 7,\n        method: str = \"nelder_mead\",\n        max_iterations: int = 50,\n        objectives: Optional[List[str]] = None,\n        weights: Optional[Dict[str, float]] = None,\n        use_constraints: bool = True,\n        **kwargs: Any,\n    ) -&gt; CalibrationResult:\n        \"\"\"Perform online re-calibration with bounded parameter adjustments.\n\n        Args:\n            measurements: Recent measurement data\n            parameters: Parameters to adjust\n            current_parameters: Current parameter values\n            variance_threshold: Variance threshold for triggering (0-1)\n            max_parameter_change: Maximum relative parameter change (0-1)\n            time_window: Days of recent data to use\n            method: Optimization method name\n            max_iterations: Max optimization iterations\n            objectives: List of outputs to match\n            weights: Objective weights\n            use_constraints: Whether to apply parameter constraints\n            **kwargs: Extra optimizer settings\n\n        Returns:\n            CalibrationResult instance\n        \"\"\"\n        start_time = time.time()\n        if objectives is None:\n            objectives = [\"Q_ch4\", \"pH\"]\n\n        if parameters is None:\n            if self.state.parameter_history:\n                parameters = list(self.state.parameter_history[-1].parameters.keys())\n            else:\n                raise ValueError(\"No parameters specified and no calibration history available\")\n\n        if current_parameters is None:\n            current_parameters = self._get_current_parameters()\n\n        windowed_data = self._extract_time_window(measurements, time_window)\n        current_variance = self._calculate_prediction_variance(windowed_data, current_parameters, objectives)\n        self.state.current_variance = current_variance\n\n        param_bounds = self._setup_online_bounds(parameters, current_parameters, max_parameter_change)\n\n        def simulator_wrapper(params: Dict[str, float]) -&gt; Dict[str, np.ndarray]:\n            return self.simulator.simulate_with_parameters(params, windowed_data)\n\n        measurements_dict: Dict[str, np.ndarray] = {\n            obj: windowed_data.get_measurement(obj).values for obj in objectives if obj in windowed_data.data.columns\n        }\n\n        objective_func: Callable[[np.ndarray], float] = MultiObjectiveFunction(\n            simulator=simulator_wrapper,\n            measurements_dict=measurements_dict,\n            objectives=objectives,\n            weights=weights or {obj: 1.0 / len(objectives) for obj in objectives},\n            parameter_names=parameters,\n            error_metric=\"rmse\",\n            normalize=True,\n        )\n\n        obj_func_final: Callable[[np.ndarray], float]\n        if use_constraints:\n            constraints = ParameterConstraints()\n            for p, (lb, ub) in param_bounds.items():\n                constraints.add_box_constraint(p, lb, ub, hard=True)\n\n            def penalized_objective(x: np.ndarray) -&gt; float:\n                params = {name: val for name, val in zip(parameters, x)}\n                return objective_func(x) + constraints.calculate_penalty(params)\n\n            obj_func_final = penalized_objective\n        else:\n            obj_func_final = objective_func\n\n        optimizer = create_optimizer(\n            method=method, bounds=param_bounds, max_iterations=max_iterations, verbose=self.verbose, **kwargs\n        )\n\n        initial_guess = np.array(\n            [current_parameters.get(p, self.parameter_bounds.get_default_values([p])[p]) for p in parameters]\n        )\n        opt_result = optimizer.optimize(obj_func_final, initial_guess=initial_guess)\n\n        validation_metrics: Dict[str, float] = {}\n        if opt_result.success:\n            val_res = self.validator.validate(\n                parameters=opt_result.parameter_dict, measurements=windowed_data, objectives=objectives\n            )\n            validation_metrics = {f\"{obj}_{k}\": float(getattr(m, k)) for obj, m in val_res.items() for k in [\"rmse\", \"r2\"]}\n\n        self.state.total_calibrations += 1\n        self.state.last_calibration_time = datetime.now()\n\n        history_entry = ParameterChangeHistory(\n            timestamp=datetime.now(),\n            parameters=opt_result.parameter_dict.copy(),\n            trigger_reason=\"variance_threshold\" if current_variance &gt; variance_threshold else \"manual\",\n            objective_value=float(opt_result.fun),\n            variance=float(current_variance),\n            success=bool(opt_result.success),\n        )\n        self.state.parameter_history.append(history_entry)\n\n        return CalibrationResult(\n            success=opt_result.success,\n            parameters=opt_result.parameter_dict,\n            initial_parameters=current_parameters,\n            objective_value=float(opt_result.fun),\n            n_iterations=int(opt_result.nit),\n            execution_time=time.time() - start_time,\n            method=method,\n            message=str(getattr(opt_result, \"message\", \"Online calibration completed\")),\n            validation_metrics=validation_metrics,\n        )\n\n    def should_recalibrate(\n        self, recent_measurements: MeasurementData, objectives: Optional[List[str]] = None\n    ) -&gt; Tuple[bool, str]:\n        \"\"\"Check if re-calibration should be triggered.\n\n        Returns:\n            Tuple of (should_recalibrate, reason)\n        \"\"\"\n        if not self.trigger.enabled:\n            return False, \"Disabled\"\n\n        if objectives is None:\n            objectives = [\"Q_ch4\", \"pH\"]\n\n        if self.state.last_calibration_time:\n            hours = (datetime.now() - self.state.last_calibration_time).total_seconds() / 3600\n            if hours &lt; self.trigger.time_threshold:\n                return False, f\"Too soon since last calibration ({hours:.1f}h)\"\n\n        variance = self._calculate_prediction_variance(recent_measurements, self._get_current_parameters(), objectives)\n        self.state.current_variance = variance\n\n        if variance &gt; self.trigger.variance_threshold:\n            self.state.consecutive_violations += 1\n            if self.state.consecutive_violations &gt;= self.trigger.consecutive_violations:\n                return True, f\"Variance {variance:.4f} &gt; {self.trigger.variance_threshold}\"\n        else:\n            self.state.consecutive_violations = 0\n\n        return False, \"Prediction within accuracy threshold\"\n\n    def apply_calibration(self, result: CalibrationResult) -&gt; None:\n        \"\"\"Apply calibration parameters to the plant.\"\"\"\n        for component in self.plant.components.values():\n            if component.component_type.value == \"digester\":\n                component.apply_calibration_parameters(result.parameters)\n\n    def _extract_time_window(self, measurements: MeasurementData, window_days: int) -&gt; MeasurementData:\n        \"\"\"Extract recent time window from measurements.\"\"\"\n        last_time = measurements.data.index[-1]\n        return measurements.get_time_window(last_time - timedelta(days=window_days), last_time)\n\n    def _calculate_prediction_variance(\n        self, measurements: MeasurementData, parameters: Dict[str, float], objectives: List[str]\n    ) -&gt; float:\n        \"\"\"Calculate prediction variance for current parameters.\"\"\"\n        try:\n            outputs = self.simulator.simulate_with_parameters(parameters, measurements)\n            variances: List[float] = []\n            for obj in objectives:\n                if obj not in outputs:\n                    continue\n                m = measurements.get_measurement(obj).values\n                s = np.atleast_1d(outputs[obj])\n                length = min(len(m), len(s))\n                m, s = m[:length], s[:length]\n                valid = ~(np.isnan(m) | np.isnan(s))\n                if not np.any(valid):\n                    continue\n                res = m[valid] - s[valid]\n                variances.append(float(np.std(res) / (np.mean(np.abs(m[valid])) + 1e-10)))\n            return float(np.mean(variances)) if variances else 0.0\n        except Exception:\n            return 0.0\n\n    def _setup_online_bounds(\n        self, parameters: List[str], current_params: Dict[str, float], max_change: float\n    ) -&gt; Dict[str, Tuple[float, float]]:\n        \"\"\"Setup bounded parameter ranges for online calibration.\"\"\"\n        bounds: Dict[str, Tuple[float, float]] = {}\n        for p in parameters:\n            curr = current_params.get(p, 0.0)\n            default = self.parameter_bounds.get_bounds_tuple(p) or (curr * 0.5, curr * 1.5)\n            bounds[p] = (max(default[0], curr * (1 - max_change)), min(default[1], curr * (1 + max_change)))\n        return bounds\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.OnlineCalibrator-functions","title":"Functions","text":""},{"location":"api/#pyadm1ode_calibration.calibration.OnlineCalibrator.__init__","title":"<code>__init__(plant, verbose=True, parameter_bounds=None)</code>","text":"<p>Initialize online calibrator.</p> <p>Parameters:</p> Name Type Description Default <code>plant</code> <code>Any</code> <p>BiogasPlant instance</p> required <code>verbose</code> <code>bool</code> <p>Enable progress output</p> <code>True</code> <code>parameter_bounds</code> <code>Optional[ParameterBounds]</code> <p>Custom parameter bounds manager</p> <code>None</code> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>def __init__(self, plant: Any, verbose: bool = True, parameter_bounds: Optional[ParameterBounds] = None):\n    \"\"\"Initialize online calibrator.\n\n    Args:\n        plant: BiogasPlant instance\n        verbose: Enable progress output\n        parameter_bounds: Custom parameter bounds manager\n    \"\"\"\n    super().__init__(plant, verbose)\n    self.parameter_bounds: ParameterBounds = parameter_bounds or create_default_bounds()\n    self.validator: CalibrationValidator = CalibrationValidator(plant, verbose=False)\n    self.trigger: OnlineCalibrationTrigger = OnlineCalibrationTrigger()\n    self.state: OnlineState = OnlineState()\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.OnlineCalibrator.apply_calibration","title":"<code>apply_calibration(result)</code>","text":"<p>Apply calibration parameters to the plant.</p> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>def apply_calibration(self, result: CalibrationResult) -&gt; None:\n    \"\"\"Apply calibration parameters to the plant.\"\"\"\n    for component in self.plant.components.values():\n        if component.component_type.value == \"digester\":\n            component.apply_calibration_parameters(result.parameters)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.OnlineCalibrator.calibrate","title":"<code>calibrate(measurements, parameters=None, current_parameters=None, variance_threshold=0.15, max_parameter_change=0.2, time_window=7, method='nelder_mead', max_iterations=50, objectives=None, weights=None, use_constraints=True, **kwargs)</code>","text":"<p>Perform online re-calibration with bounded parameter adjustments.</p> <p>Parameters:</p> Name Type Description Default <code>measurements</code> <code>MeasurementData</code> <p>Recent measurement data</p> required <code>parameters</code> <code>Optional[List[str]]</code> <p>Parameters to adjust</p> <code>None</code> <code>current_parameters</code> <code>Optional[Dict[str, float]]</code> <p>Current parameter values</p> <code>None</code> <code>variance_threshold</code> <code>float</code> <p>Variance threshold for triggering (0-1)</p> <code>0.15</code> <code>max_parameter_change</code> <code>float</code> <p>Maximum relative parameter change (0-1)</p> <code>0.2</code> <code>time_window</code> <code>int</code> <p>Days of recent data to use</p> <code>7</code> <code>method</code> <code>str</code> <p>Optimization method name</p> <code>'nelder_mead'</code> <code>max_iterations</code> <code>int</code> <p>Max optimization iterations</p> <code>50</code> <code>objectives</code> <code>Optional[List[str]]</code> <p>List of outputs to match</p> <code>None</code> <code>weights</code> <code>Optional[Dict[str, float]]</code> <p>Objective weights</p> <code>None</code> <code>use_constraints</code> <code>bool</code> <p>Whether to apply parameter constraints</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra optimizer settings</p> <code>{}</code> <p>Returns:</p> Type Description <code>CalibrationResult</code> <p>CalibrationResult instance</p> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>def calibrate(\n    self,\n    measurements: MeasurementData,\n    parameters: Optional[List[str]] = None,\n    current_parameters: Optional[Dict[str, float]] = None,\n    variance_threshold: float = 0.15,\n    max_parameter_change: float = 0.20,\n    time_window: int = 7,\n    method: str = \"nelder_mead\",\n    max_iterations: int = 50,\n    objectives: Optional[List[str]] = None,\n    weights: Optional[Dict[str, float]] = None,\n    use_constraints: bool = True,\n    **kwargs: Any,\n) -&gt; CalibrationResult:\n    \"\"\"Perform online re-calibration with bounded parameter adjustments.\n\n    Args:\n        measurements: Recent measurement data\n        parameters: Parameters to adjust\n        current_parameters: Current parameter values\n        variance_threshold: Variance threshold for triggering (0-1)\n        max_parameter_change: Maximum relative parameter change (0-1)\n        time_window: Days of recent data to use\n        method: Optimization method name\n        max_iterations: Max optimization iterations\n        objectives: List of outputs to match\n        weights: Objective weights\n        use_constraints: Whether to apply parameter constraints\n        **kwargs: Extra optimizer settings\n\n    Returns:\n        CalibrationResult instance\n    \"\"\"\n    start_time = time.time()\n    if objectives is None:\n        objectives = [\"Q_ch4\", \"pH\"]\n\n    if parameters is None:\n        if self.state.parameter_history:\n            parameters = list(self.state.parameter_history[-1].parameters.keys())\n        else:\n            raise ValueError(\"No parameters specified and no calibration history available\")\n\n    if current_parameters is None:\n        current_parameters = self._get_current_parameters()\n\n    windowed_data = self._extract_time_window(measurements, time_window)\n    current_variance = self._calculate_prediction_variance(windowed_data, current_parameters, objectives)\n    self.state.current_variance = current_variance\n\n    param_bounds = self._setup_online_bounds(parameters, current_parameters, max_parameter_change)\n\n    def simulator_wrapper(params: Dict[str, float]) -&gt; Dict[str, np.ndarray]:\n        return self.simulator.simulate_with_parameters(params, windowed_data)\n\n    measurements_dict: Dict[str, np.ndarray] = {\n        obj: windowed_data.get_measurement(obj).values for obj in objectives if obj in windowed_data.data.columns\n    }\n\n    objective_func: Callable[[np.ndarray], float] = MultiObjectiveFunction(\n        simulator=simulator_wrapper,\n        measurements_dict=measurements_dict,\n        objectives=objectives,\n        weights=weights or {obj: 1.0 / len(objectives) for obj in objectives},\n        parameter_names=parameters,\n        error_metric=\"rmse\",\n        normalize=True,\n    )\n\n    obj_func_final: Callable[[np.ndarray], float]\n    if use_constraints:\n        constraints = ParameterConstraints()\n        for p, (lb, ub) in param_bounds.items():\n            constraints.add_box_constraint(p, lb, ub, hard=True)\n\n        def penalized_objective(x: np.ndarray) -&gt; float:\n            params = {name: val for name, val in zip(parameters, x)}\n            return objective_func(x) + constraints.calculate_penalty(params)\n\n        obj_func_final = penalized_objective\n    else:\n        obj_func_final = objective_func\n\n    optimizer = create_optimizer(\n        method=method, bounds=param_bounds, max_iterations=max_iterations, verbose=self.verbose, **kwargs\n    )\n\n    initial_guess = np.array(\n        [current_parameters.get(p, self.parameter_bounds.get_default_values([p])[p]) for p in parameters]\n    )\n    opt_result = optimizer.optimize(obj_func_final, initial_guess=initial_guess)\n\n    validation_metrics: Dict[str, float] = {}\n    if opt_result.success:\n        val_res = self.validator.validate(\n            parameters=opt_result.parameter_dict, measurements=windowed_data, objectives=objectives\n        )\n        validation_metrics = {f\"{obj}_{k}\": float(getattr(m, k)) for obj, m in val_res.items() for k in [\"rmse\", \"r2\"]}\n\n    self.state.total_calibrations += 1\n    self.state.last_calibration_time = datetime.now()\n\n    history_entry = ParameterChangeHistory(\n        timestamp=datetime.now(),\n        parameters=opt_result.parameter_dict.copy(),\n        trigger_reason=\"variance_threshold\" if current_variance &gt; variance_threshold else \"manual\",\n        objective_value=float(opt_result.fun),\n        variance=float(current_variance),\n        success=bool(opt_result.success),\n    )\n    self.state.parameter_history.append(history_entry)\n\n    return CalibrationResult(\n        success=opt_result.success,\n        parameters=opt_result.parameter_dict,\n        initial_parameters=current_parameters,\n        objective_value=float(opt_result.fun),\n        n_iterations=int(opt_result.nit),\n        execution_time=time.time() - start_time,\n        method=method,\n        message=str(getattr(opt_result, \"message\", \"Online calibration completed\")),\n        validation_metrics=validation_metrics,\n    )\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.OnlineCalibrator.should_recalibrate","title":"<code>should_recalibrate(recent_measurements, objectives=None)</code>","text":"<p>Check if re-calibration should be triggered.</p> <p>Returns:</p> Type Description <code>Tuple[bool, str]</code> <p>Tuple of (should_recalibrate, reason)</p> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>def should_recalibrate(\n    self, recent_measurements: MeasurementData, objectives: Optional[List[str]] = None\n) -&gt; Tuple[bool, str]:\n    \"\"\"Check if re-calibration should be triggered.\n\n    Returns:\n        Tuple of (should_recalibrate, reason)\n    \"\"\"\n    if not self.trigger.enabled:\n        return False, \"Disabled\"\n\n    if objectives is None:\n        objectives = [\"Q_ch4\", \"pH\"]\n\n    if self.state.last_calibration_time:\n        hours = (datetime.now() - self.state.last_calibration_time).total_seconds() / 3600\n        if hours &lt; self.trigger.time_threshold:\n            return False, f\"Too soon since last calibration ({hours:.1f}h)\"\n\n    variance = self._calculate_prediction_variance(recent_measurements, self._get_current_parameters(), objectives)\n    self.state.current_variance = variance\n\n    if variance &gt; self.trigger.variance_threshold:\n        self.state.consecutive_violations += 1\n        if self.state.consecutive_violations &gt;= self.trigger.consecutive_violations:\n            return True, f\"Variance {variance:.4f} &gt; {self.trigger.variance_threshold}\"\n    else:\n        self.state.consecutive_violations = 0\n\n    return False, \"Prediction within accuracy threshold\"\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData","title":"<code>pyadm1ode_calibration.io.MeasurementData</code>","text":"<p>Container for biogas plant measurement data.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>class MeasurementData:\n    \"\"\"\n    Container for biogas plant measurement data.\n    \"\"\"\n\n    def __init__(self, data: pd.DataFrame, metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Initialize measurement data.\n\n        Args:\n            data: DataFrame with measurements\n            metadata: Optional metadata dictionary\n        \"\"\"\n        self.data = data\n        self.metadata = metadata or {}\n\n        if \"timestamp\" in self.data.columns:\n            if not pd.api.types.is_datetime64_any_dtype(self.data[\"timestamp\"]):\n                self.data[\"timestamp\"] = pd.to_datetime(self.data[\"timestamp\"])\n            self.data = self.data.set_index(\"timestamp\").sort_index()\n\n    @classmethod\n    def from_csv(\n        cls,\n        filepath: str,\n        timestamp_column: str = \"timestamp\",\n        sep: str = \",\",\n        parse_dates: bool = True,\n        resample: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; \"MeasurementData\":\n        \"\"\"Load measurement data from CSV file.\"\"\"\n        data = pd.read_csv(filepath, sep=sep, **kwargs)\n        if timestamp_column in data.columns:\n            data[\"timestamp\"] = pd.to_datetime(data[timestamp_column])\n            if timestamp_column != \"timestamp\":\n                data = data.drop(columns=[timestamp_column])\n        instance = cls(data)\n        if resample is not None:\n            instance.resample(resample)\n        return instance\n\n    def validate(\n        self, required_columns: Optional[List[str]] = None, expected_ranges: Optional[Dict[str, Tuple[float, float]]] = None\n    ) -&gt; ValidationResult:\n        \"\"\"Validate measurement data.\"\"\"\n        if expected_ranges is None:\n            expected_ranges = {\n                \"pH\": (5.0, 9.0),\n                \"VFA\": (0.0, 20.0),\n                \"TAC\": (0.0, 50.0),\n                \"Q_gas\": (0.0, 5000.0),\n                \"Q_ch4\": (0.0, 3000.0),\n                \"T_digester\": (273.15, 333.15),\n            }\n        return DataValidator.validate(self.data, required_columns=required_columns, expected_ranges=expected_ranges)\n\n    def remove_outliers(\n        self, columns: Optional[List[str]] = None, method: str = \"zscore\", threshold: float = 3.0, **kwargs: Any\n    ) -&gt; int:\n        \"\"\"Remove outliers from specified columns.\"\"\"\n        if columns is None:\n            columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n\n        n_outliers = 0\n        for col in columns:\n            if col not in self.data.columns:\n                continue\n            if method == \"zscore\":\n                is_outlier = OutlierDetector.detect_zscore(self.data[col], threshold=threshold)\n            elif method == \"iqr\":\n                is_outlier = OutlierDetector.detect_iqr(self.data[col], multiplier=threshold)\n            elif method == \"moving_window\":\n                window = kwargs.get(\"window\", 5)\n                is_outlier = OutlierDetector.detect_moving_window(self.data[col], window=window, threshold=threshold)\n            else:\n                raise ValueError(f\"Unknown outlier detection method: {method}\")\n\n            n_col_outliers = int(is_outlier.sum())\n            self.data.loc[is_outlier, col] = np.nan\n            n_outliers += n_col_outliers\n        return n_outliers\n\n    def fill_gaps(self, columns: Optional[List[str]] = None, method: str = \"interpolate\", **kwargs: Any) -&gt; None:\n        \"\"\"Fill missing values in time series.\"\"\"\n        if columns is None:\n            columns = self.data.columns.tolist()\n\n        for col in columns:\n            if col not in self.data.columns:\n                continue\n            if method == \"interpolate\":\n                limit = kwargs.get(\"limit\", None)\n                self.data[col] = self.data[col].interpolate(method=\"linear\", limit=limit)\n            elif method == \"forward\":\n                limit = kwargs.get(\"limit\", None)\n                self.data[col] = self.data[col].ffill(limit=limit)\n            elif method == \"backward\":\n                limit = kwargs.get(\"limit\", None)\n                self.data[col] = self.data[col].bfill(limit=limit)\n            elif method == \"mean\":\n                self.data[col] = self.data[col].fillna(self.data[col].mean())\n            elif method == \"median\":\n                self.data[col] = self.data[col].fillna(self.data[col].median())\n            else:\n                raise ValueError(f\"Unknown fill method: {method}\")\n\n    def resample(self, freq: str, aggregation: str = \"mean\") -&gt; None:\n        \"\"\"Resample time series.\"\"\"\n        resampler = self.data.resample(freq)\n        if aggregation == \"mean\":\n            self.data = resampler.mean()\n        elif aggregation == \"sum\":\n            self.data = resampler.sum()\n        elif aggregation == \"first\":\n            self.data = resampler.first()\n        elif aggregation == \"last\":\n            self.data = resampler.last()\n        else:\n            raise ValueError(f\"Unknown aggregation method: {aggregation}\")\n\n    def get_measurement(\n        self, column: str, start_time: Optional[Union[str, datetime]] = None, end_time: Optional[Union[str, datetime]] = None\n    ) -&gt; pd.Series:\n        \"\"\"Get measurement time series.\"\"\"\n        if column not in self.data.columns:\n            raise ValueError(f\"Column '{column}' not found\")\n        series = self.data[column]\n        if start_time is not None or end_time is not None:\n            series = series.loc[start_time:end_time]  # type: ignore\n        return series\n\n    def get_substrate_feeds(self, substrate_columns: Optional[List[str]] = None) -&gt; np.ndarray:\n        \"\"\"Get substrate feed rates as array.\"\"\"\n        if substrate_columns is None:\n            substrate_columns = [col for col in self.data.columns if col.startswith(\"Q_sub\")]\n        if not substrate_columns:\n            raise ValueError(\"No substrate columns found\")\n        return self.data[substrate_columns].values\n\n    def get_time_window(self, start_time: Union[str, datetime], end_time: Union[str, datetime]) -&gt; \"MeasurementData\":\n        \"\"\"Get data for specific time window.\"\"\"\n        windowed_data = self.data.loc[start_time:end_time].copy()  # type: ignore\n        return MeasurementData(windowed_data, metadata=self.metadata.copy())\n\n    def summary(self) -&gt; pd.DataFrame:\n        \"\"\"Get statistical summary of measurements.\"\"\"\n        return self.data.describe()\n\n    def to_csv(self, filepath: str, **kwargs: Any) -&gt; None:\n        \"\"\"Save measurement data to CSV.\"\"\"\n        self.data.to_csv(filepath, **kwargs)\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __repr__(self) -&gt; str:\n        try:\n            time_range = f\"{self.data.index[0]} to {self.data.index[-1]}\"\n        except Exception:\n            time_range = \"empty\"\n        return f\"MeasurementData(n_rows={len(self.data)}, n_columns={len(self.data.columns)}, time_range={time_range})\"\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData-functions","title":"Functions","text":""},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.__init__","title":"<code>__init__(data, metadata=None)</code>","text":"<p>Initialize measurement data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with measurements</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional metadata dictionary</p> <code>None</code> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def __init__(self, data: pd.DataFrame, metadata: Optional[Dict[str, Any]] = None):\n    \"\"\"\n    Initialize measurement data.\n\n    Args:\n        data: DataFrame with measurements\n        metadata: Optional metadata dictionary\n    \"\"\"\n    self.data = data\n    self.metadata = metadata or {}\n\n    if \"timestamp\" in self.data.columns:\n        if not pd.api.types.is_datetime64_any_dtype(self.data[\"timestamp\"]):\n            self.data[\"timestamp\"] = pd.to_datetime(self.data[\"timestamp\"])\n        self.data = self.data.set_index(\"timestamp\").sort_index()\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.fill_gaps","title":"<code>fill_gaps(columns=None, method='interpolate', **kwargs)</code>","text":"<p>Fill missing values in time series.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def fill_gaps(self, columns: Optional[List[str]] = None, method: str = \"interpolate\", **kwargs: Any) -&gt; None:\n    \"\"\"Fill missing values in time series.\"\"\"\n    if columns is None:\n        columns = self.data.columns.tolist()\n\n    for col in columns:\n        if col not in self.data.columns:\n            continue\n        if method == \"interpolate\":\n            limit = kwargs.get(\"limit\", None)\n            self.data[col] = self.data[col].interpolate(method=\"linear\", limit=limit)\n        elif method == \"forward\":\n            limit = kwargs.get(\"limit\", None)\n            self.data[col] = self.data[col].ffill(limit=limit)\n        elif method == \"backward\":\n            limit = kwargs.get(\"limit\", None)\n            self.data[col] = self.data[col].bfill(limit=limit)\n        elif method == \"mean\":\n            self.data[col] = self.data[col].fillna(self.data[col].mean())\n        elif method == \"median\":\n            self.data[col] = self.data[col].fillna(self.data[col].median())\n        else:\n            raise ValueError(f\"Unknown fill method: {method}\")\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.from_csv","title":"<code>from_csv(filepath, timestamp_column='timestamp', sep=',', parse_dates=True, resample=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load measurement data from CSV file.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    filepath: str,\n    timestamp_column: str = \"timestamp\",\n    sep: str = \",\",\n    parse_dates: bool = True,\n    resample: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; \"MeasurementData\":\n    \"\"\"Load measurement data from CSV file.\"\"\"\n    data = pd.read_csv(filepath, sep=sep, **kwargs)\n    if timestamp_column in data.columns:\n        data[\"timestamp\"] = pd.to_datetime(data[timestamp_column])\n        if timestamp_column != \"timestamp\":\n            data = data.drop(columns=[timestamp_column])\n    instance = cls(data)\n    if resample is not None:\n        instance.resample(resample)\n    return instance\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.get_measurement","title":"<code>get_measurement(column, start_time=None, end_time=None)</code>","text":"<p>Get measurement time series.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def get_measurement(\n    self, column: str, start_time: Optional[Union[str, datetime]] = None, end_time: Optional[Union[str, datetime]] = None\n) -&gt; pd.Series:\n    \"\"\"Get measurement time series.\"\"\"\n    if column not in self.data.columns:\n        raise ValueError(f\"Column '{column}' not found\")\n    series = self.data[column]\n    if start_time is not None or end_time is not None:\n        series = series.loc[start_time:end_time]  # type: ignore\n    return series\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.get_substrate_feeds","title":"<code>get_substrate_feeds(substrate_columns=None)</code>","text":"<p>Get substrate feed rates as array.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def get_substrate_feeds(self, substrate_columns: Optional[List[str]] = None) -&gt; np.ndarray:\n    \"\"\"Get substrate feed rates as array.\"\"\"\n    if substrate_columns is None:\n        substrate_columns = [col for col in self.data.columns if col.startswith(\"Q_sub\")]\n    if not substrate_columns:\n        raise ValueError(\"No substrate columns found\")\n    return self.data[substrate_columns].values\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.get_time_window","title":"<code>get_time_window(start_time, end_time)</code>","text":"<p>Get data for specific time window.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def get_time_window(self, start_time: Union[str, datetime], end_time: Union[str, datetime]) -&gt; \"MeasurementData\":\n    \"\"\"Get data for specific time window.\"\"\"\n    windowed_data = self.data.loc[start_time:end_time].copy()  # type: ignore\n    return MeasurementData(windowed_data, metadata=self.metadata.copy())\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.remove_outliers","title":"<code>remove_outliers(columns=None, method='zscore', threshold=3.0, **kwargs)</code>","text":"<p>Remove outliers from specified columns.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def remove_outliers(\n    self, columns: Optional[List[str]] = None, method: str = \"zscore\", threshold: float = 3.0, **kwargs: Any\n) -&gt; int:\n    \"\"\"Remove outliers from specified columns.\"\"\"\n    if columns is None:\n        columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n\n    n_outliers = 0\n    for col in columns:\n        if col not in self.data.columns:\n            continue\n        if method == \"zscore\":\n            is_outlier = OutlierDetector.detect_zscore(self.data[col], threshold=threshold)\n        elif method == \"iqr\":\n            is_outlier = OutlierDetector.detect_iqr(self.data[col], multiplier=threshold)\n        elif method == \"moving_window\":\n            window = kwargs.get(\"window\", 5)\n            is_outlier = OutlierDetector.detect_moving_window(self.data[col], window=window, threshold=threshold)\n        else:\n            raise ValueError(f\"Unknown outlier detection method: {method}\")\n\n        n_col_outliers = int(is_outlier.sum())\n        self.data.loc[is_outlier, col] = np.nan\n        n_outliers += n_col_outliers\n    return n_outliers\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.resample","title":"<code>resample(freq, aggregation='mean')</code>","text":"<p>Resample time series.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def resample(self, freq: str, aggregation: str = \"mean\") -&gt; None:\n    \"\"\"Resample time series.\"\"\"\n    resampler = self.data.resample(freq)\n    if aggregation == \"mean\":\n        self.data = resampler.mean()\n    elif aggregation == \"sum\":\n        self.data = resampler.sum()\n    elif aggregation == \"first\":\n        self.data = resampler.first()\n    elif aggregation == \"last\":\n        self.data = resampler.last()\n    else:\n        raise ValueError(f\"Unknown aggregation method: {aggregation}\")\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.summary","title":"<code>summary()</code>","text":"<p>Get statistical summary of measurements.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def summary(self) -&gt; pd.DataFrame:\n    \"\"\"Get statistical summary of measurements.\"\"\"\n    return self.data.describe()\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.to_csv","title":"<code>to_csv(filepath, **kwargs)</code>","text":"<p>Save measurement data to CSV.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def to_csv(self, filepath: str, **kwargs: Any) -&gt; None:\n    \"\"\"Save measurement data to CSV.\"\"\"\n    self.data.to_csv(filepath, **kwargs)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.MeasurementData.validate","title":"<code>validate(required_columns=None, expected_ranges=None)</code>","text":"<p>Validate measurement data.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def validate(\n    self, required_columns: Optional[List[str]] = None, expected_ranges: Optional[Dict[str, Tuple[float, float]]] = None\n) -&gt; ValidationResult:\n    \"\"\"Validate measurement data.\"\"\"\n    if expected_ranges is None:\n        expected_ranges = {\n            \"pH\": (5.0, 9.0),\n            \"VFA\": (0.0, 20.0),\n            \"TAC\": (0.0, 50.0),\n            \"Q_gas\": (0.0, 5000.0),\n            \"Q_ch4\": (0.0, 3000.0),\n            \"T_digester\": (273.15, 333.15),\n        }\n    return DataValidator.validate(self.data, required_columns=required_columns, expected_ranges=expected_ranges)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler","title":"<code>pyadm1ode_calibration.io.CSVHandler</code>","text":"<p>Handler for CSV file operations in PyADM1.</p> <p>Supports reading and writing various CSV formats used in biogas plant operation and laboratory analysis.</p> Example <p>handler = CSVHandler() data = handler.load_substrate_lab_data(\"lab_results.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>class CSVHandler:\n    \"\"\"\n    Handler for CSV file operations in PyADM1.\n\n    Supports reading and writing various CSV formats used in biogas\n    plant operation and laboratory analysis.\n\n    Example:\n        &gt;&gt;&gt; handler = CSVHandler()\n        &gt;&gt;&gt; data = handler.load_substrate_lab_data(\"lab_results.csv\")\n    \"\"\"\n\n    # Standard column mappings (German -&gt; English)\n    COLUMN_MAPPINGS = {\n        # Dry matter and organic content\n        \"Trockensubstanz\": \"TS\",\n        \"Trockensubstanzgehalt\": \"TS\",\n        \"TS-Gehalt\": \"TS\",\n        \"Organische Trockensubstanz\": \"VS\",\n        \"oTS\": \"VS\",\n        \"Organischer Trockensubstanzgehalt\": \"oTS\",\n        \"Fermentierbare organische Trockensubstanz\": \"foTS\",\n        \"Fl\u00fcchtige Feststoffe\": \"VS\",\n        # Weender analysis\n        \"Rohprotein\": \"RP\",\n        \"Rohfett\": \"RL\",\n        \"Rohfaser\": \"RF\",\n        \"Rohasche\": \"RA\",\n        \"N-freie Extraktstoffe\": \"NfE\",\n        # Van Soest\n        \"Neutral Detergent Fiber\": \"NDF\",\n        \"Acid Detergent Fiber\": \"ADF\",\n        \"Acid Detergent Lignin\": \"ADL\",\n        # Chemical properties\n        \"pH-Wert\": \"pH\",\n        \"Ammoniumstickstoff\": \"NH4_N\",\n        \"Ammonium-N\": \"NH4_N\",\n        \"NH4-N\": \"NH4_N\",\n        \"Alkalinit\u00e4t\": \"TAC\",\n        \"Pufferkapazit\u00e4t\": \"TAC\",\n        \"CSB\": \"COD\",\n        \"CSB-Filtrat\": \"COD_S\",\n        \"Chemischer Sauerstoffbedarf\": \"COD\",\n        \"Chemischer Sauerstoffbedarf des Filtrats\": \"COD_S\",\n        # Biogas potential\n        \"Biogaspotential\": \"BMP\",\n        \"Methanpotential\": \"BMP\",\n        \"Biochemisches Methanpotential\": \"BMP\",\n        \"Gasausbeute\": \"BMP\",\n        # Carbon and nitrogen\n        \"Kohlenstoffgehalt\": \"C_content\",\n        \"Stickstoffgehalt\": \"N_content\",\n        \"C/N-Verh\u00e4ltnis\": \"C_to_N\",\n        \"C-N-Verh\u00e4ltnis\": \"C_to_N\",\n        \"Gesamt-Kjeldahl-Stickstoff\": \"TKN\",\n        # Measurement data\n        \"Zeitstempel\": \"timestamp\",\n        \"Zeit\": \"timestamp\",\n        \"Datum\": \"timestamp\",\n        \"Biogasproduktion\": \"Q_gas\",\n        \"Methanproduktion\": \"Q_ch4\",\n        \"Elektrische Leistung\": \"P_el\",\n        \"Thermische Leistung\": \"P_th\",\n        \"Temperatur\": \"T_digester\",\n    }\n\n    # Unit conversions (from -&gt; to, factor)\n    UNIT_CONVERSIONS = {\n        # Dry matter: % FM -&gt; % FM (no conversion needed, just validation)\n        (\"TS\", \"% FM\", \"% FM\"): 1.0,\n        (\"TS\", \"%FM\", \"% FM\"): 1.0,\n        (\"TS\", \"g/100g\", \"% FM\"): 1.0,\n        # Volatile solids: % TS -&gt; % TS\n        (\"VS\", \"% TS\", \"% TS\"): 1.0,\n        (\"VS\", \"%TS\", \"% TS\"): 1.0,\n        # BMP conversions\n        (\"BMP\", \"L/kg VS\", \"L CH4/kg oTS\"): 1.0,  # Assuming same\n        (\"BMP\", \"mL/g oTS\", \"L CH4/kg oTS\"): 1.0,  # mL/g = L/kg\n        (\"BMP\", \"Nm\u00b3/t oTS\", \"L CH4/kg oTS\"): 1.0,  # Nm\u00b3/t = L/kg\n        # Nitrogen content\n        (\"NH4_N\", \"mg/L\", \"g/L\"): 0.001,\n        (\"NH4_N\", \"g/kg\", \"g/L\"): 1.0,  # Approximate for density ~1\n        # Alkalinity\n        (\"TAC\", \"mmol/L\", \"mmol/L\"): 1.0,\n        (\"TAC\", \"meq/L\", \"mmol/L\"): 1.0,  # Equivalent\n        (\"TAC\", \"g CaCO3/L\", \"mmol/L\"): 20.0,  # 1 g CaCO3 = 20 mmol\n        # COD\n        (\"COD_S\", \"mg/L\", \"g/L\"): 0.001,\n        (\"COD_S\", \"g/L\", \"g/L\"): 1.0,\n    }\n\n    def __init__(self, decimal_separator: str = \".\", thousands_separator: str = \",\"):\n        \"\"\"\n        Initialize CSV handler.\n\n        Args:\n            decimal_separator: Decimal separator (\".\" or \",\")\n            thousands_separator: Thousands separator (\",\" or \".\" or \"\")\n        \"\"\"\n        self.decimal_separator = decimal_separator\n        self.thousands_separator = thousands_separator\n\n    # ========================================================================\n    # Substrate Laboratory Data\n    # ========================================================================\n\n    def load_substrate_lab_data(\n        self,\n        filepath: str,\n        substrate_name: Optional[str] = None,\n        substrate_type: Optional[str] = None,\n        sample_date: Optional[Union[str, datetime]] = None,\n        sep: str = \",\",\n        encoding: str = \"utf-8\",\n        validate: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Load substrate characterization data from laboratory CSV.\n\n        Expected columns (German or English):\n        - Trockensubstanzgehalt (TS) [% FM]\n        - Organische Trockensubstanz (VS) [% TS]\n        - Fermentierbare organische Trockensubstanz (foTS) [% TS]\n        - Rohprotein (RP) [% TS]\n        - Rohfett (RL) [% TS]\n        - Rohfaser (RF) [% TS]\n        - NDF, ADF, ADL [% TS]\n        - pH-Wert (pH)\n        - Ammoniumstickstoff (NH4-N) [g/L or mg/L]\n        - Alkalinit\u00e4t (TAC) [mmol/L]\n        - Biochemisches Methanpotential (BMP) [L CH4/kg oTS]\n        - CSB des Filtrats (COD_S) [g/L]\n\n        Args:\n            filepath: Path to CSV file\n            substrate_name: Substrate name (if not in file)\n            substrate_type: Substrate type (maize, manure, grass, etc.)\n            sample_date: Sample date (if not in file)\n            sep: Column separator\n            encoding: File encoding\n            validate: Validate data ranges\n\n        Returns:\n            Dict with substrate data\n\n        Example:\n            &gt;&gt;&gt; handler = CSVHandler()\n            &gt;&gt;&gt; data = handler.load_substrate_lab_data(\n            ...     \"maize_analysis.csv\",\n            ...     substrate_name=\"Maize silage batch 23\",\n            ...     substrate_type=\"maize\",\n            ...     sample_date=\"2024-01-15\"\n            ... )\n            &gt;&gt;&gt; print(f\"TS: {data['TS']:.1f}% FM\")\n        \"\"\"\n        # Auto-detect separator if needed\n        if sep == \"auto\":\n            sep = self._detect_separator(filepath)\n\n        # Read CSV\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n        # Try to detect if file is in \"vertical\" format (parameter, value, unit)\n        if len(df.columns) &lt;= 3 and \"Parameter\" in df.columns or \"Messgr\u00f6\u00dfe\" in df.columns:\n            df = self._parse_vertical_format(df)\n\n        # Map column names\n        df = self._map_column_names(df)\n\n        # If multiple rows, take the first one (or could aggregate)\n        if len(df) &gt; 1:\n            warnings.warn(f\"CSV contains {len(df)} rows, using first row only\")\n\n        row = df.iloc[0]\n\n        # Extract data\n        result = {\n            \"substrate_name\": substrate_name or row.get(\"substrate_name\", \"Unknown\"),\n            \"substrate_type\": substrate_type or row.get(\"substrate_type\", \"unknown\"),\n            \"sample_date\": sample_date or row.get(\"sample_date\", datetime.now()),\n        }\n\n        # Add all available parameters\n        for param in [\n            \"TS\",\n            \"VS\",\n            \"oTS\",\n            \"foTS\",\n            \"RP\",\n            \"RL\",\n            \"RF\",\n            \"RA\",\n            \"NfE\",\n            \"NDF\",\n            \"ADF\",\n            \"ADL\",\n            \"pH\",\n            \"NH4_N\",\n            \"TAC\",\n            \"COD\",\n            \"COD_S\",\n            \"BMP\",\n            \"C_content\",\n            \"N_content\",\n            \"C_to_N\",\n            \"TKN\",\n        ]:\n            if param in df.columns:\n                value = row[param]\n                # Handle both scalar values and Series\n                if isinstance(value, pd.Series):\n                    value = value.iloc[0] if len(value) &gt; 0 else None\n                if pd.notna(value):\n                    result[param] = float(value)\n\n        # Validate if requested\n        if validate:\n            result = self._validate_substrate_data(result)\n\n        # TODO: diese Substratparameter m\u00fcssen in die substrate_....xml geschrieben werden. evtl. gibt es in einer\n        #  c# DLL auch bereits eine Methode die man aufrufen kann. glaube aber eher nicht\n\n        return result\n\n    def load_multiple_substrate_samples(\n        self,\n        filepath: str,\n        sep: str = \",\",\n        encoding: str = \"utf-8\",\n        date_column: str = \"sample_date\",\n        name_column: str = \"substrate_name\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load multiple substrate samples from CSV.\n\n        Expected format: Each row is one sample with columns for all parameters.\n\n        Args:\n            filepath: Path to CSV file\n            sep: Column separator\n            encoding: File encoding\n            date_column: Name of date column\n            name_column: Name of substrate name column\n\n        Returns:\n            DataFrame with substrate data\n\n        Example:\n            &gt;&gt;&gt; handler = CSVHandler()\n            &gt;&gt;&gt; samples = handler.load_multiple_substrate_samples(\n            ...     \"substrate_database.csv\"\n            ... )\n            &gt;&gt;&gt; print(samples.head())\n        \"\"\"\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n        # Map column names\n        df = self._map_column_names(df)\n\n        # Parse date column\n        if date_column in df.columns:\n            df[date_column] = pd.to_datetime(df[date_column])\n\n        return df\n\n    def export_substrate_data(\n        self, data: Union[Dict[str, Any], pd.DataFrame], filepath: str, sep: str = \",\", encoding: str = \"utf-8\"\n    ) -&gt; None:\n        \"\"\"\n        Export substrate data to CSV.\n\n        Args:\n            data: Dict or DataFrame with substrate data\n            filepath: Output file path\n            sep: Column separator\n            encoding: File encoding\n\n        Example:\n            &gt;&gt;&gt; handler.export_substrate_data(substrate_data, \"export.csv\")\n        \"\"\"\n        if isinstance(data, dict):\n            df = pd.DataFrame([data])\n        else:\n            df = data\n\n        df.to_csv(filepath, sep=sep, encoding=encoding, index=False)\n        print(f\"\u2713 Exported substrate data to {filepath}\")\n\n    # ========================================================================\n    # Measurement Data\n    # ========================================================================\n\n    # TODO: so eine Methode gibt es bereits in calibration.py. Dort evtl. l\u00f6schen. Vorher beide vergleichen.\n    def load_measurement_data(\n        self,\n        filepath: str,\n        timestamp_column: str = \"timestamp\",\n        sep: str = \",\",\n        encoding: str = \"utf-8\",\n        parse_dates: bool = True,\n        resample: Optional[str] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load time series measurement data from CSV.\n\n        Expected columns:\n        - timestamp (or Zeit, Zeitstempel)\n        - Q_sub_* (substrate feeds)\n        - pH, VFA, TAC, FOS_TAC\n        - T_digester\n        - Q_gas, Q_ch4, Q_co2, CH4_content, P_gas\n        - P_el, P_th\n\n        Args:\n            filepath: Path to CSV file\n            timestamp_column: Name of timestamp column\n            sep: Column separator\n            encoding: File encoding\n            parse_dates: Parse timestamp column\n            resample: Resample frequency (e.g., \"1h\", \"1d\")\n\n        Returns:\n            DataFrame with measurements\n\n        Example:\n            &gt;&gt;&gt; handler = CSVHandler()\n            &gt;&gt;&gt; data = handler.load_measurement_data(\n            ...     \"plant_data.csv\",\n            ...     resample=\"1h\"\n            ... )\n        \"\"\"\n        # Auto-detect separator\n        if sep == \"auto\":\n            sep = self._detect_separator(filepath)\n\n        # Read CSV\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n        # Map column names\n        df = self._map_column_names(df)\n\n        # Parse timestamp\n        if timestamp_column in df.columns:\n            if parse_dates:\n                df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n            df = df.set_index(timestamp_column).sort_index()\n\n        # Resample if requested\n        if resample is not None:\n            df = df.resample(resample).mean()\n\n        return df\n\n    def export_measurement_data(\n        self, data: pd.DataFrame, filepath: str, sep: str = \",\", encoding: str = \"utf-8\", include_index: bool = True\n    ) -&gt; None:\n        \"\"\"\n        Export measurement data to CSV.\n\n        Args:\n            data: DataFrame with measurements\n            filepath: Output file path\n            sep: Column separator\n            encoding: File encoding\n            include_index: Include index (timestamp) in output\n\n        Example:\n            &gt;&gt;&gt; handler.export_measurement_data(measurements, \"export.csv\")\n        \"\"\"\n        data.to_csv(filepath, sep=sep, encoding=encoding, index=include_index)\n        print(f\"\u2713 Exported measurement data to {filepath} ({len(data)} rows)\")\n\n    # ========================================================================\n    # Simulation Results\n    # ========================================================================\n\n    def export_simulation_results(\n        self,\n        results: List[Dict[str, Any]],\n        filepath: str,\n        sep: str = \",\",\n        encoding: str = \"utf-8\",\n        flatten_components: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Export simulation results to CSV.\n\n        Args:\n            results: List of result dicts from plant.simulate()\n            filepath: Output file path\n            sep: Column separator\n            encoding: File encoding\n            flatten_components: Flatten component results into columns\n\n        Example:\n            &gt;&gt;&gt; results = plant.simulate(duration=30, dt=1/24)\n            &gt;&gt;&gt; handler.export_simulation_results(results, \"simulation.csv\")\n        \"\"\"\n        if not results:\n            warnings.warn(\"No results to export\")\n            return\n\n        # Convert to DataFrame\n        if flatten_components:\n            # Flatten structure: time, component1_metric1, component1_metric2, ...\n            rows = []\n            for result in results:\n                row = {\"time\": result[\"time\"]}\n\n                for comp_id, comp_data in result[\"components\"].items():\n                    for metric, value in comp_data.items():\n                        # Skip nested dicts (like gas_storage)\n                        if isinstance(value, dict):\n                            continue\n                        col_name = f\"{comp_id}_{metric}\"\n                        row[col_name] = value\n\n                rows.append(row)\n\n            df = pd.DataFrame(rows)\n        else:\n            # Simple format: just time and first component's data\n            first_comp_id = list(results[0][\"components\"].keys())[0]\n            rows = []\n            for result in results:\n                row = {\"time\": result[\"time\"]}\n                row.update(result[\"components\"][first_comp_id])\n                # Remove nested dicts\n                row = {k: v for k, v in row.items() if not isinstance(v, dict)}\n                rows.append(row)\n\n            df = pd.DataFrame(rows)\n\n        # Export\n        df.to_csv(filepath, sep=sep, encoding=encoding, index=False)\n        print(f\"\u2713 Exported simulation results to {filepath} ({len(df)} time points)\")\n\n    def load_simulation_results(self, filepath: str, sep: str = \",\", encoding: str = \"utf-8\") -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Load simulation results from CSV.\n\n        Args:\n            filepath: Path to CSV file\n            sep: Column separator\n            encoding: File encoding\n\n        Returns:\n            List of result dicts\n\n        Example:\n            &gt;&gt;&gt; results = handler.load_simulation_results(\"simulation.csv\")\n        \"\"\"\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n        # Convert back to results format\n        results = []\n        for _, row in df.iterrows():\n            result = {\"time\": row[\"time\"], \"components\": {}}\n\n            # Group columns by component\n            for col in df.columns:\n                if col == \"time\":\n                    continue\n\n                if \"_\" in col:\n                    comp_id, metric = col.split(\"_\", 1)\n                    if comp_id not in result[\"components\"]:\n                        result[\"components\"][comp_id] = {}\n                    result[\"components\"][comp_id][metric] = row[col]\n\n            results.append(result)\n\n        return results\n\n    # ========================================================================\n    # Parameter Tables\n    # ========================================================================\n\n    def load_parameter_table(\n        self, filepath: str, sep: str = \",\", encoding: str = \"utf-8\", index_col: Optional[str] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load parameter table from CSV.\n\n        Expected format:\n        - Rows: Parameters\n        - Columns: Different scenarios/substrates\n\n        Args:\n            filepath: Path to CSV file\n            sep: Column separator\n            encoding: File encoding\n            index_col: Column to use as index (usually parameter name)\n\n        Returns:\n            DataFrame with parameters\n\n        Example:\n            &gt;&gt;&gt; params = handler.load_parameter_table(\"parameters.csv\")\n        \"\"\"\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding, index_col=index_col)\n        return df\n\n    def export_parameter_table(self, data: pd.DataFrame, filepath: str, sep: str = \",\", encoding: str = \"utf-8\") -&gt; None:\n        \"\"\"\n        Export parameter table to CSV.\n\n        Args:\n            data: DataFrame with parameters\n            filepath: Output file path\n            sep: Column separator\n            encoding: File encoding\n\n        Example:\n            &gt;&gt;&gt; handler.export_parameter_table(params_df, \"parameters.csv\")\n        \"\"\"\n        data.to_csv(filepath, sep=sep, encoding=encoding)\n        print(f\"\u2713 Exported parameter table to {filepath}\")\n\n    # ========================================================================\n    # Helper Methods\n    # ========================================================================\n\n    def _detect_separator(self, filepath: str) -&gt; str:\n        \"\"\"\n        Auto-detect CSV separator.\n\n        Args:\n            filepath: Path to CSV file\n\n        Returns:\n            Detected separator\n        \"\"\"\n        with open(filepath, \"r\") as f:\n            first_line = f.readline()\n\n        # Count occurrences of common separators\n        comma_count = first_line.count(\",\")\n        semicolon_count = first_line.count(\";\")\n        tab_count = first_line.count(\"\\t\")\n\n        if semicolon_count &gt; comma_count and semicolon_count &gt; tab_count:\n            return \";\"\n        elif tab_count &gt; comma_count:\n            return \"\\t\"\n        else:\n            return \",\"\n\n    def _map_column_names(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Map German column names to English standard names.\n\n        Args:\n            df: DataFrame with original column names\n\n        Returns:\n            DataFrame with mapped column names\n        \"\"\"\n        # Create mapping dict for this DataFrame\n        mapping = {}\n        for col in df.columns:\n            # Check if column is in mapping dict\n            if col in self.COLUMN_MAPPINGS:\n                mapping[col] = self.COLUMN_MAPPINGS[col]\n            # Also check case-insensitive\n            elif col.lower().strip() in {k.lower(): v for k, v in self.COLUMN_MAPPINGS.items()}:\n                original_key = [k for k in self.COLUMN_MAPPINGS if k.lower() == col.lower().strip()][0]\n                mapping[col] = self.COLUMN_MAPPINGS[original_key]\n\n        if mapping:\n            df = df.rename(columns=mapping)\n\n        return df\n\n    def _parse_vertical_format(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Parse vertical CSV format (parameter, value, unit columns).\n\n        Args:\n            df: DataFrame in vertical format\n\n        Returns:\n            DataFrame in horizontal format\n        \"\"\"\n        # Expected columns: Parameter/Messgr\u00f6\u00dfe, Wert/Value, Einheit/Unit\n        param_col = None\n        value_col = None\n        unit_col = None\n\n        for col in df.columns:\n            col_lower = col.lower().strip()\n            if \"parameter\" in col_lower or \"messgr\u00f6\u00dfe\" in col_lower:\n                param_col = col\n            elif \"wert\" in col_lower or \"value\" in col_lower:\n                value_col = col\n            elif \"einheit\" in col_lower or \"unit\" in col_lower:\n                unit_col = col\n                print(\"_parse_vertical_format: \", unit_col)\n\n        if not (param_col and value_col):\n            raise ValueError(\"Cannot parse vertical format: missing Parameter or Value column\")\n\n        # Create horizontal DataFrame\n        data = {}\n        for _, row in df.iterrows():\n            param = str(row[param_col]).strip()\n            value = row[value_col]\n\n            # Map parameter name\n            if param in self.COLUMN_MAPPINGS:\n                param = self.COLUMN_MAPPINGS[param]\n\n            data[param] = value\n\n        return pd.DataFrame([data])\n\n    def _validate_substrate_data(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Validate substrate data ranges.\n\n        Args:\n            data: Substrate data dict\n\n        Returns:\n            Validated data dict (with warnings for out-of-range values)\n        \"\"\"\n        # Expected ranges\n        ranges = {\n            \"TS\": (5.0, 95.0),  # % FM\n            \"VS\": (50.0, 100.0),  # % TS\n            \"RP\": (0.0, 40.0),  # % TS\n            \"RL\": (0.0, 20.0),  # % TS\n            \"RF\": (0.0, 50.0),  # % TS\n            \"NDF\": (10.0, 90.0),  # % TS\n            \"ADF\": (5.0, 70.0),  # % TS\n            \"ADL\": (0.0, 30.0),  # % TS\n            \"pH\": (3.0, 9.0),\n            \"NH4_N\": (0.0, 10.0),  # g/L\n            \"TAC\": (0.0, 500.0),  # mmol/L\n            \"BMP\": (50.0, 800.0),  # L CH4/kg oTS\n            \"C_to_N\": (5.0, 100.0),\n        }\n\n        for param, (min_val, max_val) in ranges.items():\n            if param in data:\n                value = data[param]\n                if value &lt; min_val or value &gt; max_val:\n                    warnings.warn(f\"Parameter '{param}' = {value} is outside expected range [{min_val}, {max_val}]\")\n\n        return data\n\n    def create_template_substrate_csv(self, filepath: str, format_type: str = \"horizontal\") -&gt; None:\n        \"\"\"\n        Create template CSV file for substrate data entry.\n\n        Args:\n            filepath: Output file path\n            format_type: \"horizontal\" or \"vertical\"\n\n        Example:\n            &gt;&gt;&gt; handler.create_template_substrate_csv(\"template.csv\")\n        \"\"\"\n        if format_type == \"horizontal\":\n            # One row per sample\n            template = pd.DataFrame(\n                columns=[\n                    \"substrate_name\",\n                    \"substrate_type\",\n                    \"sample_date\",\n                    \"TS\",\n                    \"VS\",\n                    \"oTS\",\n                    \"foTS\",\n                    \"RP\",\n                    \"RL\",\n                    \"RF\",\n                    \"NDF\",\n                    \"ADF\",\n                    \"ADL\",\n                    \"pH\",\n                    \"NH4_N\",\n                    \"TAC\",\n                    \"COD_S\",\n                    \"BMP\",\n                    \"C_content\",\n                    \"N_content\",\n                    \"C_to_N\",\n                ]\n            )\n\n            # Add example row\n            template.loc[0] = [\n                \"Maize silage\",\n                \"maize\",\n                \"2024-01-15\",\n                32.5,\n                96.2,\n                31.3,\n                28.5,\n                8.5,\n                3.2,\n                21.5,\n                42.1,\n                22.3,\n                2.1,\n                3.9,\n                0.5,\n                11.0,\n                18.5,\n                345.0,\n                45.2,\n                1.8,\n                25.1,\n            ]\n\n        else:  # vertical\n            template = pd.DataFrame(\n                {\n                    \"Parameter\": [\n                        \"Substrate name\",\n                        \"Substrate type\",\n                        \"TS\",\n                        \"VS\",\n                        \"RP\",\n                        \"RL\",\n                        \"NDF\",\n                        \"ADF\",\n                        \"ADL\",\n                        \"pH\",\n                        \"NH4-N\",\n                        \"TAC\",\n                        \"COD_S\",\n                        \"BMP\",\n                    ],\n                    \"Value\": [\"Maize silage\", \"maize\", 32.5, 96.2, 8.5, 3.2, 42.1, 22.3, 2.1, 3.9, 0.5, 11.0, 18.5, 345.0],\n                    \"Unit\": [\n                        \"\",\n                        \"\",\n                        \"% FM\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"-\",\n                        \"g/L\",\n                        \"mmol/L\",\n                        \"g/L\",\n                        \"L CH4/kg oTS\",\n                    ],\n                }\n            )\n\n        template.to_csv(filepath, index=False)\n        print(f\"\u2713 Created template CSV at {filepath}\")\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler-functions","title":"Functions","text":""},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.__init__","title":"<code>__init__(decimal_separator='.', thousands_separator=',')</code>","text":"<p>Initialize CSV handler.</p> <p>Parameters:</p> Name Type Description Default <code>decimal_separator</code> <code>str</code> <p>Decimal separator (\".\" or \",\")</p> <code>'.'</code> <code>thousands_separator</code> <code>str</code> <p>Thousands separator (\",\" or \".\" or \"\")</p> <code>','</code> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def __init__(self, decimal_separator: str = \".\", thousands_separator: str = \",\"):\n    \"\"\"\n    Initialize CSV handler.\n\n    Args:\n        decimal_separator: Decimal separator (\".\" or \",\")\n        thousands_separator: Thousands separator (\",\" or \".\" or \"\")\n    \"\"\"\n    self.decimal_separator = decimal_separator\n    self.thousands_separator = thousands_separator\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.create_template_substrate_csv","title":"<code>create_template_substrate_csv(filepath, format_type='horizontal')</code>","text":"<p>Create template CSV file for substrate data entry.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>format_type</code> <code>str</code> <p>\"horizontal\" or \"vertical\"</p> <code>'horizontal'</code> Example <p>handler.create_template_substrate_csv(\"template.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def create_template_substrate_csv(self, filepath: str, format_type: str = \"horizontal\") -&gt; None:\n    \"\"\"\n    Create template CSV file for substrate data entry.\n\n    Args:\n        filepath: Output file path\n        format_type: \"horizontal\" or \"vertical\"\n\n    Example:\n        &gt;&gt;&gt; handler.create_template_substrate_csv(\"template.csv\")\n    \"\"\"\n    if format_type == \"horizontal\":\n        # One row per sample\n        template = pd.DataFrame(\n            columns=[\n                \"substrate_name\",\n                \"substrate_type\",\n                \"sample_date\",\n                \"TS\",\n                \"VS\",\n                \"oTS\",\n                \"foTS\",\n                \"RP\",\n                \"RL\",\n                \"RF\",\n                \"NDF\",\n                \"ADF\",\n                \"ADL\",\n                \"pH\",\n                \"NH4_N\",\n                \"TAC\",\n                \"COD_S\",\n                \"BMP\",\n                \"C_content\",\n                \"N_content\",\n                \"C_to_N\",\n            ]\n        )\n\n        # Add example row\n        template.loc[0] = [\n            \"Maize silage\",\n            \"maize\",\n            \"2024-01-15\",\n            32.5,\n            96.2,\n            31.3,\n            28.5,\n            8.5,\n            3.2,\n            21.5,\n            42.1,\n            22.3,\n            2.1,\n            3.9,\n            0.5,\n            11.0,\n            18.5,\n            345.0,\n            45.2,\n            1.8,\n            25.1,\n        ]\n\n    else:  # vertical\n        template = pd.DataFrame(\n            {\n                \"Parameter\": [\n                    \"Substrate name\",\n                    \"Substrate type\",\n                    \"TS\",\n                    \"VS\",\n                    \"RP\",\n                    \"RL\",\n                    \"NDF\",\n                    \"ADF\",\n                    \"ADL\",\n                    \"pH\",\n                    \"NH4-N\",\n                    \"TAC\",\n                    \"COD_S\",\n                    \"BMP\",\n                ],\n                \"Value\": [\"Maize silage\", \"maize\", 32.5, 96.2, 8.5, 3.2, 42.1, 22.3, 2.1, 3.9, 0.5, 11.0, 18.5, 345.0],\n                \"Unit\": [\n                    \"\",\n                    \"\",\n                    \"% FM\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"-\",\n                    \"g/L\",\n                    \"mmol/L\",\n                    \"g/L\",\n                    \"L CH4/kg oTS\",\n                ],\n            }\n        )\n\n    template.to_csv(filepath, index=False)\n    print(f\"\u2713 Created template CSV at {filepath}\")\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.export_measurement_data","title":"<code>export_measurement_data(data, filepath, sep=',', encoding='utf-8', include_index=True)</code>","text":"<p>Export measurement data to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with measurements</p> required <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>include_index</code> <code>bool</code> <p>Include index (timestamp) in output</p> <code>True</code> Example <p>handler.export_measurement_data(measurements, \"export.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def export_measurement_data(\n    self, data: pd.DataFrame, filepath: str, sep: str = \",\", encoding: str = \"utf-8\", include_index: bool = True\n) -&gt; None:\n    \"\"\"\n    Export measurement data to CSV.\n\n    Args:\n        data: DataFrame with measurements\n        filepath: Output file path\n        sep: Column separator\n        encoding: File encoding\n        include_index: Include index (timestamp) in output\n\n    Example:\n        &gt;&gt;&gt; handler.export_measurement_data(measurements, \"export.csv\")\n    \"\"\"\n    data.to_csv(filepath, sep=sep, encoding=encoding, index=include_index)\n    print(f\"\u2713 Exported measurement data to {filepath} ({len(data)} rows)\")\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.export_parameter_table","title":"<code>export_parameter_table(data, filepath, sep=',', encoding='utf-8')</code>","text":"<p>Export parameter table to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with parameters</p> required <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Example <p>handler.export_parameter_table(params_df, \"parameters.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def export_parameter_table(self, data: pd.DataFrame, filepath: str, sep: str = \",\", encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"\n    Export parameter table to CSV.\n\n    Args:\n        data: DataFrame with parameters\n        filepath: Output file path\n        sep: Column separator\n        encoding: File encoding\n\n    Example:\n        &gt;&gt;&gt; handler.export_parameter_table(params_df, \"parameters.csv\")\n    \"\"\"\n    data.to_csv(filepath, sep=sep, encoding=encoding)\n    print(f\"\u2713 Exported parameter table to {filepath}\")\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.export_simulation_results","title":"<code>export_simulation_results(results, filepath, sep=',', encoding='utf-8', flatten_components=True)</code>","text":"<p>Export simulation results to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of result dicts from plant.simulate()</p> required <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>flatten_components</code> <code>bool</code> <p>Flatten component results into columns</p> <code>True</code> Example <p>results = plant.simulate(duration=30, dt=1/24) handler.export_simulation_results(results, \"simulation.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def export_simulation_results(\n    self,\n    results: List[Dict[str, Any]],\n    filepath: str,\n    sep: str = \",\",\n    encoding: str = \"utf-8\",\n    flatten_components: bool = True,\n) -&gt; None:\n    \"\"\"\n    Export simulation results to CSV.\n\n    Args:\n        results: List of result dicts from plant.simulate()\n        filepath: Output file path\n        sep: Column separator\n        encoding: File encoding\n        flatten_components: Flatten component results into columns\n\n    Example:\n        &gt;&gt;&gt; results = plant.simulate(duration=30, dt=1/24)\n        &gt;&gt;&gt; handler.export_simulation_results(results, \"simulation.csv\")\n    \"\"\"\n    if not results:\n        warnings.warn(\"No results to export\")\n        return\n\n    # Convert to DataFrame\n    if flatten_components:\n        # Flatten structure: time, component1_metric1, component1_metric2, ...\n        rows = []\n        for result in results:\n            row = {\"time\": result[\"time\"]}\n\n            for comp_id, comp_data in result[\"components\"].items():\n                for metric, value in comp_data.items():\n                    # Skip nested dicts (like gas_storage)\n                    if isinstance(value, dict):\n                        continue\n                    col_name = f\"{comp_id}_{metric}\"\n                    row[col_name] = value\n\n            rows.append(row)\n\n        df = pd.DataFrame(rows)\n    else:\n        # Simple format: just time and first component's data\n        first_comp_id = list(results[0][\"components\"].keys())[0]\n        rows = []\n        for result in results:\n            row = {\"time\": result[\"time\"]}\n            row.update(result[\"components\"][first_comp_id])\n            # Remove nested dicts\n            row = {k: v for k, v in row.items() if not isinstance(v, dict)}\n            rows.append(row)\n\n        df = pd.DataFrame(rows)\n\n    # Export\n    df.to_csv(filepath, sep=sep, encoding=encoding, index=False)\n    print(f\"\u2713 Exported simulation results to {filepath} ({len(df)} time points)\")\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.export_substrate_data","title":"<code>export_substrate_data(data, filepath, sep=',', encoding='utf-8')</code>","text":"<p>Export substrate data to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Dict[str, Any], DataFrame]</code> <p>Dict or DataFrame with substrate data</p> required <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Example <p>handler.export_substrate_data(substrate_data, \"export.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def export_substrate_data(\n    self, data: Union[Dict[str, Any], pd.DataFrame], filepath: str, sep: str = \",\", encoding: str = \"utf-8\"\n) -&gt; None:\n    \"\"\"\n    Export substrate data to CSV.\n\n    Args:\n        data: Dict or DataFrame with substrate data\n        filepath: Output file path\n        sep: Column separator\n        encoding: File encoding\n\n    Example:\n        &gt;&gt;&gt; handler.export_substrate_data(substrate_data, \"export.csv\")\n    \"\"\"\n    if isinstance(data, dict):\n        df = pd.DataFrame([data])\n    else:\n        df = data\n\n    df.to_csv(filepath, sep=sep, encoding=encoding, index=False)\n    print(f\"\u2713 Exported substrate data to {filepath}\")\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.load_measurement_data","title":"<code>load_measurement_data(filepath, timestamp_column='timestamp', sep=',', encoding='utf-8', parse_dates=True, resample=None)</code>","text":"<p>Load time series measurement data from CSV.</p> <p>Expected columns: - timestamp (or Zeit, Zeitstempel) - Q_sub_* (substrate feeds) - pH, VFA, TAC, FOS_TAC - T_digester - Q_gas, Q_ch4, Q_co2, CH4_content, P_gas - P_el, P_th</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>timestamp_column</code> <code>str</code> <p>Name of timestamp column</p> <code>'timestamp'</code> <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>parse_dates</code> <code>bool</code> <p>Parse timestamp column</p> <code>True</code> <code>resample</code> <code>Optional[str]</code> <p>Resample frequency (e.g., \"1h\", \"1d\")</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with measurements</p> Example <p>handler = CSVHandler() data = handler.load_measurement_data( ...     \"plant_data.csv\", ...     resample=\"1h\" ... )</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_measurement_data(\n    self,\n    filepath: str,\n    timestamp_column: str = \"timestamp\",\n    sep: str = \",\",\n    encoding: str = \"utf-8\",\n    parse_dates: bool = True,\n    resample: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load time series measurement data from CSV.\n\n    Expected columns:\n    - timestamp (or Zeit, Zeitstempel)\n    - Q_sub_* (substrate feeds)\n    - pH, VFA, TAC, FOS_TAC\n    - T_digester\n    - Q_gas, Q_ch4, Q_co2, CH4_content, P_gas\n    - P_el, P_th\n\n    Args:\n        filepath: Path to CSV file\n        timestamp_column: Name of timestamp column\n        sep: Column separator\n        encoding: File encoding\n        parse_dates: Parse timestamp column\n        resample: Resample frequency (e.g., \"1h\", \"1d\")\n\n    Returns:\n        DataFrame with measurements\n\n    Example:\n        &gt;&gt;&gt; handler = CSVHandler()\n        &gt;&gt;&gt; data = handler.load_measurement_data(\n        ...     \"plant_data.csv\",\n        ...     resample=\"1h\"\n        ... )\n    \"\"\"\n    # Auto-detect separator\n    if sep == \"auto\":\n        sep = self._detect_separator(filepath)\n\n    # Read CSV\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n    # Map column names\n    df = self._map_column_names(df)\n\n    # Parse timestamp\n    if timestamp_column in df.columns:\n        if parse_dates:\n            df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n        df = df.set_index(timestamp_column).sort_index()\n\n    # Resample if requested\n    if resample is not None:\n        df = df.resample(resample).mean()\n\n    return df\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.load_multiple_substrate_samples","title":"<code>load_multiple_substrate_samples(filepath, sep=',', encoding='utf-8', date_column='sample_date', name_column='substrate_name')</code>","text":"<p>Load multiple substrate samples from CSV.</p> <p>Expected format: Each row is one sample with columns for all parameters.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>date_column</code> <code>str</code> <p>Name of date column</p> <code>'sample_date'</code> <code>name_column</code> <code>str</code> <p>Name of substrate name column</p> <code>'substrate_name'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with substrate data</p> Example <p>handler = CSVHandler() samples = handler.load_multiple_substrate_samples( ...     \"substrate_database.csv\" ... ) print(samples.head())</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_multiple_substrate_samples(\n    self,\n    filepath: str,\n    sep: str = \",\",\n    encoding: str = \"utf-8\",\n    date_column: str = \"sample_date\",\n    name_column: str = \"substrate_name\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load multiple substrate samples from CSV.\n\n    Expected format: Each row is one sample with columns for all parameters.\n\n    Args:\n        filepath: Path to CSV file\n        sep: Column separator\n        encoding: File encoding\n        date_column: Name of date column\n        name_column: Name of substrate name column\n\n    Returns:\n        DataFrame with substrate data\n\n    Example:\n        &gt;&gt;&gt; handler = CSVHandler()\n        &gt;&gt;&gt; samples = handler.load_multiple_substrate_samples(\n        ...     \"substrate_database.csv\"\n        ... )\n        &gt;&gt;&gt; print(samples.head())\n    \"\"\"\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n    # Map column names\n    df = self._map_column_names(df)\n\n    # Parse date column\n    if date_column in df.columns:\n        df[date_column] = pd.to_datetime(df[date_column])\n\n    return df\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.load_parameter_table","title":"<code>load_parameter_table(filepath, sep=',', encoding='utf-8', index_col=None)</code>","text":"<p>Load parameter table from CSV.</p> <p>Expected format: - Rows: Parameters - Columns: Different scenarios/substrates</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>index_col</code> <code>Optional[str]</code> <p>Column to use as index (usually parameter name)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with parameters</p> Example <p>params = handler.load_parameter_table(\"parameters.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_parameter_table(\n    self, filepath: str, sep: str = \",\", encoding: str = \"utf-8\", index_col: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load parameter table from CSV.\n\n    Expected format:\n    - Rows: Parameters\n    - Columns: Different scenarios/substrates\n\n    Args:\n        filepath: Path to CSV file\n        sep: Column separator\n        encoding: File encoding\n        index_col: Column to use as index (usually parameter name)\n\n    Returns:\n        DataFrame with parameters\n\n    Example:\n        &gt;&gt;&gt; params = handler.load_parameter_table(\"parameters.csv\")\n    \"\"\"\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding, index_col=index_col)\n    return df\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.load_simulation_results","title":"<code>load_simulation_results(filepath, sep=',', encoding='utf-8')</code>","text":"<p>Load simulation results from CSV.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of result dicts</p> Example <p>results = handler.load_simulation_results(\"simulation.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_simulation_results(self, filepath: str, sep: str = \",\", encoding: str = \"utf-8\") -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Load simulation results from CSV.\n\n    Args:\n        filepath: Path to CSV file\n        sep: Column separator\n        encoding: File encoding\n\n    Returns:\n        List of result dicts\n\n    Example:\n        &gt;&gt;&gt; results = handler.load_simulation_results(\"simulation.csv\")\n    \"\"\"\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n    # Convert back to results format\n    results = []\n    for _, row in df.iterrows():\n        result = {\"time\": row[\"time\"], \"components\": {}}\n\n        # Group columns by component\n        for col in df.columns:\n            if col == \"time\":\n                continue\n\n            if \"_\" in col:\n                comp_id, metric = col.split(\"_\", 1)\n                if comp_id not in result[\"components\"]:\n                    result[\"components\"][comp_id] = {}\n                result[\"components\"][comp_id][metric] = row[col]\n\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.CSVHandler.load_substrate_lab_data","title":"<code>load_substrate_lab_data(filepath, substrate_name=None, substrate_type=None, sample_date=None, sep=',', encoding='utf-8', validate=True)</code>","text":"<p>Load substrate characterization data from laboratory CSV.</p> <p>Expected columns (German or English): - Trockensubstanzgehalt (TS) [% FM] - Organische Trockensubstanz (VS) [% TS] - Fermentierbare organische Trockensubstanz (foTS) [% TS] - Rohprotein (RP) [% TS] - Rohfett (RL) [% TS] - Rohfaser (RF) [% TS] - NDF, ADF, ADL [% TS] - pH-Wert (pH) - Ammoniumstickstoff (NH4-N) [g/L or mg/L] - Alkalinit\u00e4t (TAC) [mmol/L] - Biochemisches Methanpotential (BMP) [L CH4/kg oTS] - CSB des Filtrats (COD_S) [g/L]</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>substrate_name</code> <code>Optional[str]</code> <p>Substrate name (if not in file)</p> <code>None</code> <code>substrate_type</code> <code>Optional[str]</code> <p>Substrate type (maize, manure, grass, etc.)</p> <code>None</code> <code>sample_date</code> <code>Optional[Union[str, datetime]]</code> <p>Sample date (if not in file)</p> <code>None</code> <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>validate</code> <code>bool</code> <p>Validate data ranges</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with substrate data</p> Example <p>handler = CSVHandler() data = handler.load_substrate_lab_data( ...     \"maize_analysis.csv\", ...     substrate_name=\"Maize silage batch 23\", ...     substrate_type=\"maize\", ...     sample_date=\"2024-01-15\" ... ) print(f\"TS: {data['TS']:.1f}% FM\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_substrate_lab_data(\n    self,\n    filepath: str,\n    substrate_name: Optional[str] = None,\n    substrate_type: Optional[str] = None,\n    sample_date: Optional[Union[str, datetime]] = None,\n    sep: str = \",\",\n    encoding: str = \"utf-8\",\n    validate: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load substrate characterization data from laboratory CSV.\n\n    Expected columns (German or English):\n    - Trockensubstanzgehalt (TS) [% FM]\n    - Organische Trockensubstanz (VS) [% TS]\n    - Fermentierbare organische Trockensubstanz (foTS) [% TS]\n    - Rohprotein (RP) [% TS]\n    - Rohfett (RL) [% TS]\n    - Rohfaser (RF) [% TS]\n    - NDF, ADF, ADL [% TS]\n    - pH-Wert (pH)\n    - Ammoniumstickstoff (NH4-N) [g/L or mg/L]\n    - Alkalinit\u00e4t (TAC) [mmol/L]\n    - Biochemisches Methanpotential (BMP) [L CH4/kg oTS]\n    - CSB des Filtrats (COD_S) [g/L]\n\n    Args:\n        filepath: Path to CSV file\n        substrate_name: Substrate name (if not in file)\n        substrate_type: Substrate type (maize, manure, grass, etc.)\n        sample_date: Sample date (if not in file)\n        sep: Column separator\n        encoding: File encoding\n        validate: Validate data ranges\n\n    Returns:\n        Dict with substrate data\n\n    Example:\n        &gt;&gt;&gt; handler = CSVHandler()\n        &gt;&gt;&gt; data = handler.load_substrate_lab_data(\n        ...     \"maize_analysis.csv\",\n        ...     substrate_name=\"Maize silage batch 23\",\n        ...     substrate_type=\"maize\",\n        ...     sample_date=\"2024-01-15\"\n        ... )\n        &gt;&gt;&gt; print(f\"TS: {data['TS']:.1f}% FM\")\n    \"\"\"\n    # Auto-detect separator if needed\n    if sep == \"auto\":\n        sep = self._detect_separator(filepath)\n\n    # Read CSV\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n    # Try to detect if file is in \"vertical\" format (parameter, value, unit)\n    if len(df.columns) &lt;= 3 and \"Parameter\" in df.columns or \"Messgr\u00f6\u00dfe\" in df.columns:\n        df = self._parse_vertical_format(df)\n\n    # Map column names\n    df = self._map_column_names(df)\n\n    # If multiple rows, take the first one (or could aggregate)\n    if len(df) &gt; 1:\n        warnings.warn(f\"CSV contains {len(df)} rows, using first row only\")\n\n    row = df.iloc[0]\n\n    # Extract data\n    result = {\n        \"substrate_name\": substrate_name or row.get(\"substrate_name\", \"Unknown\"),\n        \"substrate_type\": substrate_type or row.get(\"substrate_type\", \"unknown\"),\n        \"sample_date\": sample_date or row.get(\"sample_date\", datetime.now()),\n    }\n\n    # Add all available parameters\n    for param in [\n        \"TS\",\n        \"VS\",\n        \"oTS\",\n        \"foTS\",\n        \"RP\",\n        \"RL\",\n        \"RF\",\n        \"RA\",\n        \"NfE\",\n        \"NDF\",\n        \"ADF\",\n        \"ADL\",\n        \"pH\",\n        \"NH4_N\",\n        \"TAC\",\n        \"COD\",\n        \"COD_S\",\n        \"BMP\",\n        \"C_content\",\n        \"N_content\",\n        \"C_to_N\",\n        \"TKN\",\n    ]:\n        if param in df.columns:\n            value = row[param]\n            # Handle both scalar values and Series\n            if isinstance(value, pd.Series):\n                value = value.iloc[0] if len(value) &gt; 0 else None\n            if pd.notna(value):\n                result[param] = float(value)\n\n    # Validate if requested\n    if validate:\n        result = self._validate_substrate_data(result)\n\n    # TODO: diese Substratparameter m\u00fcssen in die substrate_....xml geschrieben werden. evtl. gibt es in einer\n    #  c# DLL auch bereits eine Methode die man aufrufen kann. glaube aber eher nicht\n\n    return result\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.Database","title":"<code>pyadm1ode_calibration.io.Database</code>","text":"<p>PostgreSQL database interface for PyADM1.</p> Source code in <code>pyadm1ode_calibration/io/persistence/database.py</code> <pre><code>class Database:\n    \"\"\"\n    PostgreSQL database interface for PyADM1.\n    \"\"\"\n\n    def __init__(self, connection_string: Optional[str] = None, config: Optional[DatabaseConfig] = None):\n        self.connection_manager = ConnectionManager(connection_string, config)\n        self.engine = self.connection_manager.engine\n        self.SessionLocal = self.connection_manager.SessionLocal\n        self.connection_string = self.connection_manager.connection_string\n\n    @classmethod\n    def from_env(cls, prefix: str = \"DB\") -&gt; \"Database\":\n        \"\"\"Create database from environment variables.\"\"\"\n        import os\n        from urllib.parse import quote_plus\n\n        host = os.getenv(f\"{prefix}_HOST\", \"localhost\")\n        port = os.getenv(f\"{prefix}_PORT\", \"5432\")\n        database = os.getenv(f\"{prefix}_NAME\")\n        username = os.getenv(f\"{prefix}_USER\")\n        password = os.getenv(f\"{prefix}_PASSWORD\")\n\n        if not all([database, username, password]):\n            raise ValueError(\"Missing required environment variables\")\n\n        conn_str = f\"postgresql://{username}:{quote_plus(password)}@{host}:{port}/{database}\"\n        return cls(connection_string=conn_str)\n\n    @contextmanager\n    def get_session(self) -&gt; Session:\n        session = self.SessionLocal()\n        try:\n            yield session\n            session.commit()\n        except Exception:\n            session.rollback()\n            raise\n        finally:\n            session.close()\n\n    def create_all_tables(self) -&gt; None:\n        Base.metadata.create_all(bind=self.engine)\n\n    def drop_all_tables(self) -&gt; None:\n        Base.metadata.drop_all(bind=self.engine)\n\n    def create_plant(\n        self,\n        plant_id: str,\n        name: str,\n        location: Optional[str] = None,\n        operator: Optional[str] = None,\n        V_liq: Optional[float] = None,\n        V_gas: Optional[float] = None,\n        T_ad: Optional[float] = None,\n        P_el_nom: Optional[float] = None,\n        configuration: Optional[Dict] = None,\n    ) -&gt; Plant:\n        session = self.SessionLocal()\n        try:\n            plant = Plant(\n                id=plant_id,\n                name=name,\n                location=location,\n                operator=operator,\n                V_liq=V_liq,\n                V_gas=V_gas,\n                T_ad=T_ad,\n                P_el_nom=P_el_nom,\n                configuration=configuration,\n            )\n            session.add(plant)\n            session.commit()\n            session.refresh(plant)\n            session.expunge(plant)\n            return plant\n        except IntegrityError:\n            session.rollback()\n            raise ValueError(f\"Plant with ID '{plant_id}' already exists\")\n        finally:\n            session.close()\n\n    def get_plant(self, plant_id: str) -&gt; Plant:\n        session = self.SessionLocal()\n        try:\n            plant = session.query(Plant).filter(Plant.id == plant_id).first()\n            if plant is None:\n                raise ValueError(f\"Plant '{plant_id}' not found\")\n            session.expunge(plant)\n            return plant\n        except SQLAlchemyError as e:\n            raise DatabaseError(f\"Failed to retrieve plant '{plant_id}': {e}\")\n        finally:\n            session.close()\n\n    def list_plants(self) -&gt; List[Dict[str, Any]]:\n        with self.get_session() as session:\n            plants = session.query(Plant).all()\n            return [\n                {\n                    \"id\": p.id,\n                    \"name\": p.name,\n                    \"location\": p.location,\n                    \"V_liq\": p.V_liq,\n                    \"V_gas\": p.V_gas,\n                    \"T_ad\": p.T_ad,\n                    \"created_at\": p.created_at,\n                }\n                for p in plants\n            ]\n\n    def store_measurements(self, plant_id: str, data: pd.DataFrame, source: str = \"SCADA\", validate: bool = True) -&gt; int:\n        self.get_plant(plant_id)\n        if \"timestamp\" not in data.columns:\n            raise ValueError(\"DataFrame must have 'timestamp' column\")\n        if not pd.api.types.is_datetime64_any_dtype(data[\"timestamp\"]):\n            data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n        if validate:\n            from ..validation.validators import DataValidator\n\n            validation = DataValidator.validate(data)\n            if not validation.is_valid:\n                pass\n        records = []\n        for _, row in data.iterrows():\n            record = {\"plant_id\": plant_id, \"timestamp\": row[\"timestamp\"], \"source\": source}\n            for col in data.columns:\n                if col != \"timestamp\" and col in Measurement.__table__.columns:\n                    val = row[col]\n                    if pd.notna(val):\n                        record[col] = float(val) if isinstance(val, (int, float, np.number)) else val\n            records.append(record)\n        with self.get_session() as session:\n            try:\n                session.bulk_insert_mappings(Measurement, records)\n                return len(records)\n            except SQLAlchemyError as e:\n                raise DatabaseError(f\"Failed to store measurements: {e}\")\n\n    def load_measurements(self, plant_id: str, start_time=None, end_time=None, columns=None, source=None) -&gt; pd.DataFrame:\n        if isinstance(start_time, str):\n            start_time = pd.to_datetime(start_time)\n        if isinstance(end_time, str):\n            end_time = pd.to_datetime(end_time)\n        with self.get_session() as session:\n            query = session.query(Measurement).filter(Measurement.plant_id == plant_id)\n            if start_time:\n                query = query.filter(Measurement.timestamp &gt;= start_time)\n            if end_time:\n                query = query.filter(Measurement.timestamp &lt;= end_time)\n            if source:\n                query = query.filter(Measurement.source == source)\n            results = query.order_by(Measurement.timestamp).all()\n            if not results:\n                return pd.DataFrame()\n            data_dict = {\"timestamp\": [r.timestamp for r in results]}\n            if columns is None:\n                columns = [\n                    c.name\n                    for c in Measurement.__table__.columns\n                    if c.name not in [\"id\", \"plant_id\", \"timestamp\", \"source\", \"created_at\"]\n                ]\n            for col in columns:\n                data_dict[col] = [getattr(r, col) for r in results]\n            return pd.DataFrame(data_dict).set_index(\"timestamp\")\n\n    def store_simulation(\n        self,\n        simulation_id: str,\n        plant_id: str,\n        results: List[Dict[str, Any]],\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        duration: Optional[float] = None,\n        parameters: Optional[Dict] = None,\n        scenario: str = \"baseline\",\n    ) -&gt; Simulation:\n        self.get_plant(plant_id)\n        metrics = self._calculate_simulation_metrics(results)\n        with self.get_session() as session:\n            sim = Simulation(\n                id=simulation_id,\n                plant_id=plant_id,\n                name=name,\n                description=description,\n                duration=duration or (results[-1][\"time\"] if results else 0),\n                scenario=scenario,\n                parameters=parameters,\n                avg_Q_gas=metrics.get(\"avg_Q_gas\"),\n                avg_Q_ch4=metrics.get(\"avg_Q_ch4\"),\n                avg_CH4_content=metrics.get(\"avg_CH4_content\"),\n                avg_pH=metrics.get(\"avg_pH\"),\n                avg_VFA=metrics.get(\"avg_VFA\"),\n                total_energy=metrics.get(\"total_energy\"),\n                status=\"completed\",\n                started_at=datetime.utcnow(),\n                completed_at=datetime.utcnow(),\n            )\n            try:\n                session.add(sim)\n                session.flush()\n                ts_records = []\n                for res in results:\n                    comp_data = next(iter(res[\"components\"].values()))\n                    record = {\n                        \"simulation_id\": simulation_id,\n                        \"time\": res[\"time\"],\n                        \"Q_gas\": comp_data.get(\"Q_gas\"),\n                        \"Q_ch4\": comp_data.get(\"Q_ch4\"),\n                        \"Q_co2\": comp_data.get(\"Q_co2\"),\n                        \"pH\": comp_data.get(\"pH\"),\n                        \"VFA\": comp_data.get(\"VFA\"),\n                        \"TAC\": comp_data.get(\"TAC\"),\n                    }\n                    if record[\"Q_gas\"] and record[\"Q_ch4\"] and record[\"Q_gas\"] &gt; 0:\n                        record[\"CH4_content\"] = (record[\"Q_ch4\"] / record[\"Q_gas\"]) * 100\n                    ts_records.append(record)\n                session.bulk_insert_mappings(SimulationTimeSeries, ts_records)\n                return sim\n            except IntegrityError:\n                raise ValueError(f\"Simulation with ID '{simulation_id}' already exists\")\n\n    def load_simulation(self, simulation_id: str) -&gt; Optional[Dict[str, Any]]:\n        with self.get_session() as session:\n            sim = session.query(Simulation).filter(Simulation.id == simulation_id).first()\n            if not sim:\n                return None\n            ts = (\n                session.query(SimulationTimeSeries)\n                .filter(SimulationTimeSeries.simulation_id == simulation_id)\n                .order_by(SimulationTimeSeries.time)\n                .all()\n            )\n            df = (\n                pd.DataFrame(\n                    {\n                        c.name: [getattr(t, c.name) for t in ts]\n                        for c in SimulationTimeSeries.__table__.columns\n                        if c.name not in [\"id\", \"simulation_id\"]\n                    }\n                )\n                if ts\n                else pd.DataFrame()\n            )\n            return {**{c.name: getattr(sim, c.name) for c in Simulation.__table__.columns}, \"time_series\": df}\n\n    def list_simulations(self, plant_id: Optional[str] = None, scenario: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n        with self.get_session() as session:\n            query = session.query(Simulation)\n            if plant_id:\n                query = query.filter(Simulation.plant_id == plant_id)\n            if scenario:\n                query = query.filter(Simulation.scenario == scenario)\n            simulations = query.order_by(Simulation.created_at.desc()).all()\n            return [\n                {\n                    \"id\": s.id,\n                    \"plant_id\": s.plant_id,\n                    \"name\": s.name,\n                    \"scenario\": s.scenario,\n                    \"duration\": s.duration,\n                    \"avg_Q_ch4\": s.avg_Q_ch4,\n                    \"status\": s.status,\n                    \"created_at\": s.created_at,\n                }\n                for s in simulations\n            ]\n\n    def store_calibration(\n        self,\n        plant_id: str,\n        calibration_type: str,\n        method: str,\n        parameters: Dict[str, float],\n        objective_value: float,\n        objectives: List[str],\n        validation_metrics: Optional[Dict[str, float]] = None,\n        data_start: Optional[datetime] = None,\n        data_end: Optional[datetime] = None,\n        success: bool = True,\n        message: Optional[str] = None,\n    ) -&gt; Calibration:\n        with self.get_session() as session:\n            cal = Calibration(\n                plant_id=plant_id,\n                calibration_type=calibration_type,\n                method=method,\n                parameters=parameters,\n                objective_value=objective_value,\n                objectives=objectives,\n                validation_metrics=validation_metrics,\n                data_start=data_start,\n                data_end=data_end,\n                success=success,\n                message=message,\n            )\n            session.add(cal)\n            return cal\n\n    def load_calibrations(\n        self, plant_id: str, calibration_type: Optional[str] = None, limit: int = 10\n    ) -&gt; List[Dict[str, Any]]:\n        with self.get_session() as session:\n            query = session.query(Calibration).filter(Calibration.plant_id == plant_id)\n            if calibration_type:\n                query = query.filter(Calibration.calibration_type == calibration_type)\n            cals = query.order_by(Calibration.created_at.desc()).limit(limit).all()\n            return [{c.name: getattr(cal, c.name) for c in Calibration.__table__.columns} for cal in cals]\n\n    def get_latest_calibration(self, plant_id: str) -&gt; Optional[Dict[str, Any]]:\n        cals = self.load_calibrations(plant_id, limit=1)\n        return cals[0] if cals else None\n\n    def store_substrate(\n        self,\n        plant_id: str,\n        substrate_name: str,\n        substrate_type: str,\n        sample_date: Union[str, datetime],\n        lab_data: Dict[str, float],\n        sample_id: Optional[str] = None,\n        lab_name: Optional[str] = None,\n        notes: Optional[str] = None,\n    ) -&gt; Substrate:\n        if isinstance(sample_date, str):\n            sample_date = pd.to_datetime(sample_date)\n        session = self.SessionLocal()\n        try:\n            substrate = Substrate(\n                plant_id=plant_id,\n                substrate_name=substrate_name,\n                substrate_type=substrate_type,\n                sample_date=sample_date,\n                sample_id=sample_id,\n                lab_name=lab_name,\n                notes=notes,\n            )\n            for key, value in lab_data.items():\n                if hasattr(substrate, key):\n                    setattr(substrate, key, value)\n            session.add(substrate)\n            session.commit()\n            session.refresh(substrate)\n            session.expunge(substrate)\n            return substrate\n        except Exception:\n            session.rollback()\n            raise\n        finally:\n            session.close()\n\n    def load_substrates(\n        self,\n        plant_id: str,\n        substrate_type: Optional[str] = None,\n        start_date: Optional[Union[str, datetime]] = None,\n        end_date: Optional[Union[str, datetime]] = None,\n    ) -&gt; pd.DataFrame:\n        if isinstance(start_date, str):\n            start_date = pd.to_datetime(start_date)\n        if isinstance(end_date, str):\n            end_date = pd.to_datetime(end_date)\n        with self.get_session() as session:\n            query = session.query(Substrate).filter(Substrate.plant_id == plant_id)\n            if substrate_type:\n                query = query.filter(Substrate.substrate_type == substrate_type)\n            if start_date:\n                query = query.filter(Substrate.sample_date &gt;= start_date)\n            if end_date:\n                query = query.filter(Substrate.sample_date &lt;= end_date)\n            substrates = query.order_by(Substrate.sample_date).all()\n            if not substrates:\n                return pd.DataFrame()\n            cols = [\n                \"sample_date\",\n                \"substrate_name\",\n                \"substrate_type\",\n                \"sample_id\",\n                \"TS\",\n                \"VS\",\n                \"oTS\",\n                \"foTS\",\n                \"RP\",\n                \"RL\",\n                \"RF\",\n                \"NDF\",\n                \"ADF\",\n                \"ADL\",\n                \"pH\",\n                \"NH4_N\",\n                \"TAC\",\n                \"COD_S\",\n                \"BMP\",\n                \"C_to_N\",\n                \"lab_name\",\n            ]\n            return pd.DataFrame([{c: getattr(s, c) for c in cols} for s in substrates])\n\n    def _calculate_simulation_metrics(self, results: List[Dict[str, Any]]) -&gt; Dict[str, float]:\n        if not results:\n            return {}\n        q_gas, q_ch4, ph, vfa, p_el = [], [], [], [], []\n        for res in results:\n            comp_data = next(iter(res[\"components\"].values()))\n            if \"Q_gas\" in comp_data:\n                q_gas.append(comp_data[\"Q_gas\"])\n            if \"Q_ch4\" in comp_data:\n                q_ch4.append(comp_data[\"Q_ch4\"])\n            if \"pH\" in comp_data:\n                ph.append(comp_data[\"pH\"])\n            if \"VFA\" in comp_data:\n                vfa.append(comp_data[\"VFA\"])\n            for comp_res in res[\"components\"].values():\n                if \"P_el\" in comp_res:\n                    p_el.append(comp_res[\"P_el\"])\n        metrics = {}\n        if q_gas:\n            metrics[\"avg_Q_gas\"] = float(np.mean(q_gas))\n        if q_ch4:\n            metrics[\"avg_Q_ch4\"] = float(np.mean(q_ch4))\n            if q_gas:\n                metrics[\"avg_CH4_content\"] = float(np.mean(q_ch4) / np.mean(q_gas) * 100)\n        if ph:\n            metrics[\"avg_pH\"] = float(np.mean(ph))\n        if vfa:\n            metrics[\"avg_VFA\"] = float(np.mean(vfa))\n        if p_el:\n            metrics[\"total_energy\"] = float(np.mean(p_el) * results[-1][\"time\"] * 24)\n        return metrics\n\n    def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n        dangerous = [\"DROP\", \"DELETE\", \"TRUNCATE\", \"ALTER\"]\n        if any(kw in query.upper() for kw in dangerous):\n            raise ValueError(\"Dangerous keyword detected in query\")\n        return pd.read_sql(query, self.engine, params=params)\n\n    def get_statistics(self, plant_id: str) -&gt; Dict[str, Any]:\n        with self.get_session() as session:\n            return {\n                \"plant_id\": plant_id,\n                \"n_measurements\": session.query(Measurement).filter(Measurement.plant_id == plant_id).count(),\n                \"n_simulations\": session.query(Simulation).filter(Simulation.plant_id == plant_id).count(),\n                \"n_calibrations\": session.query(Calibration).filter(Calibration.plant_id == plant_id).count(),\n                \"n_substrates\": session.query(Substrate).filter(Substrate.plant_id == plant_id).count(),\n                \"first_measurement\": (\n                    session.query(Measurement.timestamp)\n                    .filter(Measurement.plant_id == plant_id)\n                    .order_by(Measurement.timestamp)\n                    .first()[0]\n                    if session.query(Measurement.timestamp).filter(Measurement.plant_id == plant_id).count() &gt; 0\n                    else None\n                ),\n                \"last_measurement\": (\n                    session.query(Measurement.timestamp)\n                    .filter(Measurement.plant_id == plant_id)\n                    .order_by(Measurement.timestamp.desc())\n                    .first()[0]\n                    if session.query(Measurement.timestamp).filter(Measurement.plant_id == plant_id).count() &gt; 0\n                    else None\n                ),\n            }\n\n    def close(self) -&gt; None:\n        self.engine.dispose()\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.Database-functions","title":"Functions","text":""},{"location":"api/#pyadm1ode_calibration.io.Database.from_env","title":"<code>from_env(prefix='DB')</code>  <code>classmethod</code>","text":"<p>Create database from environment variables.</p> Source code in <code>pyadm1ode_calibration/io/persistence/database.py</code> <pre><code>@classmethod\ndef from_env(cls, prefix: str = \"DB\") -&gt; \"Database\":\n    \"\"\"Create database from environment variables.\"\"\"\n    import os\n    from urllib.parse import quote_plus\n\n    host = os.getenv(f\"{prefix}_HOST\", \"localhost\")\n    port = os.getenv(f\"{prefix}_PORT\", \"5432\")\n    database = os.getenv(f\"{prefix}_NAME\")\n    username = os.getenv(f\"{prefix}_USER\")\n    password = os.getenv(f\"{prefix}_PASSWORD\")\n\n    if not all([database, username, password]):\n        raise ValueError(\"Missing required environment variables\")\n\n    conn_str = f\"postgresql://{username}:{quote_plus(password)}@{host}:{port}/{database}\"\n    return cls(connection_string=conn_str)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.DataValidator","title":"<code>pyadm1ode_calibration.io.DataValidator</code>","text":"<p>Validator for biogas plant measurement data.</p> <p>Checks data quality, identifies issues, and provides statistics.</p> Source code in <code>pyadm1ode_calibration/io/validation/validators.py</code> <pre><code>class DataValidator:\n    \"\"\"\n    Validator for biogas plant measurement data.\n\n    Checks data quality, identifies issues, and provides statistics.\n    \"\"\"\n\n    @staticmethod\n    def validate(\n        data: pd.DataFrame,\n        required_columns: Optional[List[str]] = None,\n        expected_ranges: Optional[Dict[str, Tuple[float, float]]] = None,\n    ) -&gt; ValidationResult:\n        \"\"\"\n        Validate measurement data.\n\n        Args:\n            data: DataFrame to validate\n            required_columns: List of required column names\n            expected_ranges: Dictionary mapping columns to (min, max) tuples\n\n        Returns:\n            ValidationResult object\n        \"\"\"\n        issues = []\n        warnings_list = []\n\n        # Check for required columns\n        if required_columns:\n            missing_cols = set(required_columns) - set(data.columns)\n            if missing_cols:\n                issues.append(f\"Missing required columns: {missing_cols}\")\n\n        # Calculate missing data percentages\n        missing_data = {}\n        for col in data.columns:\n            pct_missing = (data[col].isna().sum() / len(data)) * 100\n            missing_data[col] = pct_missing\n\n            if pct_missing &gt; 30:\n                issues.append(f\"Column '{col}' has {pct_missing:.1f}% missing data\")\n            elif pct_missing &gt; 5:\n                warnings_list.append(f\"Column '{col}' has {pct_missing:.1f}% missing data\")\n\n        # Check for expected ranges\n        if expected_ranges:\n            for col, (min_val, max_val) in expected_ranges.items():\n                if col in data.columns:\n                    values = data[col].dropna()\n                    if len(values) &gt; 0:\n                        actual_min = values.min()\n                        actual_max = values.max()\n\n                        if actual_min &lt; min_val or actual_max &gt; max_val:\n                            warnings_list.append(\n                                f\"Column '{col}' has values outside expected range \"\n                                f\"[{min_val}, {max_val}]: actual [{actual_min:.2f}, {actual_max:.2f}]\"\n                            )\n\n        # Check for duplicate timestamps\n        if \"timestamp\" in data.columns:\n            duplicates = data[\"timestamp\"].duplicated().sum()\n            if duplicates &gt; 0:\n                warnings_list.append(f\"Found {duplicates} duplicate timestamps\")\n\n        # Calculate statistics\n        statistics = {\n            \"n_rows\": len(data),\n            \"n_columns\": len(data.columns),\n            \"total_missing\": data.isna().sum().sum(),\n            \"pct_missing\": (data.isna().sum().sum() / (len(data) * len(data.columns))) * 100,\n        }\n\n        # Calculate quality score\n        quality_score = DataValidator._calculate_quality_score(data, len(issues), len(warnings_list), statistics)\n\n        is_valid = len(issues) == 0\n\n        return ValidationResult(\n            is_valid=is_valid,\n            quality_score=quality_score,\n            issues=issues,\n            warnings=warnings_list,\n            statistics=statistics,\n            missing_data=missing_data,\n        )\n\n    @staticmethod\n    def _calculate_quality_score(data: pd.DataFrame, n_issues: int, n_warnings: int, statistics: Dict[str, Any]) -&gt; float:\n        \"\"\"Calculate overall data quality score (0-1).\"\"\"\n        score = 1.0\n\n        # Penalize for issues\n        score -= min(0.5, n_issues * 0.1)\n\n        # Penalize for warnings\n        score -= min(0.3, n_warnings * 0.05)\n\n        # Penalize for missing data\n        pct_missing = statistics[\"pct_missing\"]\n        score -= min(0.2, pct_missing / 100 * 0.5)\n\n        return max(0.0, score)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.io.DataValidator-functions","title":"Functions","text":""},{"location":"api/#pyadm1ode_calibration.io.DataValidator.validate","title":"<code>validate(data, required_columns=None, expected_ranges=None)</code>  <code>staticmethod</code>","text":"<p>Validate measurement data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to validate</p> required <code>required_columns</code> <code>Optional[List[str]]</code> <p>List of required column names</p> <code>None</code> <code>expected_ranges</code> <code>Optional[Dict[str, Tuple[float, float]]]</code> <p>Dictionary mapping columns to (min, max) tuples</p> <code>None</code> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult object</p> Source code in <code>pyadm1ode_calibration/io/validation/validators.py</code> <pre><code>@staticmethod\ndef validate(\n    data: pd.DataFrame,\n    required_columns: Optional[List[str]] = None,\n    expected_ranges: Optional[Dict[str, Tuple[float, float]]] = None,\n) -&gt; ValidationResult:\n    \"\"\"\n    Validate measurement data.\n\n    Args:\n        data: DataFrame to validate\n        required_columns: List of required column names\n        expected_ranges: Dictionary mapping columns to (min, max) tuples\n\n    Returns:\n        ValidationResult object\n    \"\"\"\n    issues = []\n    warnings_list = []\n\n    # Check for required columns\n    if required_columns:\n        missing_cols = set(required_columns) - set(data.columns)\n        if missing_cols:\n            issues.append(f\"Missing required columns: {missing_cols}\")\n\n    # Calculate missing data percentages\n    missing_data = {}\n    for col in data.columns:\n        pct_missing = (data[col].isna().sum() / len(data)) * 100\n        missing_data[col] = pct_missing\n\n        if pct_missing &gt; 30:\n            issues.append(f\"Column '{col}' has {pct_missing:.1f}% missing data\")\n        elif pct_missing &gt; 5:\n            warnings_list.append(f\"Column '{col}' has {pct_missing:.1f}% missing data\")\n\n    # Check for expected ranges\n    if expected_ranges:\n        for col, (min_val, max_val) in expected_ranges.items():\n            if col in data.columns:\n                values = data[col].dropna()\n                if len(values) &gt; 0:\n                    actual_min = values.min()\n                    actual_max = values.max()\n\n                    if actual_min &lt; min_val or actual_max &gt; max_val:\n                        warnings_list.append(\n                            f\"Column '{col}' has values outside expected range \"\n                            f\"[{min_val}, {max_val}]: actual [{actual_min:.2f}, {actual_max:.2f}]\"\n                        )\n\n    # Check for duplicate timestamps\n    if \"timestamp\" in data.columns:\n        duplicates = data[\"timestamp\"].duplicated().sum()\n        if duplicates &gt; 0:\n            warnings_list.append(f\"Found {duplicates} duplicate timestamps\")\n\n    # Calculate statistics\n    statistics = {\n        \"n_rows\": len(data),\n        \"n_columns\": len(data.columns),\n        \"total_missing\": data.isna().sum().sum(),\n        \"pct_missing\": (data.isna().sum().sum() / (len(data) * len(data.columns))) * 100,\n    }\n\n    # Calculate quality score\n    quality_score = DataValidator._calculate_quality_score(data, len(issues), len(warnings_list), statistics)\n\n    is_valid = len(issues) == 0\n\n    return ValidationResult(\n        is_valid=is_valid,\n        quality_score=quality_score,\n        issues=issues,\n        warnings=warnings_list,\n        statistics=statistics,\n        missing_data=missing_data,\n    )\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.CalibrationValidator","title":"<code>pyadm1ode_calibration.calibration.CalibrationValidator</code>","text":"<p>Validator for calibrated model parameters.</p> Source code in <code>pyadm1ode_calibration/calibration/validation.py</code> <pre><code>class CalibrationValidator:\n    \"\"\"Validator for calibrated model parameters.\"\"\"\n\n    def __init__(self, plant: Any, verbose: bool = True):\n        self.plant = plant\n        self.verbose = verbose\n\n    def validate(\n        self,\n        parameters: Dict[str, float],\n        measurements: MeasurementData,\n        objectives: Optional[List[str]] = None,\n        simulation_duration: Optional[float] = None,\n    ) -&gt; Dict[str, ValidationMetrics]:\n        if objectives is None:\n            objectives = [\"Q_ch4\", \"pH\", \"VFA\"]\n\n        self._apply_parameters(parameters)\n\n        if simulation_duration is None:\n            simulation_duration = len(measurements) * (1.0 / 24.0)\n\n        simulated_outputs = self._simulate_plant(measurements, simulation_duration)\n\n        metrics = {}\n        for objective in objectives:\n            if objective not in simulated_outputs:\n                warnings.warn(f\"Objective '{objective}' not in simulation outputs\")\n                continue\n\n            observed = self._extract_measurements(measurements, objective)\n            predicted = simulated_outputs[objective]\n\n            observed, predicted = self._align_arrays(observed, predicted)\n\n            if len(observed) == 0:\n                warnings.warn(f\"No valid data for objective '{objective}'\")\n                continue\n\n            obj_metrics = self._calculate_metrics(objective, observed, predicted)\n            metrics[objective] = obj_metrics\n\n        return metrics\n\n    def analyze_residuals(\n        self,\n        measurements: MeasurementData,\n        simulated: Dict[str, np.ndarray],\n        objectives: Optional[List[str]] = None,\n    ) -&gt; Dict[str, ResidualAnalysis]:\n        if objectives is None:\n            objectives = list(simulated.keys())\n\n        results = {}\n        for objective in objectives:\n            if objective not in simulated:\n                continue\n\n            observed = self._extract_measurements(measurements, objective)\n            predicted = simulated[objective]\n\n            observed, predicted = self._align_arrays(observed, predicted)\n\n            if len(observed) &lt; 3:\n                continue\n\n            residuals = observed - predicted\n            std_residuals = self._standardize_residuals(residuals)\n            normality = self._test_normality(residuals)\n            autocorr = self._calculate_autocorrelation(residuals)\n            hetero = self._test_heteroscedasticity(residuals, predicted)\n            outliers = np.where(np.abs(std_residuals) &gt; 3)[0].tolist()\n\n            results[objective] = ResidualAnalysis(\n                objective=objective,\n                residuals=residuals,\n                standardized_residuals=std_residuals,\n                normality_test=normality,\n                autocorrelation=autocorr,\n                heteroscedasticity_test=hetero,\n                outlier_indices=outliers,\n            )\n\n        return results\n\n    def cross_validate(\n        self,\n        parameters: Dict[str, float],\n        measurements: MeasurementData,\n        n_folds: int = 5,\n        objectives: Optional[List[str]] = None,\n    ) -&gt; Dict[str, List[ValidationMetrics]]:\n        if objectives is None:\n            objectives = [\"Q_ch4\", \"pH\", \"VFA\"]\n\n        n_samples = len(measurements)\n        fold_size = n_samples // n_folds\n        cv_results: Dict[str, List[ValidationMetrics]] = {obj: [] for obj in objectives}\n\n        for fold in range(n_folds):\n            start_idx = fold * fold_size\n            end_idx = start_idx + fold_size if fold &lt; n_folds - 1 else n_samples\n\n            val_data = measurements.data.iloc[start_idx:end_idx].copy()\n            val_measurements = type(measurements)(val_data)\n\n            fold_metrics = self.validate(parameters, val_measurements, objectives)\n            for obj, metrics in fold_metrics.items():\n                cv_results[obj].append(metrics)\n\n        return cv_results\n\n    def _apply_parameters(self, parameters: Dict[str, float]) -&gt; None:\n        \"\"\"Apply parameters to plant.\"\"\"\n        for component in self.plant.components.values():\n            if component.component_type.value == \"digester\":\n                if not hasattr(component, \"_calibration_params\"):\n                    component._calibration_params = {}\n                component._calibration_params.update(parameters)\n\n    def _simulate_plant(self, measurements: MeasurementData, duration: float) -&gt; Dict[str, np.ndarray]:\n        # Implementation depends on plant model\n        dt = 1.0 / 24.0\n        results = self.plant.simulate(duration=duration, dt=dt, save_interval=dt)\n        return self._extract_outputs_from_results(results)\n\n    def _extract_outputs_from_results(self, results: List[Dict[str, Any]]) -&gt; Dict[str, np.ndarray]:\n        outputs: Dict[str, List[float]] = {\"Q_ch4\": [], \"pH\": [], \"VFA\": [], \"TAC\": []}\n        for result in results:\n            comp_data = next(iter(result[\"components\"].values()))\n            for key in outputs:\n                outputs[key].append(comp_data.get(key, 0.0))\n        return {k: np.array(v) for k, v in outputs.items()}\n\n    def _extract_measurements(self, measurements: MeasurementData, objective: str) -&gt; np.ndarray:\n        \"\"\"Extract measurement array for objective.\n\n        Raises:\n            DataValidationError: If objective not found or empty\n        \"\"\"\n        if objective not in measurements.data.columns:\n            raise DataValidationError(f\"Objective '{objective}' not found in measurements\")\n\n        series = measurements.get_measurement(objective)\n        if series.empty:\n            raise DataValidationError(f\"No valid data for objective '{objective}'\")\n\n        return series.values\n\n    def _align_arrays(self, observed: np.ndarray, predicted: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        min_len = min(len(observed), len(predicted))\n        observed, predicted = observed[:min_len], predicted[:min_len]\n        valid = ~(np.isnan(observed) | np.isnan(predicted))\n        return observed[valid], predicted[valid]\n\n    def _calculate_metrics(self, objective: str, observed: np.ndarray, predicted: np.ndarray) -&gt; ValidationMetrics:\n        n = len(observed)\n        obs_mean, obs_std = np.mean(observed), np.std(observed)\n        pred_mean, pred_std = np.mean(predicted), np.std(predicted)\n        residuals = observed - predicted\n\n        rmse = np.sqrt(np.mean(residuals**2))\n        mae = np.mean(np.abs(residuals))\n\n        ss_res = np.sum(residuals**2)\n        ss_tot = np.sum((observed - obs_mean) ** 2)\n        r2 = float(1 - (ss_res / ss_tot) if ss_tot &gt; 0 else 0.0)\n\n        pbias = float((np.sum(residuals) / np.sum(observed)) * 100 if np.sum(observed) != 0 else 0.0)\n        correlation = float(np.corrcoef(observed, predicted)[0, 1] if n &gt; 1 else 0.0)\n\n        nonzero = observed != 0\n        mape = float(np.mean(np.abs(residuals[nonzero] / observed[nonzero])) * 100 if np.any(nonzero) else 0.0)\n        me = float(np.mean(residuals))\n\n        return ValidationMetrics(\n            objective=objective,\n            n_samples=n,\n            rmse=float(rmse),\n            mae=float(mae),\n            r2=r2,\n            nse=r2,\n            pbias=pbias,\n            correlation=correlation,\n            mape=mape,\n            me=me,\n            observations_mean=float(obs_mean),\n            observations_std=float(obs_std),\n            predictions_mean=float(pred_mean),\n            predictions_std=float(pred_std),\n        )\n\n    def _standardize_residuals(self, residuals: np.ndarray) -&gt; np.ndarray:\n        std = np.std(residuals)\n        return (residuals - np.mean(residuals)) / std if std &gt; 0 else np.zeros_like(residuals)\n\n    def _test_normality(self, residuals: np.ndarray) -&gt; Dict[str, float]:\n        try:\n            stat, p = stats.shapiro(residuals)\n            return {\"statistic\": float(stat), \"p_value\": float(p)}\n        except Exception:\n            return {\"statistic\": 0.0, \"p_value\": 1.0}\n\n    def _calculate_autocorrelation(self, residuals: np.ndarray) -&gt; float:\n        if len(residuals) &lt; 2:\n            return 0.0\n        res_centered = residuals - np.mean(residuals)\n        return float(np.corrcoef(res_centered[:-1], res_centered[1:])[0, 1])\n\n    def _test_heteroscedasticity(self, residuals: np.ndarray, predicted: np.ndarray) -&gt; Dict[str, float]:\n        try:\n            corr = np.corrcoef(residuals**2, predicted)[0, 1]\n            stat = len(residuals) * corr**2\n            p = 1 - stats.chi2.cdf(stat, df=1)\n            return {\"statistic\": float(stat), \"p_value\": float(p)}\n        except Exception:\n            return {\"statistic\": 0.0, \"p_value\": 1.0}\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds","title":"<code>pyadm1ode_calibration.calibration.ParameterBounds</code>","text":"<p>Manager for parameter bounds in ADM1 calibration.</p> <p>Provides methods for accessing bounds, validating parameters, and calculating penalties for constraint violations.</p> <p>Attributes:</p> Name Type Description <code>bounds</code> <code>Dict[str, ParameterBound]</code> <p>Dictionary mapping parameter names to ParameterBound objects</p> Example <p>bounds = ParameterBounds() bounds.add_bound(\"k_dis\", lower=0.3, upper=0.8, default=0.5) is_valid = bounds.is_within_bounds(\"k_dis\", 0.6)</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>class ParameterBounds:\n    \"\"\"\n    Manager for parameter bounds in ADM1 calibration.\n\n    Provides methods for accessing bounds, validating parameters,\n    and calculating penalties for constraint violations.\n\n    Attributes:\n        bounds: Dictionary mapping parameter names to ParameterBound objects\n\n    Example:\n        &gt;&gt;&gt; bounds = ParameterBounds()\n        &gt;&gt;&gt; bounds.add_bound(\"k_dis\", lower=0.3, upper=0.8, default=0.5)\n        &gt;&gt;&gt; is_valid = bounds.is_within_bounds(\"k_dis\", 0.6)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize empty parameter bounds manager.\"\"\"\n        self.bounds: Dict[str, ParameterBound] = {}\n\n    def add_bound(\n        self,\n        name: str,\n        lower: float,\n        upper: float,\n        default: float,\n        bound_type: BoundType = BoundType.HARD,\n        penalty_weight: float = 1.0,\n        description: str = \"\",\n        unit: str = \"\",\n        substrate_dependent: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Add parameter bound.\n\n        Args:\n            name: Parameter name\n            lower: Lower bound\n            upper: Upper bound\n            default: Default value\n            bound_type: Type of bound\n            penalty_weight: Penalty weight for soft constraints\n            description: Parameter description\n            unit: Parameter unit\n            substrate_dependent: Whether bound varies by substrate\n        \"\"\"\n        self.bounds[name] = ParameterBound(\n            name=name,\n            lower=lower,\n            upper=upper,\n            default=default,\n            bound_type=bound_type,\n            penalty_weight=penalty_weight,\n            description=description,\n            unit=unit,\n            substrate_dependent=substrate_dependent,\n        )\n\n    def get_bounds(self, name: str) -&gt; Optional[ParameterBound]:\n        \"\"\"\n        Get bounds for parameter.\n\n        Args:\n            name: Parameter name\n\n        Returns:\n            ParameterBound object or None if not found\n        \"\"\"\n        return self.bounds.get(name)\n\n    def get_bounds_tuple(self, name: str) -&gt; Optional[Tuple[float, float]]:\n        \"\"\"\n        Get bounds as tuple (lower, upper).\n\n        Args:\n            name: Parameter name\n\n        Returns:\n            Tuple of (lower, upper) or None\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return None\n        return (bound.lower, bound.upper)\n\n    def is_within_bounds(self, name: str, value: float, tolerance: float = 0.0) -&gt; bool:\n        \"\"\"\n        Check if parameter value is within bounds.\n\n        Args:\n            name: Parameter name\n            value: Parameter value\n            tolerance: Tolerance for boundary\n\n        Returns:\n            True if within bounds, False otherwise\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return True  # No bounds defined, assume valid\n        return bound.is_within_bounds(value, tolerance)\n\n    def clip_to_bounds(self, name: str, value: float) -&gt; float:\n        \"\"\"\n        Clip parameter value to bounds.\n\n        Args:\n            name: Parameter name\n            value: Parameter value\n\n        Returns:\n            Clipped value\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return value\n        return bound.clip_to_bounds(value)\n\n    def calculate_penalty(self, name: str, value: float, penalty_type: str = \"quadratic\") -&gt; float:\n        \"\"\"\n        Calculate penalty for parameter value.\n\n        Args:\n            name: Parameter name\n            value: Parameter value\n            penalty_type: Type of penalty function\n\n        Returns:\n            Penalty value\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return 0.0\n        return bound.calculate_penalty(value, penalty_type)\n\n    def calculate_total_penalty(self, parameters: Dict[str, float], penalty_type: str = \"quadratic\") -&gt; float:\n        \"\"\"\n        Calculate total penalty for all parameters.\n\n        Args:\n            parameters: Dictionary of parameter values\n            penalty_type: Type of penalty function\n\n        Returns:\n            Total penalty\n        \"\"\"\n        total_penalty = 0.0\n        for name, value in parameters.items():\n            penalty = self.calculate_penalty(name, value, penalty_type)\n            if np.isinf(penalty):\n                return np.inf\n            total_penalty += penalty\n        return total_penalty\n\n    def validate_parameters(self, parameters: Dict[str, float], raise_on_invalid: bool = False) -&gt; Tuple[bool, List[str]]:\n        \"\"\"\n        Validate all parameters against bounds.\n\n        Args:\n            parameters: Dictionary of parameter values\n            raise_on_invalid: Raise exception if invalid\n\n        Returns:\n            Tuple of (all_valid, list of error messages)\n        \"\"\"\n        errors = []\n\n        for name, value in parameters.items():\n            bound = self.get_bounds(name)\n            if bound is None:\n                continue\n\n            if not bound.is_within_bounds(value):\n                error = f\"Parameter '{name}' = {value:.4f} is outside bounds [{bound.lower:.4f}, {bound.upper:.4f}]\"\n                errors.append(error)\n\n        if errors and raise_on_invalid:\n            raise ValueError(\"\\n\".join(errors))\n\n        return (len(errors) == 0, errors)\n\n    def get_default_values(self, parameter_names: List[str]) -&gt; Dict[str, float]:\n        \"\"\"\n        Get default values for parameters.\n\n        Args:\n            parameter_names: List of parameter names\n\n        Returns:\n            Dictionary of default values\n        \"\"\"\n        defaults = {}\n        for name in parameter_names:\n            bound = self.get_bounds(name)\n            if bound is not None:\n                defaults[name] = bound.default\n        return defaults\n\n    def scale_to_unit_interval(self, name: str, value: float) -&gt; float:\n        \"\"\"\n        Scale parameter value to unit interval [0, 1].\n\n        Useful for optimization algorithms that work better with\n        normalized parameters.\n\n        Args:\n            name: Parameter name\n            value: Parameter value\n\n        Returns:\n            Scaled value in [0, 1]\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return value\n\n        if bound.upper == bound.lower:\n            return 0.5\n\n        return (value - bound.lower) / (bound.upper - bound.lower)\n\n    def unscale_from_unit_interval(self, name: str, scaled_value: float) -&gt; float:\n        \"\"\"\n        Unscale parameter value from unit interval [0, 1].\n\n        Args:\n            name: Parameter name\n            scaled_value: Scaled value in [0, 1]\n\n        Returns:\n            Unscaled parameter value\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return scaled_value\n\n        return bound.lower + scaled_value * (bound.upper - bound.lower)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds-functions","title":"Functions","text":""},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.__init__","title":"<code>__init__()</code>","text":"<p>Initialize empty parameter bounds manager.</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize empty parameter bounds manager.\"\"\"\n    self.bounds: Dict[str, ParameterBound] = {}\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.add_bound","title":"<code>add_bound(name, lower, upper, default, bound_type=BoundType.HARD, penalty_weight=1.0, description='', unit='', substrate_dependent=False)</code>","text":"<p>Add parameter bound.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>lower</code> <code>float</code> <p>Lower bound</p> required <code>upper</code> <code>float</code> <p>Upper bound</p> required <code>default</code> <code>float</code> <p>Default value</p> required <code>bound_type</code> <code>BoundType</code> <p>Type of bound</p> <code>HARD</code> <code>penalty_weight</code> <code>float</code> <p>Penalty weight for soft constraints</p> <code>1.0</code> <code>description</code> <code>str</code> <p>Parameter description</p> <code>''</code> <code>unit</code> <code>str</code> <p>Parameter unit</p> <code>''</code> <code>substrate_dependent</code> <code>bool</code> <p>Whether bound varies by substrate</p> <code>False</code> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def add_bound(\n    self,\n    name: str,\n    lower: float,\n    upper: float,\n    default: float,\n    bound_type: BoundType = BoundType.HARD,\n    penalty_weight: float = 1.0,\n    description: str = \"\",\n    unit: str = \"\",\n    substrate_dependent: bool = False,\n) -&gt; None:\n    \"\"\"\n    Add parameter bound.\n\n    Args:\n        name: Parameter name\n        lower: Lower bound\n        upper: Upper bound\n        default: Default value\n        bound_type: Type of bound\n        penalty_weight: Penalty weight for soft constraints\n        description: Parameter description\n        unit: Parameter unit\n        substrate_dependent: Whether bound varies by substrate\n    \"\"\"\n    self.bounds[name] = ParameterBound(\n        name=name,\n        lower=lower,\n        upper=upper,\n        default=default,\n        bound_type=bound_type,\n        penalty_weight=penalty_weight,\n        description=description,\n        unit=unit,\n        substrate_dependent=substrate_dependent,\n    )\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.calculate_penalty","title":"<code>calculate_penalty(name, value, penalty_type='quadratic')</code>","text":"<p>Calculate penalty for parameter value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>float</code> <p>Parameter value</p> required <code>penalty_type</code> <code>str</code> <p>Type of penalty function</p> <code>'quadratic'</code> <p>Returns:</p> Type Description <code>float</code> <p>Penalty value</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def calculate_penalty(self, name: str, value: float, penalty_type: str = \"quadratic\") -&gt; float:\n    \"\"\"\n    Calculate penalty for parameter value.\n\n    Args:\n        name: Parameter name\n        value: Parameter value\n        penalty_type: Type of penalty function\n\n    Returns:\n        Penalty value\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return 0.0\n    return bound.calculate_penalty(value, penalty_type)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.calculate_total_penalty","title":"<code>calculate_total_penalty(parameters, penalty_type='quadratic')</code>","text":"<p>Calculate total penalty for all parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Dictionary of parameter values</p> required <code>penalty_type</code> <code>str</code> <p>Type of penalty function</p> <code>'quadratic'</code> <p>Returns:</p> Type Description <code>float</code> <p>Total penalty</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def calculate_total_penalty(self, parameters: Dict[str, float], penalty_type: str = \"quadratic\") -&gt; float:\n    \"\"\"\n    Calculate total penalty for all parameters.\n\n    Args:\n        parameters: Dictionary of parameter values\n        penalty_type: Type of penalty function\n\n    Returns:\n        Total penalty\n    \"\"\"\n    total_penalty = 0.0\n    for name, value in parameters.items():\n        penalty = self.calculate_penalty(name, value, penalty_type)\n        if np.isinf(penalty):\n            return np.inf\n        total_penalty += penalty\n    return total_penalty\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.clip_to_bounds","title":"<code>clip_to_bounds(name, value)</code>","text":"<p>Clip parameter value to bounds.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>float</code> <p>Parameter value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Clipped value</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def clip_to_bounds(self, name: str, value: float) -&gt; float:\n    \"\"\"\n    Clip parameter value to bounds.\n\n    Args:\n        name: Parameter name\n        value: Parameter value\n\n    Returns:\n        Clipped value\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return value\n    return bound.clip_to_bounds(value)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.get_bounds","title":"<code>get_bounds(name)</code>","text":"<p>Get bounds for parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>Optional[ParameterBound]</code> <p>ParameterBound object or None if not found</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def get_bounds(self, name: str) -&gt; Optional[ParameterBound]:\n    \"\"\"\n    Get bounds for parameter.\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        ParameterBound object or None if not found\n    \"\"\"\n    return self.bounds.get(name)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.get_bounds_tuple","title":"<code>get_bounds_tuple(name)</code>","text":"<p>Get bounds as tuple (lower, upper).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[float, float]]</code> <p>Tuple of (lower, upper) or None</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def get_bounds_tuple(self, name: str) -&gt; Optional[Tuple[float, float]]:\n    \"\"\"\n    Get bounds as tuple (lower, upper).\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        Tuple of (lower, upper) or None\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return None\n    return (bound.lower, bound.upper)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.get_default_values","title":"<code>get_default_values(parameter_names)</code>","text":"<p>Get default values for parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_names</code> <code>List[str]</code> <p>List of parameter names</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of default values</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def get_default_values(self, parameter_names: List[str]) -&gt; Dict[str, float]:\n    \"\"\"\n    Get default values for parameters.\n\n    Args:\n        parameter_names: List of parameter names\n\n    Returns:\n        Dictionary of default values\n    \"\"\"\n    defaults = {}\n    for name in parameter_names:\n        bound = self.get_bounds(name)\n        if bound is not None:\n            defaults[name] = bound.default\n    return defaults\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.is_within_bounds","title":"<code>is_within_bounds(name, value, tolerance=0.0)</code>","text":"<p>Check if parameter value is within bounds.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>float</code> <p>Parameter value</p> required <code>tolerance</code> <code>float</code> <p>Tolerance for boundary</p> <code>0.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if within bounds, False otherwise</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def is_within_bounds(self, name: str, value: float, tolerance: float = 0.0) -&gt; bool:\n    \"\"\"\n    Check if parameter value is within bounds.\n\n    Args:\n        name: Parameter name\n        value: Parameter value\n        tolerance: Tolerance for boundary\n\n    Returns:\n        True if within bounds, False otherwise\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return True  # No bounds defined, assume valid\n    return bound.is_within_bounds(value, tolerance)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.scale_to_unit_interval","title":"<code>scale_to_unit_interval(name, value)</code>","text":"<p>Scale parameter value to unit interval [0, 1].</p> <p>Useful for optimization algorithms that work better with normalized parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>float</code> <p>Parameter value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Scaled value in [0, 1]</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def scale_to_unit_interval(self, name: str, value: float) -&gt; float:\n    \"\"\"\n    Scale parameter value to unit interval [0, 1].\n\n    Useful for optimization algorithms that work better with\n    normalized parameters.\n\n    Args:\n        name: Parameter name\n        value: Parameter value\n\n    Returns:\n        Scaled value in [0, 1]\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return value\n\n    if bound.upper == bound.lower:\n        return 0.5\n\n    return (value - bound.lower) / (bound.upper - bound.lower)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.unscale_from_unit_interval","title":"<code>unscale_from_unit_interval(name, scaled_value)</code>","text":"<p>Unscale parameter value from unit interval [0, 1].</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>scaled_value</code> <code>float</code> <p>Scaled value in [0, 1]</p> required <p>Returns:</p> Type Description <code>float</code> <p>Unscaled parameter value</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def unscale_from_unit_interval(self, name: str, scaled_value: float) -&gt; float:\n    \"\"\"\n    Unscale parameter value from unit interval [0, 1].\n\n    Args:\n        name: Parameter name\n        scaled_value: Scaled value in [0, 1]\n\n    Returns:\n        Unscaled parameter value\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return scaled_value\n\n    return bound.lower + scaled_value * (bound.upper - bound.lower)\n</code></pre>"},{"location":"api/#pyadm1ode_calibration.calibration.ParameterBounds.validate_parameters","title":"<code>validate_parameters(parameters, raise_on_invalid=False)</code>","text":"<p>Validate all parameters against bounds.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Dictionary of parameter values</p> required <code>raise_on_invalid</code> <code>bool</code> <p>Raise exception if invalid</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[bool, List[str]]</code> <p>Tuple of (all_valid, list of error messages)</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def validate_parameters(self, parameters: Dict[str, float], raise_on_invalid: bool = False) -&gt; Tuple[bool, List[str]]:\n    \"\"\"\n    Validate all parameters against bounds.\n\n    Args:\n        parameters: Dictionary of parameter values\n        raise_on_invalid: Raise exception if invalid\n\n    Returns:\n        Tuple of (all_valid, list of error messages)\n    \"\"\"\n    errors = []\n\n    for name, value in parameters.items():\n        bound = self.get_bounds(name)\n        if bound is None:\n            continue\n\n        if not bound.is_within_bounds(value):\n            error = f\"Parameter '{name}' = {value:.4f} is outside bounds [{bound.lower:.4f}, {bound.upper:.4f}]\"\n            errors.append(error)\n\n    if errors and raise_on_invalid:\n        raise ValueError(\"\\n\".join(errors))\n\n    return (len(errors) == 0, errors)\n</code></pre>"},{"location":"api/calibration/","title":"Calibration API","text":""},{"location":"api/calibration/#pyadm1ode_calibration.calibration.Calibrator","title":"<code>pyadm1ode_calibration.calibration.Calibrator</code>","text":"<p>Orchestration layer for calibration.</p> Source code in <code>pyadm1ode_calibration/calibration/__init__.py</code> <pre><code>class Calibrator:\n    \"\"\"Orchestration layer for calibration.\"\"\"\n\n    def __init__(self, plant, verbose: bool = True):\n        self.plant = plant\n        self.verbose = verbose\n        self.initial_calibrator = InitialCalibrator(plant, verbose)\n        self.online_calibrator = OnlineCalibrator(plant, verbose)\n\n    def run_initial_calibration(self, measurements, parameters, **kwargs):\n        return self.initial_calibrator.calibrate(measurements, parameters, **kwargs)\n\n    def run_online_calibration(self, measurements, parameters, **kwargs):\n        return self.online_calibrator.calibrate(measurements, parameters, **kwargs)\n\n    def apply_calibration(self, result: CalibrationResult):\n        self.online_calibrator.apply_calibration(result)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.InitialCalibrator","title":"<code>pyadm1ode_calibration.calibration.InitialCalibrator</code>","text":"<p>               Bases: <code>BaseCalibrator</code></p> <p>Initial calibrator for ADM1 parameters from historical data.</p> Source code in <code>pyadm1ode_calibration/calibration/methods/initial.py</code> <pre><code>class InitialCalibrator(BaseCalibrator):\n    \"\"\"Initial calibrator for ADM1 parameters from historical data.\"\"\"\n\n    def __init__(self, plant: Any, verbose: bool = True):\n        super().__init__(plant, verbose)\n        self.parameter_bounds = create_default_bounds()\n        self.validator = CalibrationValidator(plant, verbose=False)\n        self.sensitivity_analyzer = SensitivityAnalyzer(plant, self.simulator, verbose)\n        self.identifiability_analyzer = IdentifiabilityAnalyzer(plant, self.sensitivity_analyzer, verbose)\n        self._optimization_history: List[Dict[str, Any]] = []\n        self._best_objective_value: float = float(\"inf\")\n        self._original_parameters: Dict[str, float] = self._get_current_parameters()\n\n    def calibrate(\n        self,\n        measurements: MeasurementData,\n        parameters: List[str],\n        bounds: Optional[Dict[str, Tuple[float, float]]] = None,\n        method: str = \"differential_evolution\",\n        objectives: Optional[List[str]] = None,\n        weights: Optional[Dict[str, float]] = None,\n        validation_split: float = 0.2,\n        max_iterations: int = 100,\n        population_size: int = 15,\n        tolerance: float = 1e-4,\n        sensitivity_analysis: bool = True,\n        use_constraints: bool = False,\n        **kwargs: Any,\n    ) -&gt; CalibrationResult:\n        start_time = time.time()\n        if objectives is None:\n            objectives = [\"Q_ch4\"]\n\n        # Split data\n        train_data, val_data = self._split_data(measurements, validation_split)\n\n        initial_params = self.parameter_bounds.get_default_values(parameters)\n        param_bounds = self._setup_bounds(parameters, bounds)\n\n        # Create objective function\n        def simulator_wrapper(params: Dict[str, float]) -&gt; Dict[str, np.ndarray]:\n            return self.simulator.simulate_with_parameters(params, train_data)\n\n        measurements_dict: Dict[str, np.ndarray] = {}\n        for obj in objectives:\n            try:\n                measurements_dict[obj] = train_data.get_measurement(obj).values\n            except Exception:\n                continue\n\n        objective_func: Callable[[np.ndarray], float]\n        if weights is None:\n            objective_func = WeightedSumObjective(\n                simulator=simulator_wrapper,\n                measurements_dict=measurements_dict,\n                objectives=objectives,\n                parameter_names=parameters,\n                error_metric=\"rmse\",\n                normalize=True,\n            )\n        else:\n            objective_func = MultiObjectiveFunction(\n                simulator=simulator_wrapper,\n                measurements_dict=measurements_dict,\n                objectives=objectives,\n                weights=weights,\n                parameter_names=parameters,\n                error_metric=\"rmse\",\n                normalize=True,\n            )\n\n        # Constraints\n        obj_func_final: Callable[[np.ndarray], float]\n        if use_constraints:\n            constraints = ParameterConstraints()\n            for param, (lb, ub) in param_bounds.items():\n                constraints.add_box_constraint(param, lb, ub, hard=True)\n\n            def penalized_objective(x: np.ndarray) -&gt; float:\n                params = {name: val for name, val in zip(parameters, x)}\n                return objective_func(x) + constraints.calculate_penalty(params)\n\n            obj_func_final = penalized_objective\n        else:\n            obj_func_final = objective_func\n\n        optimizer_kwargs = {**kwargs}\n        optimizer_kwargs[\"tolerance\"] = tolerance\n        if method in [\"differential_evolution\", \"de\"]:\n            optimizer_kwargs[\"population_size\"] = population_size\n\n        optimizer = create_optimizer(\n            method=method,\n            bounds=param_bounds,\n            max_iterations=max_iterations,\n            verbose=self.verbose,\n            **optimizer_kwargs,\n        )\n\n        initial_guess = (\n            np.array([initial_params[p] for p in parameters]) if method in [\"nelder_mead\", \"nm\", \"lbfgsb\", \"powell\"] else None\n        )\n        opt_result = optimizer.optimize(obj_func_final, initial_guess=initial_guess)\n\n        # Validation\n        validation_metrics: Dict[str, float] = {}\n        if len(val_data) &gt; 0:\n            val_result = self.validator.validate(\n                parameters=opt_result.parameter_dict, measurements=val_data, objectives=objectives\n            )\n            for obj, metrics in val_result.items():\n                validation_metrics.update(\n                    {f\"{obj}_rmse\": float(metrics.rmse), f\"{obj}_r2\": float(metrics.r2), f\"{obj}_nse\": float(metrics.nse)}\n                )\n\n        # Sensitivity\n        sensitivity_results: Dict[str, float] = {}\n        if sensitivity_analysis and opt_result.success:\n            sens = self.sensitivity_analyzer.analyze(opt_result.parameter_dict, train_data, objectives)\n            sensitivity_results = {p: float(max(abs(s) for s in r.sensitivity_indices.values())) for p, r in sens.items()}\n\n        return CalibrationResult(\n            success=opt_result.success,\n            parameters=opt_result.parameter_dict,\n            initial_parameters=initial_params,\n            objective_value=float(opt_result.fun),\n            n_iterations=int(opt_result.nit),\n            execution_time=time.time() - start_time,\n            method=method,\n            message=str(opt_result.message) if hasattr(opt_result, \"message\") else \"Optimization completed\",\n            validation_metrics=validation_metrics,\n            sensitivity=sensitivity_results,\n            history=opt_result.history,\n        )\n\n    def sensitivity_analysis(\n        self, parameters: Dict[str, float], measurements: MeasurementData, objectives: Optional[List[str]] = None\n    ) -&gt; Dict[str, SensitivityResult]:\n        return self.sensitivity_analyzer.analyze(parameters, measurements, objectives)\n\n    def identifiability_analysis(\n        self, parameters: Dict[str, float], measurements: MeasurementData\n    ) -&gt; Dict[str, IdentifiabilityResult]:\n        return self.identifiability_analyzer.analyze(parameters, measurements)\n\n    def _split_data(self, measurements: MeasurementData, split_ratio: float) -&gt; Tuple[MeasurementData, MeasurementData]:\n        n_train = int(len(measurements) * (1 - split_ratio))\n        return (\n            MeasurementData(measurements.data.iloc[:n_train].copy(), metadata=measurements.metadata.copy()),\n            MeasurementData(measurements.data.iloc[n_train:].copy(), metadata=measurements.metadata.copy()),\n        )\n\n    def _setup_bounds(\n        self, parameters: List[str], custom_bounds: Optional[Dict[str, Tuple[float, float]]]\n    ) -&gt; Dict[str, Tuple[float, float]]:\n        bounds: Dict[str, Tuple[float, float]] = {}\n        for param in parameters:\n            if custom_bounds and param in custom_bounds:\n                bounds[param] = custom_bounds[param]\n            else:\n                b = self.parameter_bounds.get_bounds_tuple(param)\n                if b:\n                    bounds[param] = b\n                else:\n                    default = self.parameter_bounds.get_default_values([param])[param]\n                    bounds[param] = (default * 0.5, default * 1.5)\n        return bounds\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.OnlineCalibrator","title":"<code>pyadm1ode_calibration.calibration.OnlineCalibrator</code>","text":"<p>               Bases: <code>BaseCalibrator</code></p> <p>Online calibrator for real-time parameter adjustment.</p> <p>Performs fast, bounded re-calibration when model predictions deviate from measurements.</p> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>class OnlineCalibrator(BaseCalibrator):\n    \"\"\"Online calibrator for real-time parameter adjustment.\n\n    Performs fast, bounded re-calibration when model predictions deviate from\n    measurements.\n    \"\"\"\n\n    def __init__(self, plant: Any, verbose: bool = True, parameter_bounds: Optional[ParameterBounds] = None):\n        \"\"\"Initialize online calibrator.\n\n        Args:\n            plant: BiogasPlant instance\n            verbose: Enable progress output\n            parameter_bounds: Custom parameter bounds manager\n        \"\"\"\n        super().__init__(plant, verbose)\n        self.parameter_bounds: ParameterBounds = parameter_bounds or create_default_bounds()\n        self.validator: CalibrationValidator = CalibrationValidator(plant, verbose=False)\n        self.trigger: OnlineCalibrationTrigger = OnlineCalibrationTrigger()\n        self.state: OnlineState = OnlineState()\n\n    def calibrate(\n        self,\n        measurements: MeasurementData,\n        parameters: Optional[List[str]] = None,\n        current_parameters: Optional[Dict[str, float]] = None,\n        variance_threshold: float = 0.15,\n        max_parameter_change: float = 0.20,\n        time_window: int = 7,\n        method: str = \"nelder_mead\",\n        max_iterations: int = 50,\n        objectives: Optional[List[str]] = None,\n        weights: Optional[Dict[str, float]] = None,\n        use_constraints: bool = True,\n        **kwargs: Any,\n    ) -&gt; CalibrationResult:\n        \"\"\"Perform online re-calibration with bounded parameter adjustments.\n\n        Args:\n            measurements: Recent measurement data\n            parameters: Parameters to adjust\n            current_parameters: Current parameter values\n            variance_threshold: Variance threshold for triggering (0-1)\n            max_parameter_change: Maximum relative parameter change (0-1)\n            time_window: Days of recent data to use\n            method: Optimization method name\n            max_iterations: Max optimization iterations\n            objectives: List of outputs to match\n            weights: Objective weights\n            use_constraints: Whether to apply parameter constraints\n            **kwargs: Extra optimizer settings\n\n        Returns:\n            CalibrationResult instance\n        \"\"\"\n        start_time = time.time()\n        if objectives is None:\n            objectives = [\"Q_ch4\", \"pH\"]\n\n        if parameters is None:\n            if self.state.parameter_history:\n                parameters = list(self.state.parameter_history[-1].parameters.keys())\n            else:\n                raise ValueError(\"No parameters specified and no calibration history available\")\n\n        if current_parameters is None:\n            current_parameters = self._get_current_parameters()\n\n        windowed_data = self._extract_time_window(measurements, time_window)\n        current_variance = self._calculate_prediction_variance(windowed_data, current_parameters, objectives)\n        self.state.current_variance = current_variance\n\n        param_bounds = self._setup_online_bounds(parameters, current_parameters, max_parameter_change)\n\n        def simulator_wrapper(params: Dict[str, float]) -&gt; Dict[str, np.ndarray]:\n            return self.simulator.simulate_with_parameters(params, windowed_data)\n\n        measurements_dict: Dict[str, np.ndarray] = {\n            obj: windowed_data.get_measurement(obj).values for obj in objectives if obj in windowed_data.data.columns\n        }\n\n        objective_func: Callable[[np.ndarray], float] = MultiObjectiveFunction(\n            simulator=simulator_wrapper,\n            measurements_dict=measurements_dict,\n            objectives=objectives,\n            weights=weights or {obj: 1.0 / len(objectives) for obj in objectives},\n            parameter_names=parameters,\n            error_metric=\"rmse\",\n            normalize=True,\n        )\n\n        obj_func_final: Callable[[np.ndarray], float]\n        if use_constraints:\n            constraints = ParameterConstraints()\n            for p, (lb, ub) in param_bounds.items():\n                constraints.add_box_constraint(p, lb, ub, hard=True)\n\n            def penalized_objective(x: np.ndarray) -&gt; float:\n                params = {name: val for name, val in zip(parameters, x)}\n                return objective_func(x) + constraints.calculate_penalty(params)\n\n            obj_func_final = penalized_objective\n        else:\n            obj_func_final = objective_func\n\n        optimizer = create_optimizer(\n            method=method, bounds=param_bounds, max_iterations=max_iterations, verbose=self.verbose, **kwargs\n        )\n\n        initial_guess = np.array(\n            [current_parameters.get(p, self.parameter_bounds.get_default_values([p])[p]) for p in parameters]\n        )\n        opt_result = optimizer.optimize(obj_func_final, initial_guess=initial_guess)\n\n        validation_metrics: Dict[str, float] = {}\n        if opt_result.success:\n            val_res = self.validator.validate(\n                parameters=opt_result.parameter_dict, measurements=windowed_data, objectives=objectives\n            )\n            validation_metrics = {f\"{obj}_{k}\": float(getattr(m, k)) for obj, m in val_res.items() for k in [\"rmse\", \"r2\"]}\n\n        self.state.total_calibrations += 1\n        self.state.last_calibration_time = datetime.now()\n\n        history_entry = ParameterChangeHistory(\n            timestamp=datetime.now(),\n            parameters=opt_result.parameter_dict.copy(),\n            trigger_reason=\"variance_threshold\" if current_variance &gt; variance_threshold else \"manual\",\n            objective_value=float(opt_result.fun),\n            variance=float(current_variance),\n            success=bool(opt_result.success),\n        )\n        self.state.parameter_history.append(history_entry)\n\n        return CalibrationResult(\n            success=opt_result.success,\n            parameters=opt_result.parameter_dict,\n            initial_parameters=current_parameters,\n            objective_value=float(opt_result.fun),\n            n_iterations=int(opt_result.nit),\n            execution_time=time.time() - start_time,\n            method=method,\n            message=str(getattr(opt_result, \"message\", \"Online calibration completed\")),\n            validation_metrics=validation_metrics,\n        )\n\n    def should_recalibrate(\n        self, recent_measurements: MeasurementData, objectives: Optional[List[str]] = None\n    ) -&gt; Tuple[bool, str]:\n        \"\"\"Check if re-calibration should be triggered.\n\n        Returns:\n            Tuple of (should_recalibrate, reason)\n        \"\"\"\n        if not self.trigger.enabled:\n            return False, \"Disabled\"\n\n        if objectives is None:\n            objectives = [\"Q_ch4\", \"pH\"]\n\n        if self.state.last_calibration_time:\n            hours = (datetime.now() - self.state.last_calibration_time).total_seconds() / 3600\n            if hours &lt; self.trigger.time_threshold:\n                return False, f\"Too soon since last calibration ({hours:.1f}h)\"\n\n        variance = self._calculate_prediction_variance(recent_measurements, self._get_current_parameters(), objectives)\n        self.state.current_variance = variance\n\n        if variance &gt; self.trigger.variance_threshold:\n            self.state.consecutive_violations += 1\n            if self.state.consecutive_violations &gt;= self.trigger.consecutive_violations:\n                return True, f\"Variance {variance:.4f} &gt; {self.trigger.variance_threshold}\"\n        else:\n            self.state.consecutive_violations = 0\n\n        return False, \"Prediction within accuracy threshold\"\n\n    def apply_calibration(self, result: CalibrationResult) -&gt; None:\n        \"\"\"Apply calibration parameters to the plant.\"\"\"\n        for component in self.plant.components.values():\n            if component.component_type.value == \"digester\":\n                component.apply_calibration_parameters(result.parameters)\n\n    def _extract_time_window(self, measurements: MeasurementData, window_days: int) -&gt; MeasurementData:\n        \"\"\"Extract recent time window from measurements.\"\"\"\n        last_time = measurements.data.index[-1]\n        return measurements.get_time_window(last_time - timedelta(days=window_days), last_time)\n\n    def _calculate_prediction_variance(\n        self, measurements: MeasurementData, parameters: Dict[str, float], objectives: List[str]\n    ) -&gt; float:\n        \"\"\"Calculate prediction variance for current parameters.\"\"\"\n        try:\n            outputs = self.simulator.simulate_with_parameters(parameters, measurements)\n            variances: List[float] = []\n            for obj in objectives:\n                if obj not in outputs:\n                    continue\n                m = measurements.get_measurement(obj).values\n                s = np.atleast_1d(outputs[obj])\n                length = min(len(m), len(s))\n                m, s = m[:length], s[:length]\n                valid = ~(np.isnan(m) | np.isnan(s))\n                if not np.any(valid):\n                    continue\n                res = m[valid] - s[valid]\n                variances.append(float(np.std(res) / (np.mean(np.abs(m[valid])) + 1e-10)))\n            return float(np.mean(variances)) if variances else 0.0\n        except Exception:\n            return 0.0\n\n    def _setup_online_bounds(\n        self, parameters: List[str], current_params: Dict[str, float], max_change: float\n    ) -&gt; Dict[str, Tuple[float, float]]:\n        \"\"\"Setup bounded parameter ranges for online calibration.\"\"\"\n        bounds: Dict[str, Tuple[float, float]] = {}\n        for p in parameters:\n            curr = current_params.get(p, 0.0)\n            default = self.parameter_bounds.get_bounds_tuple(p) or (curr * 0.5, curr * 1.5)\n            bounds[p] = (max(default[0], curr * (1 - max_change)), min(default[1], curr * (1 + max_change)))\n        return bounds\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.OnlineCalibrator-functions","title":"Functions","text":""},{"location":"api/calibration/#pyadm1ode_calibration.calibration.OnlineCalibrator.__init__","title":"<code>__init__(plant, verbose=True, parameter_bounds=None)</code>","text":"<p>Initialize online calibrator.</p> <p>Parameters:</p> Name Type Description Default <code>plant</code> <code>Any</code> <p>BiogasPlant instance</p> required <code>verbose</code> <code>bool</code> <p>Enable progress output</p> <code>True</code> <code>parameter_bounds</code> <code>Optional[ParameterBounds]</code> <p>Custom parameter bounds manager</p> <code>None</code> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>def __init__(self, plant: Any, verbose: bool = True, parameter_bounds: Optional[ParameterBounds] = None):\n    \"\"\"Initialize online calibrator.\n\n    Args:\n        plant: BiogasPlant instance\n        verbose: Enable progress output\n        parameter_bounds: Custom parameter bounds manager\n    \"\"\"\n    super().__init__(plant, verbose)\n    self.parameter_bounds: ParameterBounds = parameter_bounds or create_default_bounds()\n    self.validator: CalibrationValidator = CalibrationValidator(plant, verbose=False)\n    self.trigger: OnlineCalibrationTrigger = OnlineCalibrationTrigger()\n    self.state: OnlineState = OnlineState()\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.OnlineCalibrator.apply_calibration","title":"<code>apply_calibration(result)</code>","text":"<p>Apply calibration parameters to the plant.</p> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>def apply_calibration(self, result: CalibrationResult) -&gt; None:\n    \"\"\"Apply calibration parameters to the plant.\"\"\"\n    for component in self.plant.components.values():\n        if component.component_type.value == \"digester\":\n            component.apply_calibration_parameters(result.parameters)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.OnlineCalibrator.calibrate","title":"<code>calibrate(measurements, parameters=None, current_parameters=None, variance_threshold=0.15, max_parameter_change=0.2, time_window=7, method='nelder_mead', max_iterations=50, objectives=None, weights=None, use_constraints=True, **kwargs)</code>","text":"<p>Perform online re-calibration with bounded parameter adjustments.</p> <p>Parameters:</p> Name Type Description Default <code>measurements</code> <code>MeasurementData</code> <p>Recent measurement data</p> required <code>parameters</code> <code>Optional[List[str]]</code> <p>Parameters to adjust</p> <code>None</code> <code>current_parameters</code> <code>Optional[Dict[str, float]]</code> <p>Current parameter values</p> <code>None</code> <code>variance_threshold</code> <code>float</code> <p>Variance threshold for triggering (0-1)</p> <code>0.15</code> <code>max_parameter_change</code> <code>float</code> <p>Maximum relative parameter change (0-1)</p> <code>0.2</code> <code>time_window</code> <code>int</code> <p>Days of recent data to use</p> <code>7</code> <code>method</code> <code>str</code> <p>Optimization method name</p> <code>'nelder_mead'</code> <code>max_iterations</code> <code>int</code> <p>Max optimization iterations</p> <code>50</code> <code>objectives</code> <code>Optional[List[str]]</code> <p>List of outputs to match</p> <code>None</code> <code>weights</code> <code>Optional[Dict[str, float]]</code> <p>Objective weights</p> <code>None</code> <code>use_constraints</code> <code>bool</code> <p>Whether to apply parameter constraints</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Extra optimizer settings</p> <code>{}</code> <p>Returns:</p> Type Description <code>CalibrationResult</code> <p>CalibrationResult instance</p> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>def calibrate(\n    self,\n    measurements: MeasurementData,\n    parameters: Optional[List[str]] = None,\n    current_parameters: Optional[Dict[str, float]] = None,\n    variance_threshold: float = 0.15,\n    max_parameter_change: float = 0.20,\n    time_window: int = 7,\n    method: str = \"nelder_mead\",\n    max_iterations: int = 50,\n    objectives: Optional[List[str]] = None,\n    weights: Optional[Dict[str, float]] = None,\n    use_constraints: bool = True,\n    **kwargs: Any,\n) -&gt; CalibrationResult:\n    \"\"\"Perform online re-calibration with bounded parameter adjustments.\n\n    Args:\n        measurements: Recent measurement data\n        parameters: Parameters to adjust\n        current_parameters: Current parameter values\n        variance_threshold: Variance threshold for triggering (0-1)\n        max_parameter_change: Maximum relative parameter change (0-1)\n        time_window: Days of recent data to use\n        method: Optimization method name\n        max_iterations: Max optimization iterations\n        objectives: List of outputs to match\n        weights: Objective weights\n        use_constraints: Whether to apply parameter constraints\n        **kwargs: Extra optimizer settings\n\n    Returns:\n        CalibrationResult instance\n    \"\"\"\n    start_time = time.time()\n    if objectives is None:\n        objectives = [\"Q_ch4\", \"pH\"]\n\n    if parameters is None:\n        if self.state.parameter_history:\n            parameters = list(self.state.parameter_history[-1].parameters.keys())\n        else:\n            raise ValueError(\"No parameters specified and no calibration history available\")\n\n    if current_parameters is None:\n        current_parameters = self._get_current_parameters()\n\n    windowed_data = self._extract_time_window(measurements, time_window)\n    current_variance = self._calculate_prediction_variance(windowed_data, current_parameters, objectives)\n    self.state.current_variance = current_variance\n\n    param_bounds = self._setup_online_bounds(parameters, current_parameters, max_parameter_change)\n\n    def simulator_wrapper(params: Dict[str, float]) -&gt; Dict[str, np.ndarray]:\n        return self.simulator.simulate_with_parameters(params, windowed_data)\n\n    measurements_dict: Dict[str, np.ndarray] = {\n        obj: windowed_data.get_measurement(obj).values for obj in objectives if obj in windowed_data.data.columns\n    }\n\n    objective_func: Callable[[np.ndarray], float] = MultiObjectiveFunction(\n        simulator=simulator_wrapper,\n        measurements_dict=measurements_dict,\n        objectives=objectives,\n        weights=weights or {obj: 1.0 / len(objectives) for obj in objectives},\n        parameter_names=parameters,\n        error_metric=\"rmse\",\n        normalize=True,\n    )\n\n    obj_func_final: Callable[[np.ndarray], float]\n    if use_constraints:\n        constraints = ParameterConstraints()\n        for p, (lb, ub) in param_bounds.items():\n            constraints.add_box_constraint(p, lb, ub, hard=True)\n\n        def penalized_objective(x: np.ndarray) -&gt; float:\n            params = {name: val for name, val in zip(parameters, x)}\n            return objective_func(x) + constraints.calculate_penalty(params)\n\n        obj_func_final = penalized_objective\n    else:\n        obj_func_final = objective_func\n\n    optimizer = create_optimizer(\n        method=method, bounds=param_bounds, max_iterations=max_iterations, verbose=self.verbose, **kwargs\n    )\n\n    initial_guess = np.array(\n        [current_parameters.get(p, self.parameter_bounds.get_default_values([p])[p]) for p in parameters]\n    )\n    opt_result = optimizer.optimize(obj_func_final, initial_guess=initial_guess)\n\n    validation_metrics: Dict[str, float] = {}\n    if opt_result.success:\n        val_res = self.validator.validate(\n            parameters=opt_result.parameter_dict, measurements=windowed_data, objectives=objectives\n        )\n        validation_metrics = {f\"{obj}_{k}\": float(getattr(m, k)) for obj, m in val_res.items() for k in [\"rmse\", \"r2\"]}\n\n    self.state.total_calibrations += 1\n    self.state.last_calibration_time = datetime.now()\n\n    history_entry = ParameterChangeHistory(\n        timestamp=datetime.now(),\n        parameters=opt_result.parameter_dict.copy(),\n        trigger_reason=\"variance_threshold\" if current_variance &gt; variance_threshold else \"manual\",\n        objective_value=float(opt_result.fun),\n        variance=float(current_variance),\n        success=bool(opt_result.success),\n    )\n    self.state.parameter_history.append(history_entry)\n\n    return CalibrationResult(\n        success=opt_result.success,\n        parameters=opt_result.parameter_dict,\n        initial_parameters=current_parameters,\n        objective_value=float(opt_result.fun),\n        n_iterations=int(opt_result.nit),\n        execution_time=time.time() - start_time,\n        method=method,\n        message=str(getattr(opt_result, \"message\", \"Online calibration completed\")),\n        validation_metrics=validation_metrics,\n    )\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.OnlineCalibrator.should_recalibrate","title":"<code>should_recalibrate(recent_measurements, objectives=None)</code>","text":"<p>Check if re-calibration should be triggered.</p> <p>Returns:</p> Type Description <code>Tuple[bool, str]</code> <p>Tuple of (should_recalibrate, reason)</p> Source code in <code>pyadm1ode_calibration/calibration/methods/online.py</code> <pre><code>def should_recalibrate(\n    self, recent_measurements: MeasurementData, objectives: Optional[List[str]] = None\n) -&gt; Tuple[bool, str]:\n    \"\"\"Check if re-calibration should be triggered.\n\n    Returns:\n        Tuple of (should_recalibrate, reason)\n    \"\"\"\n    if not self.trigger.enabled:\n        return False, \"Disabled\"\n\n    if objectives is None:\n        objectives = [\"Q_ch4\", \"pH\"]\n\n    if self.state.last_calibration_time:\n        hours = (datetime.now() - self.state.last_calibration_time).total_seconds() / 3600\n        if hours &lt; self.trigger.time_threshold:\n            return False, f\"Too soon since last calibration ({hours:.1f}h)\"\n\n    variance = self._calculate_prediction_variance(recent_measurements, self._get_current_parameters(), objectives)\n    self.state.current_variance = variance\n\n    if variance &gt; self.trigger.variance_threshold:\n        self.state.consecutive_violations += 1\n        if self.state.consecutive_violations &gt;= self.trigger.consecutive_violations:\n            return True, f\"Variance {variance:.4f} &gt; {self.trigger.variance_threshold}\"\n    else:\n        self.state.consecutive_violations = 0\n\n    return False, \"Prediction within accuracy threshold\"\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds","title":"<code>pyadm1ode_calibration.calibration.ParameterBounds</code>","text":"<p>Manager for parameter bounds in ADM1 calibration.</p> <p>Provides methods for accessing bounds, validating parameters, and calculating penalties for constraint violations.</p> <p>Attributes:</p> Name Type Description <code>bounds</code> <code>Dict[str, ParameterBound]</code> <p>Dictionary mapping parameter names to ParameterBound objects</p> Example <p>bounds = ParameterBounds() bounds.add_bound(\"k_dis\", lower=0.3, upper=0.8, default=0.5) is_valid = bounds.is_within_bounds(\"k_dis\", 0.6)</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>class ParameterBounds:\n    \"\"\"\n    Manager for parameter bounds in ADM1 calibration.\n\n    Provides methods for accessing bounds, validating parameters,\n    and calculating penalties for constraint violations.\n\n    Attributes:\n        bounds: Dictionary mapping parameter names to ParameterBound objects\n\n    Example:\n        &gt;&gt;&gt; bounds = ParameterBounds()\n        &gt;&gt;&gt; bounds.add_bound(\"k_dis\", lower=0.3, upper=0.8, default=0.5)\n        &gt;&gt;&gt; is_valid = bounds.is_within_bounds(\"k_dis\", 0.6)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize empty parameter bounds manager.\"\"\"\n        self.bounds: Dict[str, ParameterBound] = {}\n\n    def add_bound(\n        self,\n        name: str,\n        lower: float,\n        upper: float,\n        default: float,\n        bound_type: BoundType = BoundType.HARD,\n        penalty_weight: float = 1.0,\n        description: str = \"\",\n        unit: str = \"\",\n        substrate_dependent: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Add parameter bound.\n\n        Args:\n            name: Parameter name\n            lower: Lower bound\n            upper: Upper bound\n            default: Default value\n            bound_type: Type of bound\n            penalty_weight: Penalty weight for soft constraints\n            description: Parameter description\n            unit: Parameter unit\n            substrate_dependent: Whether bound varies by substrate\n        \"\"\"\n        self.bounds[name] = ParameterBound(\n            name=name,\n            lower=lower,\n            upper=upper,\n            default=default,\n            bound_type=bound_type,\n            penalty_weight=penalty_weight,\n            description=description,\n            unit=unit,\n            substrate_dependent=substrate_dependent,\n        )\n\n    def get_bounds(self, name: str) -&gt; Optional[ParameterBound]:\n        \"\"\"\n        Get bounds for parameter.\n\n        Args:\n            name: Parameter name\n\n        Returns:\n            ParameterBound object or None if not found\n        \"\"\"\n        return self.bounds.get(name)\n\n    def get_bounds_tuple(self, name: str) -&gt; Optional[Tuple[float, float]]:\n        \"\"\"\n        Get bounds as tuple (lower, upper).\n\n        Args:\n            name: Parameter name\n\n        Returns:\n            Tuple of (lower, upper) or None\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return None\n        return (bound.lower, bound.upper)\n\n    def is_within_bounds(self, name: str, value: float, tolerance: float = 0.0) -&gt; bool:\n        \"\"\"\n        Check if parameter value is within bounds.\n\n        Args:\n            name: Parameter name\n            value: Parameter value\n            tolerance: Tolerance for boundary\n\n        Returns:\n            True if within bounds, False otherwise\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return True  # No bounds defined, assume valid\n        return bound.is_within_bounds(value, tolerance)\n\n    def clip_to_bounds(self, name: str, value: float) -&gt; float:\n        \"\"\"\n        Clip parameter value to bounds.\n\n        Args:\n            name: Parameter name\n            value: Parameter value\n\n        Returns:\n            Clipped value\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return value\n        return bound.clip_to_bounds(value)\n\n    def calculate_penalty(self, name: str, value: float, penalty_type: str = \"quadratic\") -&gt; float:\n        \"\"\"\n        Calculate penalty for parameter value.\n\n        Args:\n            name: Parameter name\n            value: Parameter value\n            penalty_type: Type of penalty function\n\n        Returns:\n            Penalty value\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return 0.0\n        return bound.calculate_penalty(value, penalty_type)\n\n    def calculate_total_penalty(self, parameters: Dict[str, float], penalty_type: str = \"quadratic\") -&gt; float:\n        \"\"\"\n        Calculate total penalty for all parameters.\n\n        Args:\n            parameters: Dictionary of parameter values\n            penalty_type: Type of penalty function\n\n        Returns:\n            Total penalty\n        \"\"\"\n        total_penalty = 0.0\n        for name, value in parameters.items():\n            penalty = self.calculate_penalty(name, value, penalty_type)\n            if np.isinf(penalty):\n                return np.inf\n            total_penalty += penalty\n        return total_penalty\n\n    def validate_parameters(self, parameters: Dict[str, float], raise_on_invalid: bool = False) -&gt; Tuple[bool, List[str]]:\n        \"\"\"\n        Validate all parameters against bounds.\n\n        Args:\n            parameters: Dictionary of parameter values\n            raise_on_invalid: Raise exception if invalid\n\n        Returns:\n            Tuple of (all_valid, list of error messages)\n        \"\"\"\n        errors = []\n\n        for name, value in parameters.items():\n            bound = self.get_bounds(name)\n            if bound is None:\n                continue\n\n            if not bound.is_within_bounds(value):\n                error = f\"Parameter '{name}' = {value:.4f} is outside bounds [{bound.lower:.4f}, {bound.upper:.4f}]\"\n                errors.append(error)\n\n        if errors and raise_on_invalid:\n            raise ValueError(\"\\n\".join(errors))\n\n        return (len(errors) == 0, errors)\n\n    def get_default_values(self, parameter_names: List[str]) -&gt; Dict[str, float]:\n        \"\"\"\n        Get default values for parameters.\n\n        Args:\n            parameter_names: List of parameter names\n\n        Returns:\n            Dictionary of default values\n        \"\"\"\n        defaults = {}\n        for name in parameter_names:\n            bound = self.get_bounds(name)\n            if bound is not None:\n                defaults[name] = bound.default\n        return defaults\n\n    def scale_to_unit_interval(self, name: str, value: float) -&gt; float:\n        \"\"\"\n        Scale parameter value to unit interval [0, 1].\n\n        Useful for optimization algorithms that work better with\n        normalized parameters.\n\n        Args:\n            name: Parameter name\n            value: Parameter value\n\n        Returns:\n            Scaled value in [0, 1]\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return value\n\n        if bound.upper == bound.lower:\n            return 0.5\n\n        return (value - bound.lower) / (bound.upper - bound.lower)\n\n    def unscale_from_unit_interval(self, name: str, scaled_value: float) -&gt; float:\n        \"\"\"\n        Unscale parameter value from unit interval [0, 1].\n\n        Args:\n            name: Parameter name\n            scaled_value: Scaled value in [0, 1]\n\n        Returns:\n            Unscaled parameter value\n        \"\"\"\n        bound = self.get_bounds(name)\n        if bound is None:\n            return scaled_value\n\n        return bound.lower + scaled_value * (bound.upper - bound.lower)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds-functions","title":"Functions","text":""},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.__init__","title":"<code>__init__()</code>","text":"<p>Initialize empty parameter bounds manager.</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize empty parameter bounds manager.\"\"\"\n    self.bounds: Dict[str, ParameterBound] = {}\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.add_bound","title":"<code>add_bound(name, lower, upper, default, bound_type=BoundType.HARD, penalty_weight=1.0, description='', unit='', substrate_dependent=False)</code>","text":"<p>Add parameter bound.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>lower</code> <code>float</code> <p>Lower bound</p> required <code>upper</code> <code>float</code> <p>Upper bound</p> required <code>default</code> <code>float</code> <p>Default value</p> required <code>bound_type</code> <code>BoundType</code> <p>Type of bound</p> <code>HARD</code> <code>penalty_weight</code> <code>float</code> <p>Penalty weight for soft constraints</p> <code>1.0</code> <code>description</code> <code>str</code> <p>Parameter description</p> <code>''</code> <code>unit</code> <code>str</code> <p>Parameter unit</p> <code>''</code> <code>substrate_dependent</code> <code>bool</code> <p>Whether bound varies by substrate</p> <code>False</code> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def add_bound(\n    self,\n    name: str,\n    lower: float,\n    upper: float,\n    default: float,\n    bound_type: BoundType = BoundType.HARD,\n    penalty_weight: float = 1.0,\n    description: str = \"\",\n    unit: str = \"\",\n    substrate_dependent: bool = False,\n) -&gt; None:\n    \"\"\"\n    Add parameter bound.\n\n    Args:\n        name: Parameter name\n        lower: Lower bound\n        upper: Upper bound\n        default: Default value\n        bound_type: Type of bound\n        penalty_weight: Penalty weight for soft constraints\n        description: Parameter description\n        unit: Parameter unit\n        substrate_dependent: Whether bound varies by substrate\n    \"\"\"\n    self.bounds[name] = ParameterBound(\n        name=name,\n        lower=lower,\n        upper=upper,\n        default=default,\n        bound_type=bound_type,\n        penalty_weight=penalty_weight,\n        description=description,\n        unit=unit,\n        substrate_dependent=substrate_dependent,\n    )\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.calculate_penalty","title":"<code>calculate_penalty(name, value, penalty_type='quadratic')</code>","text":"<p>Calculate penalty for parameter value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>float</code> <p>Parameter value</p> required <code>penalty_type</code> <code>str</code> <p>Type of penalty function</p> <code>'quadratic'</code> <p>Returns:</p> Type Description <code>float</code> <p>Penalty value</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def calculate_penalty(self, name: str, value: float, penalty_type: str = \"quadratic\") -&gt; float:\n    \"\"\"\n    Calculate penalty for parameter value.\n\n    Args:\n        name: Parameter name\n        value: Parameter value\n        penalty_type: Type of penalty function\n\n    Returns:\n        Penalty value\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return 0.0\n    return bound.calculate_penalty(value, penalty_type)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.calculate_total_penalty","title":"<code>calculate_total_penalty(parameters, penalty_type='quadratic')</code>","text":"<p>Calculate total penalty for all parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Dictionary of parameter values</p> required <code>penalty_type</code> <code>str</code> <p>Type of penalty function</p> <code>'quadratic'</code> <p>Returns:</p> Type Description <code>float</code> <p>Total penalty</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def calculate_total_penalty(self, parameters: Dict[str, float], penalty_type: str = \"quadratic\") -&gt; float:\n    \"\"\"\n    Calculate total penalty for all parameters.\n\n    Args:\n        parameters: Dictionary of parameter values\n        penalty_type: Type of penalty function\n\n    Returns:\n        Total penalty\n    \"\"\"\n    total_penalty = 0.0\n    for name, value in parameters.items():\n        penalty = self.calculate_penalty(name, value, penalty_type)\n        if np.isinf(penalty):\n            return np.inf\n        total_penalty += penalty\n    return total_penalty\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.clip_to_bounds","title":"<code>clip_to_bounds(name, value)</code>","text":"<p>Clip parameter value to bounds.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>float</code> <p>Parameter value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Clipped value</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def clip_to_bounds(self, name: str, value: float) -&gt; float:\n    \"\"\"\n    Clip parameter value to bounds.\n\n    Args:\n        name: Parameter name\n        value: Parameter value\n\n    Returns:\n        Clipped value\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return value\n    return bound.clip_to_bounds(value)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.get_bounds","title":"<code>get_bounds(name)</code>","text":"<p>Get bounds for parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>Optional[ParameterBound]</code> <p>ParameterBound object or None if not found</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def get_bounds(self, name: str) -&gt; Optional[ParameterBound]:\n    \"\"\"\n    Get bounds for parameter.\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        ParameterBound object or None if not found\n    \"\"\"\n    return self.bounds.get(name)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.get_bounds_tuple","title":"<code>get_bounds_tuple(name)</code>","text":"<p>Get bounds as tuple (lower, upper).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <p>Returns:</p> Type Description <code>Optional[Tuple[float, float]]</code> <p>Tuple of (lower, upper) or None</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def get_bounds_tuple(self, name: str) -&gt; Optional[Tuple[float, float]]:\n    \"\"\"\n    Get bounds as tuple (lower, upper).\n\n    Args:\n        name: Parameter name\n\n    Returns:\n        Tuple of (lower, upper) or None\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return None\n    return (bound.lower, bound.upper)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.get_default_values","title":"<code>get_default_values(parameter_names)</code>","text":"<p>Get default values for parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_names</code> <code>List[str]</code> <p>List of parameter names</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of default values</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def get_default_values(self, parameter_names: List[str]) -&gt; Dict[str, float]:\n    \"\"\"\n    Get default values for parameters.\n\n    Args:\n        parameter_names: List of parameter names\n\n    Returns:\n        Dictionary of default values\n    \"\"\"\n    defaults = {}\n    for name in parameter_names:\n        bound = self.get_bounds(name)\n        if bound is not None:\n            defaults[name] = bound.default\n    return defaults\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.is_within_bounds","title":"<code>is_within_bounds(name, value, tolerance=0.0)</code>","text":"<p>Check if parameter value is within bounds.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>float</code> <p>Parameter value</p> required <code>tolerance</code> <code>float</code> <p>Tolerance for boundary</p> <code>0.0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if within bounds, False otherwise</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def is_within_bounds(self, name: str, value: float, tolerance: float = 0.0) -&gt; bool:\n    \"\"\"\n    Check if parameter value is within bounds.\n\n    Args:\n        name: Parameter name\n        value: Parameter value\n        tolerance: Tolerance for boundary\n\n    Returns:\n        True if within bounds, False otherwise\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return True  # No bounds defined, assume valid\n    return bound.is_within_bounds(value, tolerance)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.scale_to_unit_interval","title":"<code>scale_to_unit_interval(name, value)</code>","text":"<p>Scale parameter value to unit interval [0, 1].</p> <p>Useful for optimization algorithms that work better with normalized parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>float</code> <p>Parameter value</p> required <p>Returns:</p> Type Description <code>float</code> <p>Scaled value in [0, 1]</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def scale_to_unit_interval(self, name: str, value: float) -&gt; float:\n    \"\"\"\n    Scale parameter value to unit interval [0, 1].\n\n    Useful for optimization algorithms that work better with\n    normalized parameters.\n\n    Args:\n        name: Parameter name\n        value: Parameter value\n\n    Returns:\n        Scaled value in [0, 1]\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return value\n\n    if bound.upper == bound.lower:\n        return 0.5\n\n    return (value - bound.lower) / (bound.upper - bound.lower)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.unscale_from_unit_interval","title":"<code>unscale_from_unit_interval(name, scaled_value)</code>","text":"<p>Unscale parameter value from unit interval [0, 1].</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>scaled_value</code> <code>float</code> <p>Scaled value in [0, 1]</p> required <p>Returns:</p> Type Description <code>float</code> <p>Unscaled parameter value</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def unscale_from_unit_interval(self, name: str, scaled_value: float) -&gt; float:\n    \"\"\"\n    Unscale parameter value from unit interval [0, 1].\n\n    Args:\n        name: Parameter name\n        scaled_value: Scaled value in [0, 1]\n\n    Returns:\n        Unscaled parameter value\n    \"\"\"\n    bound = self.get_bounds(name)\n    if bound is None:\n        return scaled_value\n\n    return bound.lower + scaled_value * (bound.upper - bound.lower)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.ParameterBounds.validate_parameters","title":"<code>validate_parameters(parameters, raise_on_invalid=False)</code>","text":"<p>Validate all parameters against bounds.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Dictionary of parameter values</p> required <code>raise_on_invalid</code> <code>bool</code> <p>Raise exception if invalid</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[bool, List[str]]</code> <p>Tuple of (all_valid, list of error messages)</p> Source code in <code>pyadm1ode_calibration/calibration/parameter_bounds.py</code> <pre><code>def validate_parameters(self, parameters: Dict[str, float], raise_on_invalid: bool = False) -&gt; Tuple[bool, List[str]]:\n    \"\"\"\n    Validate all parameters against bounds.\n\n    Args:\n        parameters: Dictionary of parameter values\n        raise_on_invalid: Raise exception if invalid\n\n    Returns:\n        Tuple of (all_valid, list of error messages)\n    \"\"\"\n    errors = []\n\n    for name, value in parameters.items():\n        bound = self.get_bounds(name)\n        if bound is None:\n            continue\n\n        if not bound.is_within_bounds(value):\n            error = f\"Parameter '{name}' = {value:.4f} is outside bounds [{bound.lower:.4f}, {bound.upper:.4f}]\"\n            errors.append(error)\n\n    if errors and raise_on_invalid:\n        raise ValueError(\"\\n\".join(errors))\n\n    return (len(errors) == 0, errors)\n</code></pre>"},{"location":"api/calibration/#pyadm1ode_calibration.calibration.CalibrationValidator","title":"<code>pyadm1ode_calibration.calibration.CalibrationValidator</code>","text":"<p>Validator for calibrated model parameters.</p> Source code in <code>pyadm1ode_calibration/calibration/validation.py</code> <pre><code>class CalibrationValidator:\n    \"\"\"Validator for calibrated model parameters.\"\"\"\n\n    def __init__(self, plant: Any, verbose: bool = True):\n        self.plant = plant\n        self.verbose = verbose\n\n    def validate(\n        self,\n        parameters: Dict[str, float],\n        measurements: MeasurementData,\n        objectives: Optional[List[str]] = None,\n        simulation_duration: Optional[float] = None,\n    ) -&gt; Dict[str, ValidationMetrics]:\n        if objectives is None:\n            objectives = [\"Q_ch4\", \"pH\", \"VFA\"]\n\n        self._apply_parameters(parameters)\n\n        if simulation_duration is None:\n            simulation_duration = len(measurements) * (1.0 / 24.0)\n\n        simulated_outputs = self._simulate_plant(measurements, simulation_duration)\n\n        metrics = {}\n        for objective in objectives:\n            if objective not in simulated_outputs:\n                warnings.warn(f\"Objective '{objective}' not in simulation outputs\")\n                continue\n\n            observed = self._extract_measurements(measurements, objective)\n            predicted = simulated_outputs[objective]\n\n            observed, predicted = self._align_arrays(observed, predicted)\n\n            if len(observed) == 0:\n                warnings.warn(f\"No valid data for objective '{objective}'\")\n                continue\n\n            obj_metrics = self._calculate_metrics(objective, observed, predicted)\n            metrics[objective] = obj_metrics\n\n        return metrics\n\n    def analyze_residuals(\n        self,\n        measurements: MeasurementData,\n        simulated: Dict[str, np.ndarray],\n        objectives: Optional[List[str]] = None,\n    ) -&gt; Dict[str, ResidualAnalysis]:\n        if objectives is None:\n            objectives = list(simulated.keys())\n\n        results = {}\n        for objective in objectives:\n            if objective not in simulated:\n                continue\n\n            observed = self._extract_measurements(measurements, objective)\n            predicted = simulated[objective]\n\n            observed, predicted = self._align_arrays(observed, predicted)\n\n            if len(observed) &lt; 3:\n                continue\n\n            residuals = observed - predicted\n            std_residuals = self._standardize_residuals(residuals)\n            normality = self._test_normality(residuals)\n            autocorr = self._calculate_autocorrelation(residuals)\n            hetero = self._test_heteroscedasticity(residuals, predicted)\n            outliers = np.where(np.abs(std_residuals) &gt; 3)[0].tolist()\n\n            results[objective] = ResidualAnalysis(\n                objective=objective,\n                residuals=residuals,\n                standardized_residuals=std_residuals,\n                normality_test=normality,\n                autocorrelation=autocorr,\n                heteroscedasticity_test=hetero,\n                outlier_indices=outliers,\n            )\n\n        return results\n\n    def cross_validate(\n        self,\n        parameters: Dict[str, float],\n        measurements: MeasurementData,\n        n_folds: int = 5,\n        objectives: Optional[List[str]] = None,\n    ) -&gt; Dict[str, List[ValidationMetrics]]:\n        if objectives is None:\n            objectives = [\"Q_ch4\", \"pH\", \"VFA\"]\n\n        n_samples = len(measurements)\n        fold_size = n_samples // n_folds\n        cv_results: Dict[str, List[ValidationMetrics]] = {obj: [] for obj in objectives}\n\n        for fold in range(n_folds):\n            start_idx = fold * fold_size\n            end_idx = start_idx + fold_size if fold &lt; n_folds - 1 else n_samples\n\n            val_data = measurements.data.iloc[start_idx:end_idx].copy()\n            val_measurements = type(measurements)(val_data)\n\n            fold_metrics = self.validate(parameters, val_measurements, objectives)\n            for obj, metrics in fold_metrics.items():\n                cv_results[obj].append(metrics)\n\n        return cv_results\n\n    def _apply_parameters(self, parameters: Dict[str, float]) -&gt; None:\n        \"\"\"Apply parameters to plant.\"\"\"\n        for component in self.plant.components.values():\n            if component.component_type.value == \"digester\":\n                if not hasattr(component, \"_calibration_params\"):\n                    component._calibration_params = {}\n                component._calibration_params.update(parameters)\n\n    def _simulate_plant(self, measurements: MeasurementData, duration: float) -&gt; Dict[str, np.ndarray]:\n        # Implementation depends on plant model\n        dt = 1.0 / 24.0\n        results = self.plant.simulate(duration=duration, dt=dt, save_interval=dt)\n        return self._extract_outputs_from_results(results)\n\n    def _extract_outputs_from_results(self, results: List[Dict[str, Any]]) -&gt; Dict[str, np.ndarray]:\n        outputs: Dict[str, List[float]] = {\"Q_ch4\": [], \"pH\": [], \"VFA\": [], \"TAC\": []}\n        for result in results:\n            comp_data = next(iter(result[\"components\"].values()))\n            for key in outputs:\n                outputs[key].append(comp_data.get(key, 0.0))\n        return {k: np.array(v) for k, v in outputs.items()}\n\n    def _extract_measurements(self, measurements: MeasurementData, objective: str) -&gt; np.ndarray:\n        \"\"\"Extract measurement array for objective.\n\n        Raises:\n            DataValidationError: If objective not found or empty\n        \"\"\"\n        if objective not in measurements.data.columns:\n            raise DataValidationError(f\"Objective '{objective}' not found in measurements\")\n\n        series = measurements.get_measurement(objective)\n        if series.empty:\n            raise DataValidationError(f\"No valid data for objective '{objective}'\")\n\n        return series.values\n\n    def _align_arrays(self, observed: np.ndarray, predicted: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        min_len = min(len(observed), len(predicted))\n        observed, predicted = observed[:min_len], predicted[:min_len]\n        valid = ~(np.isnan(observed) | np.isnan(predicted))\n        return observed[valid], predicted[valid]\n\n    def _calculate_metrics(self, objective: str, observed: np.ndarray, predicted: np.ndarray) -&gt; ValidationMetrics:\n        n = len(observed)\n        obs_mean, obs_std = np.mean(observed), np.std(observed)\n        pred_mean, pred_std = np.mean(predicted), np.std(predicted)\n        residuals = observed - predicted\n\n        rmse = np.sqrt(np.mean(residuals**2))\n        mae = np.mean(np.abs(residuals))\n\n        ss_res = np.sum(residuals**2)\n        ss_tot = np.sum((observed - obs_mean) ** 2)\n        r2 = float(1 - (ss_res / ss_tot) if ss_tot &gt; 0 else 0.0)\n\n        pbias = float((np.sum(residuals) / np.sum(observed)) * 100 if np.sum(observed) != 0 else 0.0)\n        correlation = float(np.corrcoef(observed, predicted)[0, 1] if n &gt; 1 else 0.0)\n\n        nonzero = observed != 0\n        mape = float(np.mean(np.abs(residuals[nonzero] / observed[nonzero])) * 100 if np.any(nonzero) else 0.0)\n        me = float(np.mean(residuals))\n\n        return ValidationMetrics(\n            objective=objective,\n            n_samples=n,\n            rmse=float(rmse),\n            mae=float(mae),\n            r2=r2,\n            nse=r2,\n            pbias=pbias,\n            correlation=correlation,\n            mape=mape,\n            me=me,\n            observations_mean=float(obs_mean),\n            observations_std=float(obs_std),\n            predictions_mean=float(pred_mean),\n            predictions_std=float(pred_std),\n        )\n\n    def _standardize_residuals(self, residuals: np.ndarray) -&gt; np.ndarray:\n        std = np.std(residuals)\n        return (residuals - np.mean(residuals)) / std if std &gt; 0 else np.zeros_like(residuals)\n\n    def _test_normality(self, residuals: np.ndarray) -&gt; Dict[str, float]:\n        try:\n            stat, p = stats.shapiro(residuals)\n            return {\"statistic\": float(stat), \"p_value\": float(p)}\n        except Exception:\n            return {\"statistic\": 0.0, \"p_value\": 1.0}\n\n    def _calculate_autocorrelation(self, residuals: np.ndarray) -&gt; float:\n        if len(residuals) &lt; 2:\n            return 0.0\n        res_centered = residuals - np.mean(residuals)\n        return float(np.corrcoef(res_centered[:-1], res_centered[1:])[0, 1])\n\n    def _test_heteroscedasticity(self, residuals: np.ndarray, predicted: np.ndarray) -&gt; Dict[str, float]:\n        try:\n            corr = np.corrcoef(residuals**2, predicted)[0, 1]\n            stat = len(residuals) * corr**2\n            p = 1 - stats.chi2.cdf(stat, df=1)\n            return {\"statistic\": float(stat), \"p_value\": float(p)}\n        except Exception:\n            return {\"statistic\": 0.0, \"p_value\": 1.0}\n</code></pre>"},{"location":"api/io/","title":"IO API","text":""},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData","title":"<code>pyadm1ode_calibration.io.MeasurementData</code>","text":"<p>Container for biogas plant measurement data.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>class MeasurementData:\n    \"\"\"\n    Container for biogas plant measurement data.\n    \"\"\"\n\n    def __init__(self, data: pd.DataFrame, metadata: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Initialize measurement data.\n\n        Args:\n            data: DataFrame with measurements\n            metadata: Optional metadata dictionary\n        \"\"\"\n        self.data = data\n        self.metadata = metadata or {}\n\n        if \"timestamp\" in self.data.columns:\n            if not pd.api.types.is_datetime64_any_dtype(self.data[\"timestamp\"]):\n                self.data[\"timestamp\"] = pd.to_datetime(self.data[\"timestamp\"])\n            self.data = self.data.set_index(\"timestamp\").sort_index()\n\n    @classmethod\n    def from_csv(\n        cls,\n        filepath: str,\n        timestamp_column: str = \"timestamp\",\n        sep: str = \",\",\n        parse_dates: bool = True,\n        resample: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; \"MeasurementData\":\n        \"\"\"Load measurement data from CSV file.\"\"\"\n        data = pd.read_csv(filepath, sep=sep, **kwargs)\n        if timestamp_column in data.columns:\n            data[\"timestamp\"] = pd.to_datetime(data[timestamp_column])\n            if timestamp_column != \"timestamp\":\n                data = data.drop(columns=[timestamp_column])\n        instance = cls(data)\n        if resample is not None:\n            instance.resample(resample)\n        return instance\n\n    def validate(\n        self, required_columns: Optional[List[str]] = None, expected_ranges: Optional[Dict[str, Tuple[float, float]]] = None\n    ) -&gt; ValidationResult:\n        \"\"\"Validate measurement data.\"\"\"\n        if expected_ranges is None:\n            expected_ranges = {\n                \"pH\": (5.0, 9.0),\n                \"VFA\": (0.0, 20.0),\n                \"TAC\": (0.0, 50.0),\n                \"Q_gas\": (0.0, 5000.0),\n                \"Q_ch4\": (0.0, 3000.0),\n                \"T_digester\": (273.15, 333.15),\n            }\n        return DataValidator.validate(self.data, required_columns=required_columns, expected_ranges=expected_ranges)\n\n    def remove_outliers(\n        self, columns: Optional[List[str]] = None, method: str = \"zscore\", threshold: float = 3.0, **kwargs: Any\n    ) -&gt; int:\n        \"\"\"Remove outliers from specified columns.\"\"\"\n        if columns is None:\n            columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n\n        n_outliers = 0\n        for col in columns:\n            if col not in self.data.columns:\n                continue\n            if method == \"zscore\":\n                is_outlier = OutlierDetector.detect_zscore(self.data[col], threshold=threshold)\n            elif method == \"iqr\":\n                is_outlier = OutlierDetector.detect_iqr(self.data[col], multiplier=threshold)\n            elif method == \"moving_window\":\n                window = kwargs.get(\"window\", 5)\n                is_outlier = OutlierDetector.detect_moving_window(self.data[col], window=window, threshold=threshold)\n            else:\n                raise ValueError(f\"Unknown outlier detection method: {method}\")\n\n            n_col_outliers = int(is_outlier.sum())\n            self.data.loc[is_outlier, col] = np.nan\n            n_outliers += n_col_outliers\n        return n_outliers\n\n    def fill_gaps(self, columns: Optional[List[str]] = None, method: str = \"interpolate\", **kwargs: Any) -&gt; None:\n        \"\"\"Fill missing values in time series.\"\"\"\n        if columns is None:\n            columns = self.data.columns.tolist()\n\n        for col in columns:\n            if col not in self.data.columns:\n                continue\n            if method == \"interpolate\":\n                limit = kwargs.get(\"limit\", None)\n                self.data[col] = self.data[col].interpolate(method=\"linear\", limit=limit)\n            elif method == \"forward\":\n                limit = kwargs.get(\"limit\", None)\n                self.data[col] = self.data[col].ffill(limit=limit)\n            elif method == \"backward\":\n                limit = kwargs.get(\"limit\", None)\n                self.data[col] = self.data[col].bfill(limit=limit)\n            elif method == \"mean\":\n                self.data[col] = self.data[col].fillna(self.data[col].mean())\n            elif method == \"median\":\n                self.data[col] = self.data[col].fillna(self.data[col].median())\n            else:\n                raise ValueError(f\"Unknown fill method: {method}\")\n\n    def resample(self, freq: str, aggregation: str = \"mean\") -&gt; None:\n        \"\"\"Resample time series.\"\"\"\n        resampler = self.data.resample(freq)\n        if aggregation == \"mean\":\n            self.data = resampler.mean()\n        elif aggregation == \"sum\":\n            self.data = resampler.sum()\n        elif aggregation == \"first\":\n            self.data = resampler.first()\n        elif aggregation == \"last\":\n            self.data = resampler.last()\n        else:\n            raise ValueError(f\"Unknown aggregation method: {aggregation}\")\n\n    def get_measurement(\n        self, column: str, start_time: Optional[Union[str, datetime]] = None, end_time: Optional[Union[str, datetime]] = None\n    ) -&gt; pd.Series:\n        \"\"\"Get measurement time series.\"\"\"\n        if column not in self.data.columns:\n            raise ValueError(f\"Column '{column}' not found\")\n        series = self.data[column]\n        if start_time is not None or end_time is not None:\n            series = series.loc[start_time:end_time]  # type: ignore\n        return series\n\n    def get_substrate_feeds(self, substrate_columns: Optional[List[str]] = None) -&gt; np.ndarray:\n        \"\"\"Get substrate feed rates as array.\"\"\"\n        if substrate_columns is None:\n            substrate_columns = [col for col in self.data.columns if col.startswith(\"Q_sub\")]\n        if not substrate_columns:\n            raise ValueError(\"No substrate columns found\")\n        return self.data[substrate_columns].values\n\n    def get_time_window(self, start_time: Union[str, datetime], end_time: Union[str, datetime]) -&gt; \"MeasurementData\":\n        \"\"\"Get data for specific time window.\"\"\"\n        windowed_data = self.data.loc[start_time:end_time].copy()  # type: ignore\n        return MeasurementData(windowed_data, metadata=self.metadata.copy())\n\n    def summary(self) -&gt; pd.DataFrame:\n        \"\"\"Get statistical summary of measurements.\"\"\"\n        return self.data.describe()\n\n    def to_csv(self, filepath: str, **kwargs: Any) -&gt; None:\n        \"\"\"Save measurement data to CSV.\"\"\"\n        self.data.to_csv(filepath, **kwargs)\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __repr__(self) -&gt; str:\n        try:\n            time_range = f\"{self.data.index[0]} to {self.data.index[-1]}\"\n        except Exception:\n            time_range = \"empty\"\n        return f\"MeasurementData(n_rows={len(self.data)}, n_columns={len(self.data.columns)}, time_range={time_range})\"\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData-functions","title":"Functions","text":""},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.__init__","title":"<code>__init__(data, metadata=None)</code>","text":"<p>Initialize measurement data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with measurements</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional metadata dictionary</p> <code>None</code> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def __init__(self, data: pd.DataFrame, metadata: Optional[Dict[str, Any]] = None):\n    \"\"\"\n    Initialize measurement data.\n\n    Args:\n        data: DataFrame with measurements\n        metadata: Optional metadata dictionary\n    \"\"\"\n    self.data = data\n    self.metadata = metadata or {}\n\n    if \"timestamp\" in self.data.columns:\n        if not pd.api.types.is_datetime64_any_dtype(self.data[\"timestamp\"]):\n            self.data[\"timestamp\"] = pd.to_datetime(self.data[\"timestamp\"])\n        self.data = self.data.set_index(\"timestamp\").sort_index()\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.fill_gaps","title":"<code>fill_gaps(columns=None, method='interpolate', **kwargs)</code>","text":"<p>Fill missing values in time series.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def fill_gaps(self, columns: Optional[List[str]] = None, method: str = \"interpolate\", **kwargs: Any) -&gt; None:\n    \"\"\"Fill missing values in time series.\"\"\"\n    if columns is None:\n        columns = self.data.columns.tolist()\n\n    for col in columns:\n        if col not in self.data.columns:\n            continue\n        if method == \"interpolate\":\n            limit = kwargs.get(\"limit\", None)\n            self.data[col] = self.data[col].interpolate(method=\"linear\", limit=limit)\n        elif method == \"forward\":\n            limit = kwargs.get(\"limit\", None)\n            self.data[col] = self.data[col].ffill(limit=limit)\n        elif method == \"backward\":\n            limit = kwargs.get(\"limit\", None)\n            self.data[col] = self.data[col].bfill(limit=limit)\n        elif method == \"mean\":\n            self.data[col] = self.data[col].fillna(self.data[col].mean())\n        elif method == \"median\":\n            self.data[col] = self.data[col].fillna(self.data[col].median())\n        else:\n            raise ValueError(f\"Unknown fill method: {method}\")\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.from_csv","title":"<code>from_csv(filepath, timestamp_column='timestamp', sep=',', parse_dates=True, resample=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load measurement data from CSV file.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    filepath: str,\n    timestamp_column: str = \"timestamp\",\n    sep: str = \",\",\n    parse_dates: bool = True,\n    resample: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; \"MeasurementData\":\n    \"\"\"Load measurement data from CSV file.\"\"\"\n    data = pd.read_csv(filepath, sep=sep, **kwargs)\n    if timestamp_column in data.columns:\n        data[\"timestamp\"] = pd.to_datetime(data[timestamp_column])\n        if timestamp_column != \"timestamp\":\n            data = data.drop(columns=[timestamp_column])\n    instance = cls(data)\n    if resample is not None:\n        instance.resample(resample)\n    return instance\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.get_measurement","title":"<code>get_measurement(column, start_time=None, end_time=None)</code>","text":"<p>Get measurement time series.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def get_measurement(\n    self, column: str, start_time: Optional[Union[str, datetime]] = None, end_time: Optional[Union[str, datetime]] = None\n) -&gt; pd.Series:\n    \"\"\"Get measurement time series.\"\"\"\n    if column not in self.data.columns:\n        raise ValueError(f\"Column '{column}' not found\")\n    series = self.data[column]\n    if start_time is not None or end_time is not None:\n        series = series.loc[start_time:end_time]  # type: ignore\n    return series\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.get_substrate_feeds","title":"<code>get_substrate_feeds(substrate_columns=None)</code>","text":"<p>Get substrate feed rates as array.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def get_substrate_feeds(self, substrate_columns: Optional[List[str]] = None) -&gt; np.ndarray:\n    \"\"\"Get substrate feed rates as array.\"\"\"\n    if substrate_columns is None:\n        substrate_columns = [col for col in self.data.columns if col.startswith(\"Q_sub\")]\n    if not substrate_columns:\n        raise ValueError(\"No substrate columns found\")\n    return self.data[substrate_columns].values\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.get_time_window","title":"<code>get_time_window(start_time, end_time)</code>","text":"<p>Get data for specific time window.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def get_time_window(self, start_time: Union[str, datetime], end_time: Union[str, datetime]) -&gt; \"MeasurementData\":\n    \"\"\"Get data for specific time window.\"\"\"\n    windowed_data = self.data.loc[start_time:end_time].copy()  # type: ignore\n    return MeasurementData(windowed_data, metadata=self.metadata.copy())\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.remove_outliers","title":"<code>remove_outliers(columns=None, method='zscore', threshold=3.0, **kwargs)</code>","text":"<p>Remove outliers from specified columns.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def remove_outliers(\n    self, columns: Optional[List[str]] = None, method: str = \"zscore\", threshold: float = 3.0, **kwargs: Any\n) -&gt; int:\n    \"\"\"Remove outliers from specified columns.\"\"\"\n    if columns is None:\n        columns = self.data.select_dtypes(include=[np.number]).columns.tolist()\n\n    n_outliers = 0\n    for col in columns:\n        if col not in self.data.columns:\n            continue\n        if method == \"zscore\":\n            is_outlier = OutlierDetector.detect_zscore(self.data[col], threshold=threshold)\n        elif method == \"iqr\":\n            is_outlier = OutlierDetector.detect_iqr(self.data[col], multiplier=threshold)\n        elif method == \"moving_window\":\n            window = kwargs.get(\"window\", 5)\n            is_outlier = OutlierDetector.detect_moving_window(self.data[col], window=window, threshold=threshold)\n        else:\n            raise ValueError(f\"Unknown outlier detection method: {method}\")\n\n        n_col_outliers = int(is_outlier.sum())\n        self.data.loc[is_outlier, col] = np.nan\n        n_outliers += n_col_outliers\n    return n_outliers\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.resample","title":"<code>resample(freq, aggregation='mean')</code>","text":"<p>Resample time series.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def resample(self, freq: str, aggregation: str = \"mean\") -&gt; None:\n    \"\"\"Resample time series.\"\"\"\n    resampler = self.data.resample(freq)\n    if aggregation == \"mean\":\n        self.data = resampler.mean()\n    elif aggregation == \"sum\":\n        self.data = resampler.sum()\n    elif aggregation == \"first\":\n        self.data = resampler.first()\n    elif aggregation == \"last\":\n        self.data = resampler.last()\n    else:\n        raise ValueError(f\"Unknown aggregation method: {aggregation}\")\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.summary","title":"<code>summary()</code>","text":"<p>Get statistical summary of measurements.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def summary(self) -&gt; pd.DataFrame:\n    \"\"\"Get statistical summary of measurements.\"\"\"\n    return self.data.describe()\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.to_csv","title":"<code>to_csv(filepath, **kwargs)</code>","text":"<p>Save measurement data to CSV.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def to_csv(self, filepath: str, **kwargs: Any) -&gt; None:\n    \"\"\"Save measurement data to CSV.\"\"\"\n    self.data.to_csv(filepath, **kwargs)\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.MeasurementData.validate","title":"<code>validate(required_columns=None, expected_ranges=None)</code>","text":"<p>Validate measurement data.</p> Source code in <code>pyadm1ode_calibration/io/loaders/measurement_data.py</code> <pre><code>def validate(\n    self, required_columns: Optional[List[str]] = None, expected_ranges: Optional[Dict[str, Tuple[float, float]]] = None\n) -&gt; ValidationResult:\n    \"\"\"Validate measurement data.\"\"\"\n    if expected_ranges is None:\n        expected_ranges = {\n            \"pH\": (5.0, 9.0),\n            \"VFA\": (0.0, 20.0),\n            \"TAC\": (0.0, 50.0),\n            \"Q_gas\": (0.0, 5000.0),\n            \"Q_ch4\": (0.0, 3000.0),\n            \"T_digester\": (273.15, 333.15),\n        }\n    return DataValidator.validate(self.data, required_columns=required_columns, expected_ranges=expected_ranges)\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler","title":"<code>pyadm1ode_calibration.io.CSVHandler</code>","text":"<p>Handler for CSV file operations in PyADM1.</p> <p>Supports reading and writing various CSV formats used in biogas plant operation and laboratory analysis.</p> Example <p>handler = CSVHandler() data = handler.load_substrate_lab_data(\"lab_results.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>class CSVHandler:\n    \"\"\"\n    Handler for CSV file operations in PyADM1.\n\n    Supports reading and writing various CSV formats used in biogas\n    plant operation and laboratory analysis.\n\n    Example:\n        &gt;&gt;&gt; handler = CSVHandler()\n        &gt;&gt;&gt; data = handler.load_substrate_lab_data(\"lab_results.csv\")\n    \"\"\"\n\n    # Standard column mappings (German -&gt; English)\n    COLUMN_MAPPINGS = {\n        # Dry matter and organic content\n        \"Trockensubstanz\": \"TS\",\n        \"Trockensubstanzgehalt\": \"TS\",\n        \"TS-Gehalt\": \"TS\",\n        \"Organische Trockensubstanz\": \"VS\",\n        \"oTS\": \"VS\",\n        \"Organischer Trockensubstanzgehalt\": \"oTS\",\n        \"Fermentierbare organische Trockensubstanz\": \"foTS\",\n        \"Fl\u00fcchtige Feststoffe\": \"VS\",\n        # Weender analysis\n        \"Rohprotein\": \"RP\",\n        \"Rohfett\": \"RL\",\n        \"Rohfaser\": \"RF\",\n        \"Rohasche\": \"RA\",\n        \"N-freie Extraktstoffe\": \"NfE\",\n        # Van Soest\n        \"Neutral Detergent Fiber\": \"NDF\",\n        \"Acid Detergent Fiber\": \"ADF\",\n        \"Acid Detergent Lignin\": \"ADL\",\n        # Chemical properties\n        \"pH-Wert\": \"pH\",\n        \"Ammoniumstickstoff\": \"NH4_N\",\n        \"Ammonium-N\": \"NH4_N\",\n        \"NH4-N\": \"NH4_N\",\n        \"Alkalinit\u00e4t\": \"TAC\",\n        \"Pufferkapazit\u00e4t\": \"TAC\",\n        \"CSB\": \"COD\",\n        \"CSB-Filtrat\": \"COD_S\",\n        \"Chemischer Sauerstoffbedarf\": \"COD\",\n        \"Chemischer Sauerstoffbedarf des Filtrats\": \"COD_S\",\n        # Biogas potential\n        \"Biogaspotential\": \"BMP\",\n        \"Methanpotential\": \"BMP\",\n        \"Biochemisches Methanpotential\": \"BMP\",\n        \"Gasausbeute\": \"BMP\",\n        # Carbon and nitrogen\n        \"Kohlenstoffgehalt\": \"C_content\",\n        \"Stickstoffgehalt\": \"N_content\",\n        \"C/N-Verh\u00e4ltnis\": \"C_to_N\",\n        \"C-N-Verh\u00e4ltnis\": \"C_to_N\",\n        \"Gesamt-Kjeldahl-Stickstoff\": \"TKN\",\n        # Measurement data\n        \"Zeitstempel\": \"timestamp\",\n        \"Zeit\": \"timestamp\",\n        \"Datum\": \"timestamp\",\n        \"Biogasproduktion\": \"Q_gas\",\n        \"Methanproduktion\": \"Q_ch4\",\n        \"Elektrische Leistung\": \"P_el\",\n        \"Thermische Leistung\": \"P_th\",\n        \"Temperatur\": \"T_digester\",\n    }\n\n    # Unit conversions (from -&gt; to, factor)\n    UNIT_CONVERSIONS = {\n        # Dry matter: % FM -&gt; % FM (no conversion needed, just validation)\n        (\"TS\", \"% FM\", \"% FM\"): 1.0,\n        (\"TS\", \"%FM\", \"% FM\"): 1.0,\n        (\"TS\", \"g/100g\", \"% FM\"): 1.0,\n        # Volatile solids: % TS -&gt; % TS\n        (\"VS\", \"% TS\", \"% TS\"): 1.0,\n        (\"VS\", \"%TS\", \"% TS\"): 1.0,\n        # BMP conversions\n        (\"BMP\", \"L/kg VS\", \"L CH4/kg oTS\"): 1.0,  # Assuming same\n        (\"BMP\", \"mL/g oTS\", \"L CH4/kg oTS\"): 1.0,  # mL/g = L/kg\n        (\"BMP\", \"Nm\u00b3/t oTS\", \"L CH4/kg oTS\"): 1.0,  # Nm\u00b3/t = L/kg\n        # Nitrogen content\n        (\"NH4_N\", \"mg/L\", \"g/L\"): 0.001,\n        (\"NH4_N\", \"g/kg\", \"g/L\"): 1.0,  # Approximate for density ~1\n        # Alkalinity\n        (\"TAC\", \"mmol/L\", \"mmol/L\"): 1.0,\n        (\"TAC\", \"meq/L\", \"mmol/L\"): 1.0,  # Equivalent\n        (\"TAC\", \"g CaCO3/L\", \"mmol/L\"): 20.0,  # 1 g CaCO3 = 20 mmol\n        # COD\n        (\"COD_S\", \"mg/L\", \"g/L\"): 0.001,\n        (\"COD_S\", \"g/L\", \"g/L\"): 1.0,\n    }\n\n    def __init__(self, decimal_separator: str = \".\", thousands_separator: str = \",\"):\n        \"\"\"\n        Initialize CSV handler.\n\n        Args:\n            decimal_separator: Decimal separator (\".\" or \",\")\n            thousands_separator: Thousands separator (\",\" or \".\" or \"\")\n        \"\"\"\n        self.decimal_separator = decimal_separator\n        self.thousands_separator = thousands_separator\n\n    # ========================================================================\n    # Substrate Laboratory Data\n    # ========================================================================\n\n    def load_substrate_lab_data(\n        self,\n        filepath: str,\n        substrate_name: Optional[str] = None,\n        substrate_type: Optional[str] = None,\n        sample_date: Optional[Union[str, datetime]] = None,\n        sep: str = \",\",\n        encoding: str = \"utf-8\",\n        validate: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Load substrate characterization data from laboratory CSV.\n\n        Expected columns (German or English):\n        - Trockensubstanzgehalt (TS) [% FM]\n        - Organische Trockensubstanz (VS) [% TS]\n        - Fermentierbare organische Trockensubstanz (foTS) [% TS]\n        - Rohprotein (RP) [% TS]\n        - Rohfett (RL) [% TS]\n        - Rohfaser (RF) [% TS]\n        - NDF, ADF, ADL [% TS]\n        - pH-Wert (pH)\n        - Ammoniumstickstoff (NH4-N) [g/L or mg/L]\n        - Alkalinit\u00e4t (TAC) [mmol/L]\n        - Biochemisches Methanpotential (BMP) [L CH4/kg oTS]\n        - CSB des Filtrats (COD_S) [g/L]\n\n        Args:\n            filepath: Path to CSV file\n            substrate_name: Substrate name (if not in file)\n            substrate_type: Substrate type (maize, manure, grass, etc.)\n            sample_date: Sample date (if not in file)\n            sep: Column separator\n            encoding: File encoding\n            validate: Validate data ranges\n\n        Returns:\n            Dict with substrate data\n\n        Example:\n            &gt;&gt;&gt; handler = CSVHandler()\n            &gt;&gt;&gt; data = handler.load_substrate_lab_data(\n            ...     \"maize_analysis.csv\",\n            ...     substrate_name=\"Maize silage batch 23\",\n            ...     substrate_type=\"maize\",\n            ...     sample_date=\"2024-01-15\"\n            ... )\n            &gt;&gt;&gt; print(f\"TS: {data['TS']:.1f}% FM\")\n        \"\"\"\n        # Auto-detect separator if needed\n        if sep == \"auto\":\n            sep = self._detect_separator(filepath)\n\n        # Read CSV\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n        # Try to detect if file is in \"vertical\" format (parameter, value, unit)\n        if len(df.columns) &lt;= 3 and \"Parameter\" in df.columns or \"Messgr\u00f6\u00dfe\" in df.columns:\n            df = self._parse_vertical_format(df)\n\n        # Map column names\n        df = self._map_column_names(df)\n\n        # If multiple rows, take the first one (or could aggregate)\n        if len(df) &gt; 1:\n            warnings.warn(f\"CSV contains {len(df)} rows, using first row only\")\n\n        row = df.iloc[0]\n\n        # Extract data\n        result = {\n            \"substrate_name\": substrate_name or row.get(\"substrate_name\", \"Unknown\"),\n            \"substrate_type\": substrate_type or row.get(\"substrate_type\", \"unknown\"),\n            \"sample_date\": sample_date or row.get(\"sample_date\", datetime.now()),\n        }\n\n        # Add all available parameters\n        for param in [\n            \"TS\",\n            \"VS\",\n            \"oTS\",\n            \"foTS\",\n            \"RP\",\n            \"RL\",\n            \"RF\",\n            \"RA\",\n            \"NfE\",\n            \"NDF\",\n            \"ADF\",\n            \"ADL\",\n            \"pH\",\n            \"NH4_N\",\n            \"TAC\",\n            \"COD\",\n            \"COD_S\",\n            \"BMP\",\n            \"C_content\",\n            \"N_content\",\n            \"C_to_N\",\n            \"TKN\",\n        ]:\n            if param in df.columns:\n                value = row[param]\n                # Handle both scalar values and Series\n                if isinstance(value, pd.Series):\n                    value = value.iloc[0] if len(value) &gt; 0 else None\n                if pd.notna(value):\n                    result[param] = float(value)\n\n        # Validate if requested\n        if validate:\n            result = self._validate_substrate_data(result)\n\n        # TODO: diese Substratparameter m\u00fcssen in die substrate_....xml geschrieben werden. evtl. gibt es in einer\n        #  c# DLL auch bereits eine Methode die man aufrufen kann. glaube aber eher nicht\n\n        return result\n\n    def load_multiple_substrate_samples(\n        self,\n        filepath: str,\n        sep: str = \",\",\n        encoding: str = \"utf-8\",\n        date_column: str = \"sample_date\",\n        name_column: str = \"substrate_name\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load multiple substrate samples from CSV.\n\n        Expected format: Each row is one sample with columns for all parameters.\n\n        Args:\n            filepath: Path to CSV file\n            sep: Column separator\n            encoding: File encoding\n            date_column: Name of date column\n            name_column: Name of substrate name column\n\n        Returns:\n            DataFrame with substrate data\n\n        Example:\n            &gt;&gt;&gt; handler = CSVHandler()\n            &gt;&gt;&gt; samples = handler.load_multiple_substrate_samples(\n            ...     \"substrate_database.csv\"\n            ... )\n            &gt;&gt;&gt; print(samples.head())\n        \"\"\"\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n        # Map column names\n        df = self._map_column_names(df)\n\n        # Parse date column\n        if date_column in df.columns:\n            df[date_column] = pd.to_datetime(df[date_column])\n\n        return df\n\n    def export_substrate_data(\n        self, data: Union[Dict[str, Any], pd.DataFrame], filepath: str, sep: str = \",\", encoding: str = \"utf-8\"\n    ) -&gt; None:\n        \"\"\"\n        Export substrate data to CSV.\n\n        Args:\n            data: Dict or DataFrame with substrate data\n            filepath: Output file path\n            sep: Column separator\n            encoding: File encoding\n\n        Example:\n            &gt;&gt;&gt; handler.export_substrate_data(substrate_data, \"export.csv\")\n        \"\"\"\n        if isinstance(data, dict):\n            df = pd.DataFrame([data])\n        else:\n            df = data\n\n        df.to_csv(filepath, sep=sep, encoding=encoding, index=False)\n        print(f\"\u2713 Exported substrate data to {filepath}\")\n\n    # ========================================================================\n    # Measurement Data\n    # ========================================================================\n\n    # TODO: so eine Methode gibt es bereits in calibration.py. Dort evtl. l\u00f6schen. Vorher beide vergleichen.\n    def load_measurement_data(\n        self,\n        filepath: str,\n        timestamp_column: str = \"timestamp\",\n        sep: str = \",\",\n        encoding: str = \"utf-8\",\n        parse_dates: bool = True,\n        resample: Optional[str] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load time series measurement data from CSV.\n\n        Expected columns:\n        - timestamp (or Zeit, Zeitstempel)\n        - Q_sub_* (substrate feeds)\n        - pH, VFA, TAC, FOS_TAC\n        - T_digester\n        - Q_gas, Q_ch4, Q_co2, CH4_content, P_gas\n        - P_el, P_th\n\n        Args:\n            filepath: Path to CSV file\n            timestamp_column: Name of timestamp column\n            sep: Column separator\n            encoding: File encoding\n            parse_dates: Parse timestamp column\n            resample: Resample frequency (e.g., \"1h\", \"1d\")\n\n        Returns:\n            DataFrame with measurements\n\n        Example:\n            &gt;&gt;&gt; handler = CSVHandler()\n            &gt;&gt;&gt; data = handler.load_measurement_data(\n            ...     \"plant_data.csv\",\n            ...     resample=\"1h\"\n            ... )\n        \"\"\"\n        # Auto-detect separator\n        if sep == \"auto\":\n            sep = self._detect_separator(filepath)\n\n        # Read CSV\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n        # Map column names\n        df = self._map_column_names(df)\n\n        # Parse timestamp\n        if timestamp_column in df.columns:\n            if parse_dates:\n                df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n            df = df.set_index(timestamp_column).sort_index()\n\n        # Resample if requested\n        if resample is not None:\n            df = df.resample(resample).mean()\n\n        return df\n\n    def export_measurement_data(\n        self, data: pd.DataFrame, filepath: str, sep: str = \",\", encoding: str = \"utf-8\", include_index: bool = True\n    ) -&gt; None:\n        \"\"\"\n        Export measurement data to CSV.\n\n        Args:\n            data: DataFrame with measurements\n            filepath: Output file path\n            sep: Column separator\n            encoding: File encoding\n            include_index: Include index (timestamp) in output\n\n        Example:\n            &gt;&gt;&gt; handler.export_measurement_data(measurements, \"export.csv\")\n        \"\"\"\n        data.to_csv(filepath, sep=sep, encoding=encoding, index=include_index)\n        print(f\"\u2713 Exported measurement data to {filepath} ({len(data)} rows)\")\n\n    # ========================================================================\n    # Simulation Results\n    # ========================================================================\n\n    def export_simulation_results(\n        self,\n        results: List[Dict[str, Any]],\n        filepath: str,\n        sep: str = \",\",\n        encoding: str = \"utf-8\",\n        flatten_components: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Export simulation results to CSV.\n\n        Args:\n            results: List of result dicts from plant.simulate()\n            filepath: Output file path\n            sep: Column separator\n            encoding: File encoding\n            flatten_components: Flatten component results into columns\n\n        Example:\n            &gt;&gt;&gt; results = plant.simulate(duration=30, dt=1/24)\n            &gt;&gt;&gt; handler.export_simulation_results(results, \"simulation.csv\")\n        \"\"\"\n        if not results:\n            warnings.warn(\"No results to export\")\n            return\n\n        # Convert to DataFrame\n        if flatten_components:\n            # Flatten structure: time, component1_metric1, component1_metric2, ...\n            rows = []\n            for result in results:\n                row = {\"time\": result[\"time\"]}\n\n                for comp_id, comp_data in result[\"components\"].items():\n                    for metric, value in comp_data.items():\n                        # Skip nested dicts (like gas_storage)\n                        if isinstance(value, dict):\n                            continue\n                        col_name = f\"{comp_id}_{metric}\"\n                        row[col_name] = value\n\n                rows.append(row)\n\n            df = pd.DataFrame(rows)\n        else:\n            # Simple format: just time and first component's data\n            first_comp_id = list(results[0][\"components\"].keys())[0]\n            rows = []\n            for result in results:\n                row = {\"time\": result[\"time\"]}\n                row.update(result[\"components\"][first_comp_id])\n                # Remove nested dicts\n                row = {k: v for k, v in row.items() if not isinstance(v, dict)}\n                rows.append(row)\n\n            df = pd.DataFrame(rows)\n\n        # Export\n        df.to_csv(filepath, sep=sep, encoding=encoding, index=False)\n        print(f\"\u2713 Exported simulation results to {filepath} ({len(df)} time points)\")\n\n    def load_simulation_results(self, filepath: str, sep: str = \",\", encoding: str = \"utf-8\") -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Load simulation results from CSV.\n\n        Args:\n            filepath: Path to CSV file\n            sep: Column separator\n            encoding: File encoding\n\n        Returns:\n            List of result dicts\n\n        Example:\n            &gt;&gt;&gt; results = handler.load_simulation_results(\"simulation.csv\")\n        \"\"\"\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n        # Convert back to results format\n        results = []\n        for _, row in df.iterrows():\n            result = {\"time\": row[\"time\"], \"components\": {}}\n\n            # Group columns by component\n            for col in df.columns:\n                if col == \"time\":\n                    continue\n\n                if \"_\" in col:\n                    comp_id, metric = col.split(\"_\", 1)\n                    if comp_id not in result[\"components\"]:\n                        result[\"components\"][comp_id] = {}\n                    result[\"components\"][comp_id][metric] = row[col]\n\n            results.append(result)\n\n        return results\n\n    # ========================================================================\n    # Parameter Tables\n    # ========================================================================\n\n    def load_parameter_table(\n        self, filepath: str, sep: str = \",\", encoding: str = \"utf-8\", index_col: Optional[str] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Load parameter table from CSV.\n\n        Expected format:\n        - Rows: Parameters\n        - Columns: Different scenarios/substrates\n\n        Args:\n            filepath: Path to CSV file\n            sep: Column separator\n            encoding: File encoding\n            index_col: Column to use as index (usually parameter name)\n\n        Returns:\n            DataFrame with parameters\n\n        Example:\n            &gt;&gt;&gt; params = handler.load_parameter_table(\"parameters.csv\")\n        \"\"\"\n        df = pd.read_csv(filepath, sep=sep, encoding=encoding, index_col=index_col)\n        return df\n\n    def export_parameter_table(self, data: pd.DataFrame, filepath: str, sep: str = \",\", encoding: str = \"utf-8\") -&gt; None:\n        \"\"\"\n        Export parameter table to CSV.\n\n        Args:\n            data: DataFrame with parameters\n            filepath: Output file path\n            sep: Column separator\n            encoding: File encoding\n\n        Example:\n            &gt;&gt;&gt; handler.export_parameter_table(params_df, \"parameters.csv\")\n        \"\"\"\n        data.to_csv(filepath, sep=sep, encoding=encoding)\n        print(f\"\u2713 Exported parameter table to {filepath}\")\n\n    # ========================================================================\n    # Helper Methods\n    # ========================================================================\n\n    def _detect_separator(self, filepath: str) -&gt; str:\n        \"\"\"\n        Auto-detect CSV separator.\n\n        Args:\n            filepath: Path to CSV file\n\n        Returns:\n            Detected separator\n        \"\"\"\n        with open(filepath, \"r\") as f:\n            first_line = f.readline()\n\n        # Count occurrences of common separators\n        comma_count = first_line.count(\",\")\n        semicolon_count = first_line.count(\";\")\n        tab_count = first_line.count(\"\\t\")\n\n        if semicolon_count &gt; comma_count and semicolon_count &gt; tab_count:\n            return \";\"\n        elif tab_count &gt; comma_count:\n            return \"\\t\"\n        else:\n            return \",\"\n\n    def _map_column_names(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Map German column names to English standard names.\n\n        Args:\n            df: DataFrame with original column names\n\n        Returns:\n            DataFrame with mapped column names\n        \"\"\"\n        # Create mapping dict for this DataFrame\n        mapping = {}\n        for col in df.columns:\n            # Check if column is in mapping dict\n            if col in self.COLUMN_MAPPINGS:\n                mapping[col] = self.COLUMN_MAPPINGS[col]\n            # Also check case-insensitive\n            elif col.lower().strip() in {k.lower(): v for k, v in self.COLUMN_MAPPINGS.items()}:\n                original_key = [k for k in self.COLUMN_MAPPINGS if k.lower() == col.lower().strip()][0]\n                mapping[col] = self.COLUMN_MAPPINGS[original_key]\n\n        if mapping:\n            df = df.rename(columns=mapping)\n\n        return df\n\n    def _parse_vertical_format(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Parse vertical CSV format (parameter, value, unit columns).\n\n        Args:\n            df: DataFrame in vertical format\n\n        Returns:\n            DataFrame in horizontal format\n        \"\"\"\n        # Expected columns: Parameter/Messgr\u00f6\u00dfe, Wert/Value, Einheit/Unit\n        param_col = None\n        value_col = None\n        unit_col = None\n\n        for col in df.columns:\n            col_lower = col.lower().strip()\n            if \"parameter\" in col_lower or \"messgr\u00f6\u00dfe\" in col_lower:\n                param_col = col\n            elif \"wert\" in col_lower or \"value\" in col_lower:\n                value_col = col\n            elif \"einheit\" in col_lower or \"unit\" in col_lower:\n                unit_col = col\n                print(\"_parse_vertical_format: \", unit_col)\n\n        if not (param_col and value_col):\n            raise ValueError(\"Cannot parse vertical format: missing Parameter or Value column\")\n\n        # Create horizontal DataFrame\n        data = {}\n        for _, row in df.iterrows():\n            param = str(row[param_col]).strip()\n            value = row[value_col]\n\n            # Map parameter name\n            if param in self.COLUMN_MAPPINGS:\n                param = self.COLUMN_MAPPINGS[param]\n\n            data[param] = value\n\n        return pd.DataFrame([data])\n\n    def _validate_substrate_data(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Validate substrate data ranges.\n\n        Args:\n            data: Substrate data dict\n\n        Returns:\n            Validated data dict (with warnings for out-of-range values)\n        \"\"\"\n        # Expected ranges\n        ranges = {\n            \"TS\": (5.0, 95.0),  # % FM\n            \"VS\": (50.0, 100.0),  # % TS\n            \"RP\": (0.0, 40.0),  # % TS\n            \"RL\": (0.0, 20.0),  # % TS\n            \"RF\": (0.0, 50.0),  # % TS\n            \"NDF\": (10.0, 90.0),  # % TS\n            \"ADF\": (5.0, 70.0),  # % TS\n            \"ADL\": (0.0, 30.0),  # % TS\n            \"pH\": (3.0, 9.0),\n            \"NH4_N\": (0.0, 10.0),  # g/L\n            \"TAC\": (0.0, 500.0),  # mmol/L\n            \"BMP\": (50.0, 800.0),  # L CH4/kg oTS\n            \"C_to_N\": (5.0, 100.0),\n        }\n\n        for param, (min_val, max_val) in ranges.items():\n            if param in data:\n                value = data[param]\n                if value &lt; min_val or value &gt; max_val:\n                    warnings.warn(f\"Parameter '{param}' = {value} is outside expected range [{min_val}, {max_val}]\")\n\n        return data\n\n    def create_template_substrate_csv(self, filepath: str, format_type: str = \"horizontal\") -&gt; None:\n        \"\"\"\n        Create template CSV file for substrate data entry.\n\n        Args:\n            filepath: Output file path\n            format_type: \"horizontal\" or \"vertical\"\n\n        Example:\n            &gt;&gt;&gt; handler.create_template_substrate_csv(\"template.csv\")\n        \"\"\"\n        if format_type == \"horizontal\":\n            # One row per sample\n            template = pd.DataFrame(\n                columns=[\n                    \"substrate_name\",\n                    \"substrate_type\",\n                    \"sample_date\",\n                    \"TS\",\n                    \"VS\",\n                    \"oTS\",\n                    \"foTS\",\n                    \"RP\",\n                    \"RL\",\n                    \"RF\",\n                    \"NDF\",\n                    \"ADF\",\n                    \"ADL\",\n                    \"pH\",\n                    \"NH4_N\",\n                    \"TAC\",\n                    \"COD_S\",\n                    \"BMP\",\n                    \"C_content\",\n                    \"N_content\",\n                    \"C_to_N\",\n                ]\n            )\n\n            # Add example row\n            template.loc[0] = [\n                \"Maize silage\",\n                \"maize\",\n                \"2024-01-15\",\n                32.5,\n                96.2,\n                31.3,\n                28.5,\n                8.5,\n                3.2,\n                21.5,\n                42.1,\n                22.3,\n                2.1,\n                3.9,\n                0.5,\n                11.0,\n                18.5,\n                345.0,\n                45.2,\n                1.8,\n                25.1,\n            ]\n\n        else:  # vertical\n            template = pd.DataFrame(\n                {\n                    \"Parameter\": [\n                        \"Substrate name\",\n                        \"Substrate type\",\n                        \"TS\",\n                        \"VS\",\n                        \"RP\",\n                        \"RL\",\n                        \"NDF\",\n                        \"ADF\",\n                        \"ADL\",\n                        \"pH\",\n                        \"NH4-N\",\n                        \"TAC\",\n                        \"COD_S\",\n                        \"BMP\",\n                    ],\n                    \"Value\": [\"Maize silage\", \"maize\", 32.5, 96.2, 8.5, 3.2, 42.1, 22.3, 2.1, 3.9, 0.5, 11.0, 18.5, 345.0],\n                    \"Unit\": [\n                        \"\",\n                        \"\",\n                        \"% FM\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"% TS\",\n                        \"-\",\n                        \"g/L\",\n                        \"mmol/L\",\n                        \"g/L\",\n                        \"L CH4/kg oTS\",\n                    ],\n                }\n            )\n\n        template.to_csv(filepath, index=False)\n        print(f\"\u2713 Created template CSV at {filepath}\")\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler-functions","title":"Functions","text":""},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.__init__","title":"<code>__init__(decimal_separator='.', thousands_separator=',')</code>","text":"<p>Initialize CSV handler.</p> <p>Parameters:</p> Name Type Description Default <code>decimal_separator</code> <code>str</code> <p>Decimal separator (\".\" or \",\")</p> <code>'.'</code> <code>thousands_separator</code> <code>str</code> <p>Thousands separator (\",\" or \".\" or \"\")</p> <code>','</code> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def __init__(self, decimal_separator: str = \".\", thousands_separator: str = \",\"):\n    \"\"\"\n    Initialize CSV handler.\n\n    Args:\n        decimal_separator: Decimal separator (\".\" or \",\")\n        thousands_separator: Thousands separator (\",\" or \".\" or \"\")\n    \"\"\"\n    self.decimal_separator = decimal_separator\n    self.thousands_separator = thousands_separator\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.create_template_substrate_csv","title":"<code>create_template_substrate_csv(filepath, format_type='horizontal')</code>","text":"<p>Create template CSV file for substrate data entry.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>format_type</code> <code>str</code> <p>\"horizontal\" or \"vertical\"</p> <code>'horizontal'</code> Example <p>handler.create_template_substrate_csv(\"template.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def create_template_substrate_csv(self, filepath: str, format_type: str = \"horizontal\") -&gt; None:\n    \"\"\"\n    Create template CSV file for substrate data entry.\n\n    Args:\n        filepath: Output file path\n        format_type: \"horizontal\" or \"vertical\"\n\n    Example:\n        &gt;&gt;&gt; handler.create_template_substrate_csv(\"template.csv\")\n    \"\"\"\n    if format_type == \"horizontal\":\n        # One row per sample\n        template = pd.DataFrame(\n            columns=[\n                \"substrate_name\",\n                \"substrate_type\",\n                \"sample_date\",\n                \"TS\",\n                \"VS\",\n                \"oTS\",\n                \"foTS\",\n                \"RP\",\n                \"RL\",\n                \"RF\",\n                \"NDF\",\n                \"ADF\",\n                \"ADL\",\n                \"pH\",\n                \"NH4_N\",\n                \"TAC\",\n                \"COD_S\",\n                \"BMP\",\n                \"C_content\",\n                \"N_content\",\n                \"C_to_N\",\n            ]\n        )\n\n        # Add example row\n        template.loc[0] = [\n            \"Maize silage\",\n            \"maize\",\n            \"2024-01-15\",\n            32.5,\n            96.2,\n            31.3,\n            28.5,\n            8.5,\n            3.2,\n            21.5,\n            42.1,\n            22.3,\n            2.1,\n            3.9,\n            0.5,\n            11.0,\n            18.5,\n            345.0,\n            45.2,\n            1.8,\n            25.1,\n        ]\n\n    else:  # vertical\n        template = pd.DataFrame(\n            {\n                \"Parameter\": [\n                    \"Substrate name\",\n                    \"Substrate type\",\n                    \"TS\",\n                    \"VS\",\n                    \"RP\",\n                    \"RL\",\n                    \"NDF\",\n                    \"ADF\",\n                    \"ADL\",\n                    \"pH\",\n                    \"NH4-N\",\n                    \"TAC\",\n                    \"COD_S\",\n                    \"BMP\",\n                ],\n                \"Value\": [\"Maize silage\", \"maize\", 32.5, 96.2, 8.5, 3.2, 42.1, 22.3, 2.1, 3.9, 0.5, 11.0, 18.5, 345.0],\n                \"Unit\": [\n                    \"\",\n                    \"\",\n                    \"% FM\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"% TS\",\n                    \"-\",\n                    \"g/L\",\n                    \"mmol/L\",\n                    \"g/L\",\n                    \"L CH4/kg oTS\",\n                ],\n            }\n        )\n\n    template.to_csv(filepath, index=False)\n    print(f\"\u2713 Created template CSV at {filepath}\")\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.export_measurement_data","title":"<code>export_measurement_data(data, filepath, sep=',', encoding='utf-8', include_index=True)</code>","text":"<p>Export measurement data to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with measurements</p> required <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>include_index</code> <code>bool</code> <p>Include index (timestamp) in output</p> <code>True</code> Example <p>handler.export_measurement_data(measurements, \"export.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def export_measurement_data(\n    self, data: pd.DataFrame, filepath: str, sep: str = \",\", encoding: str = \"utf-8\", include_index: bool = True\n) -&gt; None:\n    \"\"\"\n    Export measurement data to CSV.\n\n    Args:\n        data: DataFrame with measurements\n        filepath: Output file path\n        sep: Column separator\n        encoding: File encoding\n        include_index: Include index (timestamp) in output\n\n    Example:\n        &gt;&gt;&gt; handler.export_measurement_data(measurements, \"export.csv\")\n    \"\"\"\n    data.to_csv(filepath, sep=sep, encoding=encoding, index=include_index)\n    print(f\"\u2713 Exported measurement data to {filepath} ({len(data)} rows)\")\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.export_parameter_table","title":"<code>export_parameter_table(data, filepath, sep=',', encoding='utf-8')</code>","text":"<p>Export parameter table to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame with parameters</p> required <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Example <p>handler.export_parameter_table(params_df, \"parameters.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def export_parameter_table(self, data: pd.DataFrame, filepath: str, sep: str = \",\", encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"\n    Export parameter table to CSV.\n\n    Args:\n        data: DataFrame with parameters\n        filepath: Output file path\n        sep: Column separator\n        encoding: File encoding\n\n    Example:\n        &gt;&gt;&gt; handler.export_parameter_table(params_df, \"parameters.csv\")\n    \"\"\"\n    data.to_csv(filepath, sep=sep, encoding=encoding)\n    print(f\"\u2713 Exported parameter table to {filepath}\")\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.export_simulation_results","title":"<code>export_simulation_results(results, filepath, sep=',', encoding='utf-8', flatten_components=True)</code>","text":"<p>Export simulation results to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of result dicts from plant.simulate()</p> required <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>flatten_components</code> <code>bool</code> <p>Flatten component results into columns</p> <code>True</code> Example <p>results = plant.simulate(duration=30, dt=1/24) handler.export_simulation_results(results, \"simulation.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def export_simulation_results(\n    self,\n    results: List[Dict[str, Any]],\n    filepath: str,\n    sep: str = \",\",\n    encoding: str = \"utf-8\",\n    flatten_components: bool = True,\n) -&gt; None:\n    \"\"\"\n    Export simulation results to CSV.\n\n    Args:\n        results: List of result dicts from plant.simulate()\n        filepath: Output file path\n        sep: Column separator\n        encoding: File encoding\n        flatten_components: Flatten component results into columns\n\n    Example:\n        &gt;&gt;&gt; results = plant.simulate(duration=30, dt=1/24)\n        &gt;&gt;&gt; handler.export_simulation_results(results, \"simulation.csv\")\n    \"\"\"\n    if not results:\n        warnings.warn(\"No results to export\")\n        return\n\n    # Convert to DataFrame\n    if flatten_components:\n        # Flatten structure: time, component1_metric1, component1_metric2, ...\n        rows = []\n        for result in results:\n            row = {\"time\": result[\"time\"]}\n\n            for comp_id, comp_data in result[\"components\"].items():\n                for metric, value in comp_data.items():\n                    # Skip nested dicts (like gas_storage)\n                    if isinstance(value, dict):\n                        continue\n                    col_name = f\"{comp_id}_{metric}\"\n                    row[col_name] = value\n\n            rows.append(row)\n\n        df = pd.DataFrame(rows)\n    else:\n        # Simple format: just time and first component's data\n        first_comp_id = list(results[0][\"components\"].keys())[0]\n        rows = []\n        for result in results:\n            row = {\"time\": result[\"time\"]}\n            row.update(result[\"components\"][first_comp_id])\n            # Remove nested dicts\n            row = {k: v for k, v in row.items() if not isinstance(v, dict)}\n            rows.append(row)\n\n        df = pd.DataFrame(rows)\n\n    # Export\n    df.to_csv(filepath, sep=sep, encoding=encoding, index=False)\n    print(f\"\u2713 Exported simulation results to {filepath} ({len(df)} time points)\")\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.export_substrate_data","title":"<code>export_substrate_data(data, filepath, sep=',', encoding='utf-8')</code>","text":"<p>Export substrate data to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Dict[str, Any], DataFrame]</code> <p>Dict or DataFrame with substrate data</p> required <code>filepath</code> <code>str</code> <p>Output file path</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> Example <p>handler.export_substrate_data(substrate_data, \"export.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def export_substrate_data(\n    self, data: Union[Dict[str, Any], pd.DataFrame], filepath: str, sep: str = \",\", encoding: str = \"utf-8\"\n) -&gt; None:\n    \"\"\"\n    Export substrate data to CSV.\n\n    Args:\n        data: Dict or DataFrame with substrate data\n        filepath: Output file path\n        sep: Column separator\n        encoding: File encoding\n\n    Example:\n        &gt;&gt;&gt; handler.export_substrate_data(substrate_data, \"export.csv\")\n    \"\"\"\n    if isinstance(data, dict):\n        df = pd.DataFrame([data])\n    else:\n        df = data\n\n    df.to_csv(filepath, sep=sep, encoding=encoding, index=False)\n    print(f\"\u2713 Exported substrate data to {filepath}\")\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.load_measurement_data","title":"<code>load_measurement_data(filepath, timestamp_column='timestamp', sep=',', encoding='utf-8', parse_dates=True, resample=None)</code>","text":"<p>Load time series measurement data from CSV.</p> <p>Expected columns: - timestamp (or Zeit, Zeitstempel) - Q_sub_* (substrate feeds) - pH, VFA, TAC, FOS_TAC - T_digester - Q_gas, Q_ch4, Q_co2, CH4_content, P_gas - P_el, P_th</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>timestamp_column</code> <code>str</code> <p>Name of timestamp column</p> <code>'timestamp'</code> <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>parse_dates</code> <code>bool</code> <p>Parse timestamp column</p> <code>True</code> <code>resample</code> <code>Optional[str]</code> <p>Resample frequency (e.g., \"1h\", \"1d\")</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with measurements</p> Example <p>handler = CSVHandler() data = handler.load_measurement_data( ...     \"plant_data.csv\", ...     resample=\"1h\" ... )</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_measurement_data(\n    self,\n    filepath: str,\n    timestamp_column: str = \"timestamp\",\n    sep: str = \",\",\n    encoding: str = \"utf-8\",\n    parse_dates: bool = True,\n    resample: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load time series measurement data from CSV.\n\n    Expected columns:\n    - timestamp (or Zeit, Zeitstempel)\n    - Q_sub_* (substrate feeds)\n    - pH, VFA, TAC, FOS_TAC\n    - T_digester\n    - Q_gas, Q_ch4, Q_co2, CH4_content, P_gas\n    - P_el, P_th\n\n    Args:\n        filepath: Path to CSV file\n        timestamp_column: Name of timestamp column\n        sep: Column separator\n        encoding: File encoding\n        parse_dates: Parse timestamp column\n        resample: Resample frequency (e.g., \"1h\", \"1d\")\n\n    Returns:\n        DataFrame with measurements\n\n    Example:\n        &gt;&gt;&gt; handler = CSVHandler()\n        &gt;&gt;&gt; data = handler.load_measurement_data(\n        ...     \"plant_data.csv\",\n        ...     resample=\"1h\"\n        ... )\n    \"\"\"\n    # Auto-detect separator\n    if sep == \"auto\":\n        sep = self._detect_separator(filepath)\n\n    # Read CSV\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n    # Map column names\n    df = self._map_column_names(df)\n\n    # Parse timestamp\n    if timestamp_column in df.columns:\n        if parse_dates:\n            df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n        df = df.set_index(timestamp_column).sort_index()\n\n    # Resample if requested\n    if resample is not None:\n        df = df.resample(resample).mean()\n\n    return df\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.load_multiple_substrate_samples","title":"<code>load_multiple_substrate_samples(filepath, sep=',', encoding='utf-8', date_column='sample_date', name_column='substrate_name')</code>","text":"<p>Load multiple substrate samples from CSV.</p> <p>Expected format: Each row is one sample with columns for all parameters.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>date_column</code> <code>str</code> <p>Name of date column</p> <code>'sample_date'</code> <code>name_column</code> <code>str</code> <p>Name of substrate name column</p> <code>'substrate_name'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with substrate data</p> Example <p>handler = CSVHandler() samples = handler.load_multiple_substrate_samples( ...     \"substrate_database.csv\" ... ) print(samples.head())</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_multiple_substrate_samples(\n    self,\n    filepath: str,\n    sep: str = \",\",\n    encoding: str = \"utf-8\",\n    date_column: str = \"sample_date\",\n    name_column: str = \"substrate_name\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load multiple substrate samples from CSV.\n\n    Expected format: Each row is one sample with columns for all parameters.\n\n    Args:\n        filepath: Path to CSV file\n        sep: Column separator\n        encoding: File encoding\n        date_column: Name of date column\n        name_column: Name of substrate name column\n\n    Returns:\n        DataFrame with substrate data\n\n    Example:\n        &gt;&gt;&gt; handler = CSVHandler()\n        &gt;&gt;&gt; samples = handler.load_multiple_substrate_samples(\n        ...     \"substrate_database.csv\"\n        ... )\n        &gt;&gt;&gt; print(samples.head())\n    \"\"\"\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n    # Map column names\n    df = self._map_column_names(df)\n\n    # Parse date column\n    if date_column in df.columns:\n        df[date_column] = pd.to_datetime(df[date_column])\n\n    return df\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.load_parameter_table","title":"<code>load_parameter_table(filepath, sep=',', encoding='utf-8', index_col=None)</code>","text":"<p>Load parameter table from CSV.</p> <p>Expected format: - Rows: Parameters - Columns: Different scenarios/substrates</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>index_col</code> <code>Optional[str]</code> <p>Column to use as index (usually parameter name)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with parameters</p> Example <p>params = handler.load_parameter_table(\"parameters.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_parameter_table(\n    self, filepath: str, sep: str = \",\", encoding: str = \"utf-8\", index_col: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load parameter table from CSV.\n\n    Expected format:\n    - Rows: Parameters\n    - Columns: Different scenarios/substrates\n\n    Args:\n        filepath: Path to CSV file\n        sep: Column separator\n        encoding: File encoding\n        index_col: Column to use as index (usually parameter name)\n\n    Returns:\n        DataFrame with parameters\n\n    Example:\n        &gt;&gt;&gt; params = handler.load_parameter_table(\"parameters.csv\")\n    \"\"\"\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding, index_col=index_col)\n    return df\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.load_simulation_results","title":"<code>load_simulation_results(filepath, sep=',', encoding='utf-8')</code>","text":"<p>Load simulation results from CSV.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of result dicts</p> Example <p>results = handler.load_simulation_results(\"simulation.csv\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_simulation_results(self, filepath: str, sep: str = \",\", encoding: str = \"utf-8\") -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Load simulation results from CSV.\n\n    Args:\n        filepath: Path to CSV file\n        sep: Column separator\n        encoding: File encoding\n\n    Returns:\n        List of result dicts\n\n    Example:\n        &gt;&gt;&gt; results = handler.load_simulation_results(\"simulation.csv\")\n    \"\"\"\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n    # Convert back to results format\n    results = []\n    for _, row in df.iterrows():\n        result = {\"time\": row[\"time\"], \"components\": {}}\n\n        # Group columns by component\n        for col in df.columns:\n            if col == \"time\":\n                continue\n\n            if \"_\" in col:\n                comp_id, metric = col.split(\"_\", 1)\n                if comp_id not in result[\"components\"]:\n                    result[\"components\"][comp_id] = {}\n                result[\"components\"][comp_id][metric] = row[col]\n\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.CSVHandler.load_substrate_lab_data","title":"<code>load_substrate_lab_data(filepath, substrate_name=None, substrate_type=None, sample_date=None, sep=',', encoding='utf-8', validate=True)</code>","text":"<p>Load substrate characterization data from laboratory CSV.</p> <p>Expected columns (German or English): - Trockensubstanzgehalt (TS) [% FM] - Organische Trockensubstanz (VS) [% TS] - Fermentierbare organische Trockensubstanz (foTS) [% TS] - Rohprotein (RP) [% TS] - Rohfett (RL) [% TS] - Rohfaser (RF) [% TS] - NDF, ADF, ADL [% TS] - pH-Wert (pH) - Ammoniumstickstoff (NH4-N) [g/L or mg/L] - Alkalinit\u00e4t (TAC) [mmol/L] - Biochemisches Methanpotential (BMP) [L CH4/kg oTS] - CSB des Filtrats (COD_S) [g/L]</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to CSV file</p> required <code>substrate_name</code> <code>Optional[str]</code> <p>Substrate name (if not in file)</p> <code>None</code> <code>substrate_type</code> <code>Optional[str]</code> <p>Substrate type (maize, manure, grass, etc.)</p> <code>None</code> <code>sample_date</code> <code>Optional[Union[str, datetime]]</code> <p>Sample date (if not in file)</p> <code>None</code> <code>sep</code> <code>str</code> <p>Column separator</p> <code>','</code> <code>encoding</code> <code>str</code> <p>File encoding</p> <code>'utf-8'</code> <code>validate</code> <code>bool</code> <p>Validate data ranges</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict with substrate data</p> Example <p>handler = CSVHandler() data = handler.load_substrate_lab_data( ...     \"maize_analysis.csv\", ...     substrate_name=\"Maize silage batch 23\", ...     substrate_type=\"maize\", ...     sample_date=\"2024-01-15\" ... ) print(f\"TS: {data['TS']:.1f}% FM\")</p> Source code in <code>pyadm1ode_calibration/io/loaders/csv_handler.py</code> <pre><code>def load_substrate_lab_data(\n    self,\n    filepath: str,\n    substrate_name: Optional[str] = None,\n    substrate_type: Optional[str] = None,\n    sample_date: Optional[Union[str, datetime]] = None,\n    sep: str = \",\",\n    encoding: str = \"utf-8\",\n    validate: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load substrate characterization data from laboratory CSV.\n\n    Expected columns (German or English):\n    - Trockensubstanzgehalt (TS) [% FM]\n    - Organische Trockensubstanz (VS) [% TS]\n    - Fermentierbare organische Trockensubstanz (foTS) [% TS]\n    - Rohprotein (RP) [% TS]\n    - Rohfett (RL) [% TS]\n    - Rohfaser (RF) [% TS]\n    - NDF, ADF, ADL [% TS]\n    - pH-Wert (pH)\n    - Ammoniumstickstoff (NH4-N) [g/L or mg/L]\n    - Alkalinit\u00e4t (TAC) [mmol/L]\n    - Biochemisches Methanpotential (BMP) [L CH4/kg oTS]\n    - CSB des Filtrats (COD_S) [g/L]\n\n    Args:\n        filepath: Path to CSV file\n        substrate_name: Substrate name (if not in file)\n        substrate_type: Substrate type (maize, manure, grass, etc.)\n        sample_date: Sample date (if not in file)\n        sep: Column separator\n        encoding: File encoding\n        validate: Validate data ranges\n\n    Returns:\n        Dict with substrate data\n\n    Example:\n        &gt;&gt;&gt; handler = CSVHandler()\n        &gt;&gt;&gt; data = handler.load_substrate_lab_data(\n        ...     \"maize_analysis.csv\",\n        ...     substrate_name=\"Maize silage batch 23\",\n        ...     substrate_type=\"maize\",\n        ...     sample_date=\"2024-01-15\"\n        ... )\n        &gt;&gt;&gt; print(f\"TS: {data['TS']:.1f}% FM\")\n    \"\"\"\n    # Auto-detect separator if needed\n    if sep == \"auto\":\n        sep = self._detect_separator(filepath)\n\n    # Read CSV\n    df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n\n    # Try to detect if file is in \"vertical\" format (parameter, value, unit)\n    if len(df.columns) &lt;= 3 and \"Parameter\" in df.columns or \"Messgr\u00f6\u00dfe\" in df.columns:\n        df = self._parse_vertical_format(df)\n\n    # Map column names\n    df = self._map_column_names(df)\n\n    # If multiple rows, take the first one (or could aggregate)\n    if len(df) &gt; 1:\n        warnings.warn(f\"CSV contains {len(df)} rows, using first row only\")\n\n    row = df.iloc[0]\n\n    # Extract data\n    result = {\n        \"substrate_name\": substrate_name or row.get(\"substrate_name\", \"Unknown\"),\n        \"substrate_type\": substrate_type or row.get(\"substrate_type\", \"unknown\"),\n        \"sample_date\": sample_date or row.get(\"sample_date\", datetime.now()),\n    }\n\n    # Add all available parameters\n    for param in [\n        \"TS\",\n        \"VS\",\n        \"oTS\",\n        \"foTS\",\n        \"RP\",\n        \"RL\",\n        \"RF\",\n        \"RA\",\n        \"NfE\",\n        \"NDF\",\n        \"ADF\",\n        \"ADL\",\n        \"pH\",\n        \"NH4_N\",\n        \"TAC\",\n        \"COD\",\n        \"COD_S\",\n        \"BMP\",\n        \"C_content\",\n        \"N_content\",\n        \"C_to_N\",\n        \"TKN\",\n    ]:\n        if param in df.columns:\n            value = row[param]\n            # Handle both scalar values and Series\n            if isinstance(value, pd.Series):\n                value = value.iloc[0] if len(value) &gt; 0 else None\n            if pd.notna(value):\n                result[param] = float(value)\n\n    # Validate if requested\n    if validate:\n        result = self._validate_substrate_data(result)\n\n    # TODO: diese Substratparameter m\u00fcssen in die substrate_....xml geschrieben werden. evtl. gibt es in einer\n    #  c# DLL auch bereits eine Methode die man aufrufen kann. glaube aber eher nicht\n\n    return result\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.Database","title":"<code>pyadm1ode_calibration.io.Database</code>","text":"<p>PostgreSQL database interface for PyADM1.</p> Source code in <code>pyadm1ode_calibration/io/persistence/database.py</code> <pre><code>class Database:\n    \"\"\"\n    PostgreSQL database interface for PyADM1.\n    \"\"\"\n\n    def __init__(self, connection_string: Optional[str] = None, config: Optional[DatabaseConfig] = None):\n        self.connection_manager = ConnectionManager(connection_string, config)\n        self.engine = self.connection_manager.engine\n        self.SessionLocal = self.connection_manager.SessionLocal\n        self.connection_string = self.connection_manager.connection_string\n\n    @classmethod\n    def from_env(cls, prefix: str = \"DB\") -&gt; \"Database\":\n        \"\"\"Create database from environment variables.\"\"\"\n        import os\n        from urllib.parse import quote_plus\n\n        host = os.getenv(f\"{prefix}_HOST\", \"localhost\")\n        port = os.getenv(f\"{prefix}_PORT\", \"5432\")\n        database = os.getenv(f\"{prefix}_NAME\")\n        username = os.getenv(f\"{prefix}_USER\")\n        password = os.getenv(f\"{prefix}_PASSWORD\")\n\n        if not all([database, username, password]):\n            raise ValueError(\"Missing required environment variables\")\n\n        conn_str = f\"postgresql://{username}:{quote_plus(password)}@{host}:{port}/{database}\"\n        return cls(connection_string=conn_str)\n\n    @contextmanager\n    def get_session(self) -&gt; Session:\n        session = self.SessionLocal()\n        try:\n            yield session\n            session.commit()\n        except Exception:\n            session.rollback()\n            raise\n        finally:\n            session.close()\n\n    def create_all_tables(self) -&gt; None:\n        Base.metadata.create_all(bind=self.engine)\n\n    def drop_all_tables(self) -&gt; None:\n        Base.metadata.drop_all(bind=self.engine)\n\n    def create_plant(\n        self,\n        plant_id: str,\n        name: str,\n        location: Optional[str] = None,\n        operator: Optional[str] = None,\n        V_liq: Optional[float] = None,\n        V_gas: Optional[float] = None,\n        T_ad: Optional[float] = None,\n        P_el_nom: Optional[float] = None,\n        configuration: Optional[Dict] = None,\n    ) -&gt; Plant:\n        session = self.SessionLocal()\n        try:\n            plant = Plant(\n                id=plant_id,\n                name=name,\n                location=location,\n                operator=operator,\n                V_liq=V_liq,\n                V_gas=V_gas,\n                T_ad=T_ad,\n                P_el_nom=P_el_nom,\n                configuration=configuration,\n            )\n            session.add(plant)\n            session.commit()\n            session.refresh(plant)\n            session.expunge(plant)\n            return plant\n        except IntegrityError:\n            session.rollback()\n            raise ValueError(f\"Plant with ID '{plant_id}' already exists\")\n        finally:\n            session.close()\n\n    def get_plant(self, plant_id: str) -&gt; Plant:\n        session = self.SessionLocal()\n        try:\n            plant = session.query(Plant).filter(Plant.id == plant_id).first()\n            if plant is None:\n                raise ValueError(f\"Plant '{plant_id}' not found\")\n            session.expunge(plant)\n            return plant\n        except SQLAlchemyError as e:\n            raise DatabaseError(f\"Failed to retrieve plant '{plant_id}': {e}\")\n        finally:\n            session.close()\n\n    def list_plants(self) -&gt; List[Dict[str, Any]]:\n        with self.get_session() as session:\n            plants = session.query(Plant).all()\n            return [\n                {\n                    \"id\": p.id,\n                    \"name\": p.name,\n                    \"location\": p.location,\n                    \"V_liq\": p.V_liq,\n                    \"V_gas\": p.V_gas,\n                    \"T_ad\": p.T_ad,\n                    \"created_at\": p.created_at,\n                }\n                for p in plants\n            ]\n\n    def store_measurements(self, plant_id: str, data: pd.DataFrame, source: str = \"SCADA\", validate: bool = True) -&gt; int:\n        self.get_plant(plant_id)\n        if \"timestamp\" not in data.columns:\n            raise ValueError(\"DataFrame must have 'timestamp' column\")\n        if not pd.api.types.is_datetime64_any_dtype(data[\"timestamp\"]):\n            data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n        if validate:\n            from ..validation.validators import DataValidator\n\n            validation = DataValidator.validate(data)\n            if not validation.is_valid:\n                pass\n        records = []\n        for _, row in data.iterrows():\n            record = {\"plant_id\": plant_id, \"timestamp\": row[\"timestamp\"], \"source\": source}\n            for col in data.columns:\n                if col != \"timestamp\" and col in Measurement.__table__.columns:\n                    val = row[col]\n                    if pd.notna(val):\n                        record[col] = float(val) if isinstance(val, (int, float, np.number)) else val\n            records.append(record)\n        with self.get_session() as session:\n            try:\n                session.bulk_insert_mappings(Measurement, records)\n                return len(records)\n            except SQLAlchemyError as e:\n                raise DatabaseError(f\"Failed to store measurements: {e}\")\n\n    def load_measurements(self, plant_id: str, start_time=None, end_time=None, columns=None, source=None) -&gt; pd.DataFrame:\n        if isinstance(start_time, str):\n            start_time = pd.to_datetime(start_time)\n        if isinstance(end_time, str):\n            end_time = pd.to_datetime(end_time)\n        with self.get_session() as session:\n            query = session.query(Measurement).filter(Measurement.plant_id == plant_id)\n            if start_time:\n                query = query.filter(Measurement.timestamp &gt;= start_time)\n            if end_time:\n                query = query.filter(Measurement.timestamp &lt;= end_time)\n            if source:\n                query = query.filter(Measurement.source == source)\n            results = query.order_by(Measurement.timestamp).all()\n            if not results:\n                return pd.DataFrame()\n            data_dict = {\"timestamp\": [r.timestamp for r in results]}\n            if columns is None:\n                columns = [\n                    c.name\n                    for c in Measurement.__table__.columns\n                    if c.name not in [\"id\", \"plant_id\", \"timestamp\", \"source\", \"created_at\"]\n                ]\n            for col in columns:\n                data_dict[col] = [getattr(r, col) for r in results]\n            return pd.DataFrame(data_dict).set_index(\"timestamp\")\n\n    def store_simulation(\n        self,\n        simulation_id: str,\n        plant_id: str,\n        results: List[Dict[str, Any]],\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        duration: Optional[float] = None,\n        parameters: Optional[Dict] = None,\n        scenario: str = \"baseline\",\n    ) -&gt; Simulation:\n        self.get_plant(plant_id)\n        metrics = self._calculate_simulation_metrics(results)\n        with self.get_session() as session:\n            sim = Simulation(\n                id=simulation_id,\n                plant_id=plant_id,\n                name=name,\n                description=description,\n                duration=duration or (results[-1][\"time\"] if results else 0),\n                scenario=scenario,\n                parameters=parameters,\n                avg_Q_gas=metrics.get(\"avg_Q_gas\"),\n                avg_Q_ch4=metrics.get(\"avg_Q_ch4\"),\n                avg_CH4_content=metrics.get(\"avg_CH4_content\"),\n                avg_pH=metrics.get(\"avg_pH\"),\n                avg_VFA=metrics.get(\"avg_VFA\"),\n                total_energy=metrics.get(\"total_energy\"),\n                status=\"completed\",\n                started_at=datetime.utcnow(),\n                completed_at=datetime.utcnow(),\n            )\n            try:\n                session.add(sim)\n                session.flush()\n                ts_records = []\n                for res in results:\n                    comp_data = next(iter(res[\"components\"].values()))\n                    record = {\n                        \"simulation_id\": simulation_id,\n                        \"time\": res[\"time\"],\n                        \"Q_gas\": comp_data.get(\"Q_gas\"),\n                        \"Q_ch4\": comp_data.get(\"Q_ch4\"),\n                        \"Q_co2\": comp_data.get(\"Q_co2\"),\n                        \"pH\": comp_data.get(\"pH\"),\n                        \"VFA\": comp_data.get(\"VFA\"),\n                        \"TAC\": comp_data.get(\"TAC\"),\n                    }\n                    if record[\"Q_gas\"] and record[\"Q_ch4\"] and record[\"Q_gas\"] &gt; 0:\n                        record[\"CH4_content\"] = (record[\"Q_ch4\"] / record[\"Q_gas\"]) * 100\n                    ts_records.append(record)\n                session.bulk_insert_mappings(SimulationTimeSeries, ts_records)\n                return sim\n            except IntegrityError:\n                raise ValueError(f\"Simulation with ID '{simulation_id}' already exists\")\n\n    def load_simulation(self, simulation_id: str) -&gt; Optional[Dict[str, Any]]:\n        with self.get_session() as session:\n            sim = session.query(Simulation).filter(Simulation.id == simulation_id).first()\n            if not sim:\n                return None\n            ts = (\n                session.query(SimulationTimeSeries)\n                .filter(SimulationTimeSeries.simulation_id == simulation_id)\n                .order_by(SimulationTimeSeries.time)\n                .all()\n            )\n            df = (\n                pd.DataFrame(\n                    {\n                        c.name: [getattr(t, c.name) for t in ts]\n                        for c in SimulationTimeSeries.__table__.columns\n                        if c.name not in [\"id\", \"simulation_id\"]\n                    }\n                )\n                if ts\n                else pd.DataFrame()\n            )\n            return {**{c.name: getattr(sim, c.name) for c in Simulation.__table__.columns}, \"time_series\": df}\n\n    def list_simulations(self, plant_id: Optional[str] = None, scenario: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n        with self.get_session() as session:\n            query = session.query(Simulation)\n            if plant_id:\n                query = query.filter(Simulation.plant_id == plant_id)\n            if scenario:\n                query = query.filter(Simulation.scenario == scenario)\n            simulations = query.order_by(Simulation.created_at.desc()).all()\n            return [\n                {\n                    \"id\": s.id,\n                    \"plant_id\": s.plant_id,\n                    \"name\": s.name,\n                    \"scenario\": s.scenario,\n                    \"duration\": s.duration,\n                    \"avg_Q_ch4\": s.avg_Q_ch4,\n                    \"status\": s.status,\n                    \"created_at\": s.created_at,\n                }\n                for s in simulations\n            ]\n\n    def store_calibration(\n        self,\n        plant_id: str,\n        calibration_type: str,\n        method: str,\n        parameters: Dict[str, float],\n        objective_value: float,\n        objectives: List[str],\n        validation_metrics: Optional[Dict[str, float]] = None,\n        data_start: Optional[datetime] = None,\n        data_end: Optional[datetime] = None,\n        success: bool = True,\n        message: Optional[str] = None,\n    ) -&gt; Calibration:\n        with self.get_session() as session:\n            cal = Calibration(\n                plant_id=plant_id,\n                calibration_type=calibration_type,\n                method=method,\n                parameters=parameters,\n                objective_value=objective_value,\n                objectives=objectives,\n                validation_metrics=validation_metrics,\n                data_start=data_start,\n                data_end=data_end,\n                success=success,\n                message=message,\n            )\n            session.add(cal)\n            return cal\n\n    def load_calibrations(\n        self, plant_id: str, calibration_type: Optional[str] = None, limit: int = 10\n    ) -&gt; List[Dict[str, Any]]:\n        with self.get_session() as session:\n            query = session.query(Calibration).filter(Calibration.plant_id == plant_id)\n            if calibration_type:\n                query = query.filter(Calibration.calibration_type == calibration_type)\n            cals = query.order_by(Calibration.created_at.desc()).limit(limit).all()\n            return [{c.name: getattr(cal, c.name) for c in Calibration.__table__.columns} for cal in cals]\n\n    def get_latest_calibration(self, plant_id: str) -&gt; Optional[Dict[str, Any]]:\n        cals = self.load_calibrations(plant_id, limit=1)\n        return cals[0] if cals else None\n\n    def store_substrate(\n        self,\n        plant_id: str,\n        substrate_name: str,\n        substrate_type: str,\n        sample_date: Union[str, datetime],\n        lab_data: Dict[str, float],\n        sample_id: Optional[str] = None,\n        lab_name: Optional[str] = None,\n        notes: Optional[str] = None,\n    ) -&gt; Substrate:\n        if isinstance(sample_date, str):\n            sample_date = pd.to_datetime(sample_date)\n        session = self.SessionLocal()\n        try:\n            substrate = Substrate(\n                plant_id=plant_id,\n                substrate_name=substrate_name,\n                substrate_type=substrate_type,\n                sample_date=sample_date,\n                sample_id=sample_id,\n                lab_name=lab_name,\n                notes=notes,\n            )\n            for key, value in lab_data.items():\n                if hasattr(substrate, key):\n                    setattr(substrate, key, value)\n            session.add(substrate)\n            session.commit()\n            session.refresh(substrate)\n            session.expunge(substrate)\n            return substrate\n        except Exception:\n            session.rollback()\n            raise\n        finally:\n            session.close()\n\n    def load_substrates(\n        self,\n        plant_id: str,\n        substrate_type: Optional[str] = None,\n        start_date: Optional[Union[str, datetime]] = None,\n        end_date: Optional[Union[str, datetime]] = None,\n    ) -&gt; pd.DataFrame:\n        if isinstance(start_date, str):\n            start_date = pd.to_datetime(start_date)\n        if isinstance(end_date, str):\n            end_date = pd.to_datetime(end_date)\n        with self.get_session() as session:\n            query = session.query(Substrate).filter(Substrate.plant_id == plant_id)\n            if substrate_type:\n                query = query.filter(Substrate.substrate_type == substrate_type)\n            if start_date:\n                query = query.filter(Substrate.sample_date &gt;= start_date)\n            if end_date:\n                query = query.filter(Substrate.sample_date &lt;= end_date)\n            substrates = query.order_by(Substrate.sample_date).all()\n            if not substrates:\n                return pd.DataFrame()\n            cols = [\n                \"sample_date\",\n                \"substrate_name\",\n                \"substrate_type\",\n                \"sample_id\",\n                \"TS\",\n                \"VS\",\n                \"oTS\",\n                \"foTS\",\n                \"RP\",\n                \"RL\",\n                \"RF\",\n                \"NDF\",\n                \"ADF\",\n                \"ADL\",\n                \"pH\",\n                \"NH4_N\",\n                \"TAC\",\n                \"COD_S\",\n                \"BMP\",\n                \"C_to_N\",\n                \"lab_name\",\n            ]\n            return pd.DataFrame([{c: getattr(s, c) for c in cols} for s in substrates])\n\n    def _calculate_simulation_metrics(self, results: List[Dict[str, Any]]) -&gt; Dict[str, float]:\n        if not results:\n            return {}\n        q_gas, q_ch4, ph, vfa, p_el = [], [], [], [], []\n        for res in results:\n            comp_data = next(iter(res[\"components\"].values()))\n            if \"Q_gas\" in comp_data:\n                q_gas.append(comp_data[\"Q_gas\"])\n            if \"Q_ch4\" in comp_data:\n                q_ch4.append(comp_data[\"Q_ch4\"])\n            if \"pH\" in comp_data:\n                ph.append(comp_data[\"pH\"])\n            if \"VFA\" in comp_data:\n                vfa.append(comp_data[\"VFA\"])\n            for comp_res in res[\"components\"].values():\n                if \"P_el\" in comp_res:\n                    p_el.append(comp_res[\"P_el\"])\n        metrics = {}\n        if q_gas:\n            metrics[\"avg_Q_gas\"] = float(np.mean(q_gas))\n        if q_ch4:\n            metrics[\"avg_Q_ch4\"] = float(np.mean(q_ch4))\n            if q_gas:\n                metrics[\"avg_CH4_content\"] = float(np.mean(q_ch4) / np.mean(q_gas) * 100)\n        if ph:\n            metrics[\"avg_pH\"] = float(np.mean(ph))\n        if vfa:\n            metrics[\"avg_VFA\"] = float(np.mean(vfa))\n        if p_el:\n            metrics[\"total_energy\"] = float(np.mean(p_el) * results[-1][\"time\"] * 24)\n        return metrics\n\n    def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None) -&gt; pd.DataFrame:\n        dangerous = [\"DROP\", \"DELETE\", \"TRUNCATE\", \"ALTER\"]\n        if any(kw in query.upper() for kw in dangerous):\n            raise ValueError(\"Dangerous keyword detected in query\")\n        return pd.read_sql(query, self.engine, params=params)\n\n    def get_statistics(self, plant_id: str) -&gt; Dict[str, Any]:\n        with self.get_session() as session:\n            return {\n                \"plant_id\": plant_id,\n                \"n_measurements\": session.query(Measurement).filter(Measurement.plant_id == plant_id).count(),\n                \"n_simulations\": session.query(Simulation).filter(Simulation.plant_id == plant_id).count(),\n                \"n_calibrations\": session.query(Calibration).filter(Calibration.plant_id == plant_id).count(),\n                \"n_substrates\": session.query(Substrate).filter(Substrate.plant_id == plant_id).count(),\n                \"first_measurement\": (\n                    session.query(Measurement.timestamp)\n                    .filter(Measurement.plant_id == plant_id)\n                    .order_by(Measurement.timestamp)\n                    .first()[0]\n                    if session.query(Measurement.timestamp).filter(Measurement.plant_id == plant_id).count() &gt; 0\n                    else None\n                ),\n                \"last_measurement\": (\n                    session.query(Measurement.timestamp)\n                    .filter(Measurement.plant_id == plant_id)\n                    .order_by(Measurement.timestamp.desc())\n                    .first()[0]\n                    if session.query(Measurement.timestamp).filter(Measurement.plant_id == plant_id).count() &gt; 0\n                    else None\n                ),\n            }\n\n    def close(self) -&gt; None:\n        self.engine.dispose()\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.Database-functions","title":"Functions","text":""},{"location":"api/io/#pyadm1ode_calibration.io.Database.from_env","title":"<code>from_env(prefix='DB')</code>  <code>classmethod</code>","text":"<p>Create database from environment variables.</p> Source code in <code>pyadm1ode_calibration/io/persistence/database.py</code> <pre><code>@classmethod\ndef from_env(cls, prefix: str = \"DB\") -&gt; \"Database\":\n    \"\"\"Create database from environment variables.\"\"\"\n    import os\n    from urllib.parse import quote_plus\n\n    host = os.getenv(f\"{prefix}_HOST\", \"localhost\")\n    port = os.getenv(f\"{prefix}_PORT\", \"5432\")\n    database = os.getenv(f\"{prefix}_NAME\")\n    username = os.getenv(f\"{prefix}_USER\")\n    password = os.getenv(f\"{prefix}_PASSWORD\")\n\n    if not all([database, username, password]):\n        raise ValueError(\"Missing required environment variables\")\n\n    conn_str = f\"postgresql://{username}:{quote_plus(password)}@{host}:{port}/{database}\"\n    return cls(connection_string=conn_str)\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.DataValidator","title":"<code>pyadm1ode_calibration.io.DataValidator</code>","text":"<p>Validator for biogas plant measurement data.</p> <p>Checks data quality, identifies issues, and provides statistics.</p> Source code in <code>pyadm1ode_calibration/io/validation/validators.py</code> <pre><code>class DataValidator:\n    \"\"\"\n    Validator for biogas plant measurement data.\n\n    Checks data quality, identifies issues, and provides statistics.\n    \"\"\"\n\n    @staticmethod\n    def validate(\n        data: pd.DataFrame,\n        required_columns: Optional[List[str]] = None,\n        expected_ranges: Optional[Dict[str, Tuple[float, float]]] = None,\n    ) -&gt; ValidationResult:\n        \"\"\"\n        Validate measurement data.\n\n        Args:\n            data: DataFrame to validate\n            required_columns: List of required column names\n            expected_ranges: Dictionary mapping columns to (min, max) tuples\n\n        Returns:\n            ValidationResult object\n        \"\"\"\n        issues = []\n        warnings_list = []\n\n        # Check for required columns\n        if required_columns:\n            missing_cols = set(required_columns) - set(data.columns)\n            if missing_cols:\n                issues.append(f\"Missing required columns: {missing_cols}\")\n\n        # Calculate missing data percentages\n        missing_data = {}\n        for col in data.columns:\n            pct_missing = (data[col].isna().sum() / len(data)) * 100\n            missing_data[col] = pct_missing\n\n            if pct_missing &gt; 30:\n                issues.append(f\"Column '{col}' has {pct_missing:.1f}% missing data\")\n            elif pct_missing &gt; 5:\n                warnings_list.append(f\"Column '{col}' has {pct_missing:.1f}% missing data\")\n\n        # Check for expected ranges\n        if expected_ranges:\n            for col, (min_val, max_val) in expected_ranges.items():\n                if col in data.columns:\n                    values = data[col].dropna()\n                    if len(values) &gt; 0:\n                        actual_min = values.min()\n                        actual_max = values.max()\n\n                        if actual_min &lt; min_val or actual_max &gt; max_val:\n                            warnings_list.append(\n                                f\"Column '{col}' has values outside expected range \"\n                                f\"[{min_val}, {max_val}]: actual [{actual_min:.2f}, {actual_max:.2f}]\"\n                            )\n\n        # Check for duplicate timestamps\n        if \"timestamp\" in data.columns:\n            duplicates = data[\"timestamp\"].duplicated().sum()\n            if duplicates &gt; 0:\n                warnings_list.append(f\"Found {duplicates} duplicate timestamps\")\n\n        # Calculate statistics\n        statistics = {\n            \"n_rows\": len(data),\n            \"n_columns\": len(data.columns),\n            \"total_missing\": data.isna().sum().sum(),\n            \"pct_missing\": (data.isna().sum().sum() / (len(data) * len(data.columns))) * 100,\n        }\n\n        # Calculate quality score\n        quality_score = DataValidator._calculate_quality_score(data, len(issues), len(warnings_list), statistics)\n\n        is_valid = len(issues) == 0\n\n        return ValidationResult(\n            is_valid=is_valid,\n            quality_score=quality_score,\n            issues=issues,\n            warnings=warnings_list,\n            statistics=statistics,\n            missing_data=missing_data,\n        )\n\n    @staticmethod\n    def _calculate_quality_score(data: pd.DataFrame, n_issues: int, n_warnings: int, statistics: Dict[str, Any]) -&gt; float:\n        \"\"\"Calculate overall data quality score (0-1).\"\"\"\n        score = 1.0\n\n        # Penalize for issues\n        score -= min(0.5, n_issues * 0.1)\n\n        # Penalize for warnings\n        score -= min(0.3, n_warnings * 0.05)\n\n        # Penalize for missing data\n        pct_missing = statistics[\"pct_missing\"]\n        score -= min(0.2, pct_missing / 100 * 0.5)\n\n        return max(0.0, score)\n</code></pre>"},{"location":"api/io/#pyadm1ode_calibration.io.DataValidator-functions","title":"Functions","text":""},{"location":"api/io/#pyadm1ode_calibration.io.DataValidator.validate","title":"<code>validate(data, required_columns=None, expected_ranges=None)</code>  <code>staticmethod</code>","text":"<p>Validate measurement data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to validate</p> required <code>required_columns</code> <code>Optional[List[str]]</code> <p>List of required column names</p> <code>None</code> <code>expected_ranges</code> <code>Optional[Dict[str, Tuple[float, float]]]</code> <p>Dictionary mapping columns to (min, max) tuples</p> <code>None</code> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult object</p> Source code in <code>pyadm1ode_calibration/io/validation/validators.py</code> <pre><code>@staticmethod\ndef validate(\n    data: pd.DataFrame,\n    required_columns: Optional[List[str]] = None,\n    expected_ranges: Optional[Dict[str, Tuple[float, float]]] = None,\n) -&gt; ValidationResult:\n    \"\"\"\n    Validate measurement data.\n\n    Args:\n        data: DataFrame to validate\n        required_columns: List of required column names\n        expected_ranges: Dictionary mapping columns to (min, max) tuples\n\n    Returns:\n        ValidationResult object\n    \"\"\"\n    issues = []\n    warnings_list = []\n\n    # Check for required columns\n    if required_columns:\n        missing_cols = set(required_columns) - set(data.columns)\n        if missing_cols:\n            issues.append(f\"Missing required columns: {missing_cols}\")\n\n    # Calculate missing data percentages\n    missing_data = {}\n    for col in data.columns:\n        pct_missing = (data[col].isna().sum() / len(data)) * 100\n        missing_data[col] = pct_missing\n\n        if pct_missing &gt; 30:\n            issues.append(f\"Column '{col}' has {pct_missing:.1f}% missing data\")\n        elif pct_missing &gt; 5:\n            warnings_list.append(f\"Column '{col}' has {pct_missing:.1f}% missing data\")\n\n    # Check for expected ranges\n    if expected_ranges:\n        for col, (min_val, max_val) in expected_ranges.items():\n            if col in data.columns:\n                values = data[col].dropna()\n                if len(values) &gt; 0:\n                    actual_min = values.min()\n                    actual_max = values.max()\n\n                    if actual_min &lt; min_val or actual_max &gt; max_val:\n                        warnings_list.append(\n                            f\"Column '{col}' has values outside expected range \"\n                            f\"[{min_val}, {max_val}]: actual [{actual_min:.2f}, {actual_max:.2f}]\"\n                        )\n\n    # Check for duplicate timestamps\n    if \"timestamp\" in data.columns:\n        duplicates = data[\"timestamp\"].duplicated().sum()\n        if duplicates &gt; 0:\n            warnings_list.append(f\"Found {duplicates} duplicate timestamps\")\n\n    # Calculate statistics\n    statistics = {\n        \"n_rows\": len(data),\n        \"n_columns\": len(data.columns),\n        \"total_missing\": data.isna().sum().sum(),\n        \"pct_missing\": (data.isna().sum().sum() / (len(data) * len(data.columns))) * 100,\n    }\n\n    # Calculate quality score\n    quality_score = DataValidator._calculate_quality_score(data, len(issues), len(warnings_list), statistics)\n\n    is_valid = len(issues) == 0\n\n    return ValidationResult(\n        is_valid=is_valid,\n        quality_score=quality_score,\n        issues=issues,\n        warnings=warnings_list,\n        statistics=statistics,\n        missing_data=missing_data,\n    )\n</code></pre>"},{"location":"api/optimization/","title":"Optimization API","text":""},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.optimizer.Optimizer","title":"<code>pyadm1ode_calibration.calibration.optimization.optimizer.Optimizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for optimization algorithms.</p> <p>All optimizers must implement the optimize() method and provide a consistent interface for parameter calibration.</p> <p>Attributes:</p> Name Type Description <code>bounds</code> <p>Parameter bounds as {name: (min, max)}</p> <code>max_iterations</code> <p>Maximum number of iterations</p> <code>tolerance</code> <p>Convergence tolerance</p> <code>verbose</code> <p>Enable progress output</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/optimizer.py</code> <pre><code>class Optimizer(ABC):\n    \"\"\"\n    Abstract base class for optimization algorithms.\n\n    All optimizers must implement the optimize() method and provide\n    a consistent interface for parameter calibration.\n\n    Attributes:\n        bounds: Parameter bounds as {name: (min, max)}\n        max_iterations: Maximum number of iterations\n        tolerance: Convergence tolerance\n        verbose: Enable progress output\n    \"\"\"\n\n    def __init__(\n        self,\n        bounds: Dict[str, Tuple[float, float]],\n        max_iterations: int = 100,\n        tolerance: float = 1e-6,\n        verbose: bool = True,\n    ):\n        \"\"\"\n        Initialize optimizer.\n\n        Args:\n            bounds: Parameter bounds {name: (min, max)}\n            max_iterations: Maximum iterations\n            tolerance: Convergence tolerance\n            verbose: Enable output\n        \"\"\"\n        self.bounds = bounds\n        self.max_iterations = max_iterations\n        self.tolerance = tolerance\n        self.verbose = verbose\n\n        self.parameter_names = list(bounds.keys())\n        self.bounds_array = np.array([bounds[name] for name in self.parameter_names])\n\n        # History tracking\n        self.history: List[Dict[str, Any]] = []\n        self._best_value = float(\"inf\")\n        self._n_evaluations = 0\n\n    @abstractmethod\n    def optimize(\n        self, objective_func: Callable[[np.ndarray], float], initial_guess: Optional[np.ndarray] = None\n    ) -&gt; OptimizationResult:\n        \"\"\"\n        Run optimization.\n\n        Args:\n            objective_func: Function to minimize f(x) -&gt; float\n            initial_guess: Optional initial parameter guess\n\n        Returns:\n            OptimizationResult object\n        \"\"\"\n        pass\n\n    def _wrap_objective(self, objective_func: Callable[[np.ndarray], float]) -&gt; Callable[[np.ndarray], float]:\n        \"\"\"\n        Wrap objective function to track evaluations and history.\n\n        Args:\n            objective_func: Original objective function\n\n        Returns:\n            Wrapped objective function\n        \"\"\"\n\n        def wrapped(x: np.ndarray) -&gt; float:\n            # Evaluate objective\n            value = objective_func(x)\n\n            # Track evaluation\n            self._n_evaluations += 1\n\n            # Update history\n            param_dict = {name: float(val) for name, val in zip(self.parameter_names, x)}\n            self.history.append({\"parameters\": param_dict, \"objective\": float(value), \"iteration\": self._n_evaluations})\n\n            # Track best\n            if value &lt; self._best_value:\n                self._best_value = value\n                if self.verbose:\n                    param_str = \", \".join([f\"{name}={val:.4f}\" for name, val in param_dict.items()])\n                    print(f\"  Iteration {self._n_evaluations}: f={value:.6f} | {param_str}\")\n\n            return value\n\n        return wrapped\n\n    def _reset_tracking(self):\n        \"\"\"Reset history and counters.\"\"\"\n        self.history = []\n        self._best_value = float(\"inf\")\n        self._n_evaluations = 0\n\n    def _check_bounds(self, x: np.ndarray) -&gt; bool:\n        \"\"\"Check if parameters are within bounds.\"\"\"\n        return np.all(x &gt;= self.bounds_array[:, 0]) and np.all(x &lt;= self.bounds_array[:, 1])\n\n    def _project_to_bounds(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Project parameters to bounds.\"\"\"\n        return np.clip(x, self.bounds_array[:, 0], self.bounds_array[:, 1])\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.optimizer.Optimizer-functions","title":"Functions","text":""},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.optimizer.Optimizer.__init__","title":"<code>__init__(bounds, max_iterations=100, tolerance=1e-06, verbose=True)</code>","text":"<p>Initialize optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>Dict[str, Tuple[float, float]]</code> <p>Parameter bounds {name: (min, max)}</p> required <code>max_iterations</code> <code>int</code> <p>Maximum iterations</p> <code>100</code> <code>tolerance</code> <code>float</code> <p>Convergence tolerance</p> <code>1e-06</code> <code>verbose</code> <code>bool</code> <p>Enable output</p> <code>True</code> Source code in <code>pyadm1ode_calibration/calibration/optimization/optimizer.py</code> <pre><code>def __init__(\n    self,\n    bounds: Dict[str, Tuple[float, float]],\n    max_iterations: int = 100,\n    tolerance: float = 1e-6,\n    verbose: bool = True,\n):\n    \"\"\"\n    Initialize optimizer.\n\n    Args:\n        bounds: Parameter bounds {name: (min, max)}\n        max_iterations: Maximum iterations\n        tolerance: Convergence tolerance\n        verbose: Enable output\n    \"\"\"\n    self.bounds = bounds\n    self.max_iterations = max_iterations\n    self.tolerance = tolerance\n    self.verbose = verbose\n\n    self.parameter_names = list(bounds.keys())\n    self.bounds_array = np.array([bounds[name] for name in self.parameter_names])\n\n    # History tracking\n    self.history: List[Dict[str, Any]] = []\n    self._best_value = float(\"inf\")\n    self._n_evaluations = 0\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.optimizer.Optimizer.optimize","title":"<code>optimize(objective_func, initial_guess=None)</code>  <code>abstractmethod</code>","text":"<p>Run optimization.</p> <p>Parameters:</p> Name Type Description Default <code>objective_func</code> <code>Callable[[ndarray], float]</code> <p>Function to minimize f(x) -&gt; float</p> required <code>initial_guess</code> <code>Optional[ndarray]</code> <p>Optional initial parameter guess</p> <code>None</code> <p>Returns:</p> Type Description <code>OptimizationResult</code> <p>OptimizationResult object</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/optimizer.py</code> <pre><code>@abstractmethod\ndef optimize(\n    self, objective_func: Callable[[np.ndarray], float], initial_guess: Optional[np.ndarray] = None\n) -&gt; OptimizationResult:\n    \"\"\"\n    Run optimization.\n\n    Args:\n        objective_func: Function to minimize f(x) -&gt; float\n        initial_guess: Optional initial parameter guess\n\n    Returns:\n        OptimizationResult object\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.objective.MultiObjectiveFunction","title":"<code>pyadm1ode_calibration.calibration.optimization.objective.MultiObjectiveFunction</code>","text":"<p>               Bases: <code>ObjectiveFunction</code></p> <p>Multi-objective function with weighted combination.</p> <p>Combines errors from multiple outputs using weights to create a single scalar objective.</p> Example <p>objective = MultiObjectiveFunction( ...     simulator=simulator, ...     measurements_dict={ ...         \"Q_ch4\": measured_ch4, ...         \"pH\": measured_ph, ...         \"VFA\": measured_vfa ...     }, ...     objectives=[\"Q_ch4\", \"pH\", \"VFA\"], ...     weights={\"Q_ch4\": 0.6, \"pH\": 0.2, \"VFA\": 0.2}, ...     parameter_names=[\"k_dis\", \"Y_su\", \"k_hyd_ch\"], ...     error_metric=\"rmse\" ... )</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/objective.py</code> <pre><code>class MultiObjectiveFunction(ObjectiveFunction):\n    \"\"\"\n    Multi-objective function with weighted combination.\n\n    Combines errors from multiple outputs using weights to create\n    a single scalar objective.\n\n    Example:\n        &gt;&gt;&gt; objective = MultiObjectiveFunction(\n        ...     simulator=simulator,\n        ...     measurements_dict={\n        ...         \"Q_ch4\": measured_ch4,\n        ...         \"pH\": measured_ph,\n        ...         \"VFA\": measured_vfa\n        ...     },\n        ...     objectives=[\"Q_ch4\", \"pH\", \"VFA\"],\n        ...     weights={\"Q_ch4\": 0.6, \"pH\": 0.2, \"VFA\": 0.2},\n        ...     parameter_names=[\"k_dis\", \"Y_su\", \"k_hyd_ch\"],\n        ...     error_metric=\"rmse\"\n        ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        simulator: Callable[[Dict[str, float]], Dict[str, np.ndarray]],\n        measurements_dict: Dict[str, np.ndarray],\n        objectives: List[str],\n        weights: Dict[str, float],\n        parameter_names: List[str],\n        error_metric: str = \"rmse\",\n        normalize: bool = True,\n    ):\n        \"\"\"\n        Initialize multi-objective function.\n\n        Args:\n            simulator: Function that takes parameters and returns outputs\n            measurements_dict: Dictionary mapping objective names to measurements\n            objectives: List of objective names\n            weights: Dictionary of weights for each objective\n            parameter_names: Names of parameters\n            error_metric: Error metric to use\n            normalize: Normalize errors by mean of measurements\n        \"\"\"\n        super().__init__(parameter_names)\n\n        self.simulator = simulator\n        self.measurements_dict = measurements_dict\n        self.objectives = objectives\n        self.weights = weights\n        self.error_metric = error_metric.lower()\n        self.normalize = normalize\n\n        # Normalize weights\n        total_weight = sum(weights.values())\n        if total_weight &gt; 0:\n            self.weights = {k: v / total_weight for k, v in weights.items()}\n\n    def __call__(self, x: np.ndarray) -&gt; float:\n        \"\"\"\n        Evaluate multi-objective function.\n\n        Args:\n            x: Parameter values\n\n        Returns:\n            Weighted sum of errors\n        \"\"\"\n        params = self._params_to_dict(x)\n\n        try:\n            # Run simulation\n            outputs = self.simulator(params)\n\n            # Calculate weighted error\n            total_error = 0.0\n            n_valid = 0\n\n            for obj_name in self.objectives:\n                if obj_name not in outputs or obj_name not in self.measurements_dict:\n                    continue\n\n                simulated = outputs[obj_name]\n                measured = self.measurements_dict[obj_name]\n\n                # Compute error\n                metrics = ErrorMetrics.compute(measured, simulated)\n\n                # Get error value\n                if self.error_metric == \"mse\":\n                    error = metrics.mse\n                elif self.error_metric == \"mae\":\n                    error = metrics.mae\n                elif self.error_metric == \"mape\":\n                    error = metrics.mape\n                elif self.error_metric == \"nse\":\n                    error = -metrics.nse\n                elif self.error_metric == \"r2\":\n                    error = -metrics.r2\n                else:  # Default to RMSE\n                    error = metrics.rmse\n\n                # Normalize by mean of measurements if requested\n                if self.normalize:\n                    mean_measured = np.mean(np.abs(measured))\n                    if mean_measured &gt; 1e-10:\n                        error = error / mean_measured\n\n                # Add weighted error\n                weight = self.weights.get(obj_name, 0.0)\n                total_error += weight * error\n                n_valid += 1\n\n            if n_valid == 0:\n                return 1e10\n\n            return total_error\n\n        except Exception as e:\n            print(f\"Simulation error: {e}\")\n            return 1e10\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.objective.MultiObjectiveFunction-functions","title":"Functions","text":""},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.objective.MultiObjectiveFunction.__call__","title":"<code>__call__(x)</code>","text":"<p>Evaluate multi-objective function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Parameter values</p> required <p>Returns:</p> Type Description <code>float</code> <p>Weighted sum of errors</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/objective.py</code> <pre><code>def __call__(self, x: np.ndarray) -&gt; float:\n    \"\"\"\n    Evaluate multi-objective function.\n\n    Args:\n        x: Parameter values\n\n    Returns:\n        Weighted sum of errors\n    \"\"\"\n    params = self._params_to_dict(x)\n\n    try:\n        # Run simulation\n        outputs = self.simulator(params)\n\n        # Calculate weighted error\n        total_error = 0.0\n        n_valid = 0\n\n        for obj_name in self.objectives:\n            if obj_name not in outputs or obj_name not in self.measurements_dict:\n                continue\n\n            simulated = outputs[obj_name]\n            measured = self.measurements_dict[obj_name]\n\n            # Compute error\n            metrics = ErrorMetrics.compute(measured, simulated)\n\n            # Get error value\n            if self.error_metric == \"mse\":\n                error = metrics.mse\n            elif self.error_metric == \"mae\":\n                error = metrics.mae\n            elif self.error_metric == \"mape\":\n                error = metrics.mape\n            elif self.error_metric == \"nse\":\n                error = -metrics.nse\n            elif self.error_metric == \"r2\":\n                error = -metrics.r2\n            else:  # Default to RMSE\n                error = metrics.rmse\n\n            # Normalize by mean of measurements if requested\n            if self.normalize:\n                mean_measured = np.mean(np.abs(measured))\n                if mean_measured &gt; 1e-10:\n                    error = error / mean_measured\n\n            # Add weighted error\n            weight = self.weights.get(obj_name, 0.0)\n            total_error += weight * error\n            n_valid += 1\n\n        if n_valid == 0:\n            return 1e10\n\n        return total_error\n\n    except Exception as e:\n        print(f\"Simulation error: {e}\")\n        return 1e10\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.objective.MultiObjectiveFunction.__init__","title":"<code>__init__(simulator, measurements_dict, objectives, weights, parameter_names, error_metric='rmse', normalize=True)</code>","text":"<p>Initialize multi-objective function.</p> <p>Parameters:</p> Name Type Description Default <code>simulator</code> <code>Callable[[Dict[str, float]], Dict[str, ndarray]]</code> <p>Function that takes parameters and returns outputs</p> required <code>measurements_dict</code> <code>Dict[str, ndarray]</code> <p>Dictionary mapping objective names to measurements</p> required <code>objectives</code> <code>List[str]</code> <p>List of objective names</p> required <code>weights</code> <code>Dict[str, float]</code> <p>Dictionary of weights for each objective</p> required <code>parameter_names</code> <code>List[str]</code> <p>Names of parameters</p> required <code>error_metric</code> <code>str</code> <p>Error metric to use</p> <code>'rmse'</code> <code>normalize</code> <code>bool</code> <p>Normalize errors by mean of measurements</p> <code>True</code> Source code in <code>pyadm1ode_calibration/calibration/optimization/objective.py</code> <pre><code>def __init__(\n    self,\n    simulator: Callable[[Dict[str, float]], Dict[str, np.ndarray]],\n    measurements_dict: Dict[str, np.ndarray],\n    objectives: List[str],\n    weights: Dict[str, float],\n    parameter_names: List[str],\n    error_metric: str = \"rmse\",\n    normalize: bool = True,\n):\n    \"\"\"\n    Initialize multi-objective function.\n\n    Args:\n        simulator: Function that takes parameters and returns outputs\n        measurements_dict: Dictionary mapping objective names to measurements\n        objectives: List of objective names\n        weights: Dictionary of weights for each objective\n        parameter_names: Names of parameters\n        error_metric: Error metric to use\n        normalize: Normalize errors by mean of measurements\n    \"\"\"\n    super().__init__(parameter_names)\n\n    self.simulator = simulator\n    self.measurements_dict = measurements_dict\n    self.objectives = objectives\n    self.weights = weights\n    self.error_metric = error_metric.lower()\n    self.normalize = normalize\n\n    # Normalize weights\n    total_weight = sum(weights.values())\n    if total_weight &gt; 0:\n        self.weights = {k: v / total_weight for k, v in weights.items()}\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints","title":"<code>pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints</code>","text":"<p>Manager for all parameter constraints.</p> <p>Handles box constraints, linear constraints, and nonlinear constraints with support for penalty functions.</p> Example <p>constraints = ParameterConstraints() constraints.add_box_constraint(\"k_dis\", 0.3, 0.8) constraints.add_linear_inequality({\"k_dis\": 1, \"Y_su\": 1}, upper_bound=1.0) is_valid = constraints.is_feasible({\"k_dis\": 0.5, \"Y_su\": 0.1})</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>class ParameterConstraints:\n    \"\"\"\n    Manager for all parameter constraints.\n\n    Handles box constraints, linear constraints, and nonlinear constraints\n    with support for penalty functions.\n\n    Example:\n        &gt;&gt;&gt; constraints = ParameterConstraints()\n        &gt;&gt;&gt; constraints.add_box_constraint(\"k_dis\", 0.3, 0.8)\n        &gt;&gt;&gt; constraints.add_linear_inequality({\"k_dis\": 1, \"Y_su\": 1}, upper_bound=1.0)\n        &gt;&gt;&gt; is_valid = constraints.is_feasible({\"k_dis\": 0.5, \"Y_su\": 0.1})\n    \"\"\"\n\n    def __init__(self, penalty_function: Optional[PenaltyFunction] = None):\n        \"\"\"\n        Initialize constraint manager.\n\n        Args:\n            penalty_function: Penalty function for soft constraints\n                            (default: QuadraticPenalty)\n        \"\"\"\n        self.box_constraints: Dict[str, BoxConstraint] = {}\n        self.linear_constraints: List[LinearConstraint] = []\n        self.nonlinear_constraints: List[NonlinearConstraint] = []\n\n        self.penalty_function = penalty_function or QuadraticPenalty()\n        self.penalty_weights: Dict[str, float] = {}\n\n    def add_box_constraint(self, parameter_name: str, lower: float, upper: float, hard: bool = True, weight: float = 1.0):\n        \"\"\"\n        Add box constraint (bounds) for a parameter.\n\n        Args:\n            parameter_name: Parameter name\n            lower: Lower bound\n            upper: Upper bound\n            hard: Hard constraint (infinite penalty if violated)\n            weight: Penalty weight for soft constraints\n        \"\"\"\n        self.box_constraints[parameter_name] = BoxConstraint(parameter_name, lower, upper, hard)\n\n        if not hard:\n            self.penalty_weights[f\"box_{parameter_name}\"] = weight\n\n    def add_linear_inequality(\n        self,\n        coefficients: Dict[str, float],\n        lower_bound: Optional[float] = None,\n        upper_bound: Optional[float] = None,\n        weight: float = 1.0,\n    ):\n        \"\"\"\n        Add linear inequality constraint.\n\n        Args:\n            coefficients: Coefficients for each parameter\n            lower_bound: Lower bound (sum &gt;= lower_bound)\n            upper_bound: Upper bound (sum &lt;= upper_bound)\n            weight: Penalty weight\n        \"\"\"\n        constraint = LinearConstraint(coefficients, lower_bound, upper_bound, constraint_type=\"inequality\")\n\n        self.linear_constraints.append(constraint)\n\n        # Store penalty weight\n        constraint_id = f\"linear_{len(self.linear_constraints)}\"\n        self.penalty_weights[constraint_id] = weight\n\n    def add_linear_equality(self, coefficients: Dict[str, float], target: float, weight: float = 1.0):\n        \"\"\"\n        Add linear equality constraint: sum(coefficients * parameters) == target\n\n        Args:\n            coefficients: Coefficients for each parameter\n            target: Target value\n            weight: Penalty weight\n        \"\"\"\n        constraint = LinearConstraint(coefficients, lower_bound=None, upper_bound=target, constraint_type=\"equality\")\n\n        self.linear_constraints.append(constraint)\n\n        constraint_id = f\"linear_eq_{len(self.linear_constraints)}\"\n        self.penalty_weights[constraint_id] = weight\n\n    def add_nonlinear_constraint(\n        self,\n        name: str,\n        function: Callable[[Dict[str, float]], float],\n        constraint_type: str = \"inequality\",\n        weight: float = 1.0,\n    ):\n        \"\"\"\n        Add nonlinear constraint.\n\n        Args:\n            name: Constraint name\n            function: Constraint function g(params) -&gt; float\n                     For inequality: g(x) &lt;= 0\n                     For equality: g(x) == 0\n            constraint_type: \"inequality\" or \"equality\"\n            weight: Penalty weight\n        \"\"\"\n        constraint = NonlinearConstraint(name, function, constraint_type)\n\n        self.nonlinear_constraints.append(constraint)\n\n        constraint_id = f\"nonlinear_{name}\"\n        self.penalty_weights[constraint_id] = weight\n\n    def is_feasible(self, parameters: Dict[str, float]) -&gt; bool:\n        \"\"\"\n        Check if parameters satisfy all hard constraints.\n\n        Args:\n            parameters: Parameter values\n\n        Returns:\n            True if all hard constraints are satisfied\n        \"\"\"\n        # Check box constraints\n        for name, constraint in self.box_constraints.items():\n            if constraint.hard:\n                value = parameters.get(name, 0.0)\n                if not constraint.is_feasible(value):\n                    return False\n\n        # Check linear constraints (all treated as hard for feasibility)\n        for constraint in self.linear_constraints:\n            if not constraint.is_feasible(parameters):\n                return False\n\n        # Check nonlinear constraints (all treated as hard)\n        for constraint in self.nonlinear_constraints:\n            if not constraint.is_feasible(parameters):\n                return False\n\n        return True\n\n    def calculate_penalty(self, parameters: Dict[str, float]) -&gt; float:\n        \"\"\"\n        Calculate total penalty for constraint violations.\n\n        Args:\n            parameters: Parameter values\n\n        Returns:\n            Total penalty value\n        \"\"\"\n        total_penalty = 0.0\n\n        # Box constraint penalties\n        for name, constraint in self.box_constraints.items():\n            value = parameters.get(name, 0.0)\n            violation = constraint.violation(value)\n\n            if violation &gt; 0:\n                if constraint.hard:\n                    return float(\"inf\")\n\n                weight = self.penalty_weights.get(f\"box_{name}\", 1.0)\n                penalty = self.penalty_function(violation, weight)\n                total_penalty += penalty\n\n        # Linear constraint penalties\n        for i, constraint in enumerate(self.linear_constraints, 1):\n            violation = constraint.violation(parameters)\n\n            if violation &gt; 0:\n                weight = self.penalty_weights.get(f\"linear_{i}\", 1.0)\n                penalty = self.penalty_function(violation, weight)\n                total_penalty += penalty\n\n        # Nonlinear constraint penalties\n        for constraint in self.nonlinear_constraints:\n            violation = constraint.violation(parameters)\n\n            if violation &gt; 0:\n                weight = self.penalty_weights.get(f\"nonlinear_{constraint.name}\", 1.0)\n                penalty = self.penalty_function(violation, weight)\n                total_penalty += penalty\n\n        return total_penalty\n\n    def project_to_feasible(self, parameters: Dict[str, float]) -&gt; Dict[str, float]:\n        \"\"\"\n        Project parameters to feasible region (box constraints only).\n\n        Args:\n            parameters: Parameter values\n\n        Returns:\n            Projected parameters\n        \"\"\"\n        projected = parameters.copy()\n\n        for name, constraint in self.box_constraints.items():\n            if name in projected:\n                projected[name] = constraint.project(projected[name])\n\n        return projected\n\n    def get_bounds_array(self, parameter_names: List[str]) -&gt; np.ndarray:\n        \"\"\"\n        Get bounds as array for scipy optimizers.\n\n        Args:\n            parameter_names: Ordered list of parameter names\n\n        Returns:\n            Array of shape (n_params, 2) with [lower, upper] for each parameter\n        \"\"\"\n        bounds = []\n        for name in parameter_names:\n            if name in self.box_constraints:\n                constraint = self.box_constraints[name]\n                bounds.append([constraint.lower, constraint.upper])\n            else:\n                bounds.append([None, None])\n\n        return np.array(bounds)\n\n    def get_scipy_constraints(self, parameter_names: List[str]) -&gt; List[Dict]:\n        \"\"\"\n        Convert constraints to scipy format for constrained optimization.\n\n        Args:\n            parameter_names: Ordered list of parameter names\n\n        Returns:\n            List of constraint dictionaries for scipy.optimize.minimize\n        \"\"\"\n        scipy_constraints = []\n\n        # Linear constraints\n        for constraint in self.linear_constraints:\n            # Build coefficient array\n            coef_array = np.array([constraint.coefficients.get(name, 0.0) for name in parameter_names])\n\n            if constraint.constraint_type == \"equality\":\n                # Equality: sum(coef * x) == target\n                scipy_constraints.append(\n                    {\"type\": \"eq\", \"fun\": lambda x, c=coef_array, b=constraint.upper_bound: np.dot(c, x) - b}\n                )\n            else:\n                # Inequality constraints\n                if constraint.lower_bound is not None:\n                    # sum(coef * x) &gt;= lower\n                    scipy_constraints.append(\n                        {\"type\": \"ineq\", \"fun\": lambda x, c=coef_array, b=constraint.lower_bound: np.dot(c, x) - b}\n                    )\n                if constraint.upper_bound is not None:\n                    # sum(coef * x) &lt;= upper  =&gt;  upper - sum(coef * x) &gt;= 0\n                    scipy_constraints.append(\n                        {\"type\": \"ineq\", \"fun\": lambda x, c=coef_array, b=constraint.upper_bound: b - np.dot(c, x)}\n                    )\n\n        # Nonlinear constraints\n        for constraint in self.nonlinear_constraints:\n\n            def constraint_func(x, names=parameter_names, func=constraint.function):\n                params = {name: val for name, val in zip(names, x)}\n                return func(params)\n\n            if constraint.constraint_type == \"equality\":\n                scipy_constraints.append({\"type\": \"eq\", \"fun\": constraint_func})\n            else:\n                # g(x) &lt;= 0  =&gt;  -g(x) &gt;= 0\n                scipy_constraints.append({\"type\": \"ineq\", \"fun\": lambda x, f=constraint_func: -f(x)})\n\n        return scipy_constraints\n\n    def validate_parameters(self, parameters: Dict[str, float]) -&gt; Tuple[bool, List[str]]:\n        \"\"\"\n        Validate parameters and return detailed error messages.\n\n        Args:\n            parameters: Parameter values\n\n        Returns:\n            Tuple of (is_valid, error_messages)\n        \"\"\"\n        errors = []\n\n        # Check box constraints\n        for name, constraint in self.box_constraints.items():\n            if name in parameters:\n                value = parameters[name]\n                if not constraint.is_feasible(value):\n                    errors.append(\n                        f\"Parameter '{name}' = {value:.4f} violates bounds [{constraint.lower:.4f}, {constraint.upper:.4f}]\"\n                    )\n\n        # Check linear constraints\n        for i, constraint in enumerate(self.linear_constraints, 1):\n            if not constraint.is_feasible(parameters):\n                value = constraint.evaluate(parameters)\n                if constraint.constraint_type == \"equality\":\n                    errors.append(f\"Linear constraint {i}: {value:.4f} != {constraint.upper_bound:.4f}\")\n                else:\n                    if constraint.lower_bound and value &lt; constraint.lower_bound:\n                        errors.append(f\"Linear constraint {i}: {value:.4f} &lt; {constraint.lower_bound:.4f}\")\n                    if constraint.upper_bound and value &gt; constraint.upper_bound:\n                        errors.append(f\"Linear constraint {i}: {value:.4f} &gt; {constraint.upper_bound:.4f}\")\n\n        # Check nonlinear constraints\n        for constraint in self.nonlinear_constraints:\n            if not constraint.is_feasible(parameters):\n                value = constraint.evaluate(parameters)\n                errors.append(\n                    f\"Nonlinear constraint '{constraint.name}': g(x) = {value:.4f} violates \"\n                    f\"{constraint.constraint_type} constraint\"\n                )\n\n        return len(errors) == 0, errors\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints-functions","title":"Functions","text":""},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.__init__","title":"<code>__init__(penalty_function=None)</code>","text":"<p>Initialize constraint manager.</p> <p>Parameters:</p> Name Type Description Default <code>penalty_function</code> <code>Optional[PenaltyFunction]</code> <p>Penalty function for soft constraints             (default: QuadraticPenalty)</p> <code>None</code> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def __init__(self, penalty_function: Optional[PenaltyFunction] = None):\n    \"\"\"\n    Initialize constraint manager.\n\n    Args:\n        penalty_function: Penalty function for soft constraints\n                        (default: QuadraticPenalty)\n    \"\"\"\n    self.box_constraints: Dict[str, BoxConstraint] = {}\n    self.linear_constraints: List[LinearConstraint] = []\n    self.nonlinear_constraints: List[NonlinearConstraint] = []\n\n    self.penalty_function = penalty_function or QuadraticPenalty()\n    self.penalty_weights: Dict[str, float] = {}\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.add_box_constraint","title":"<code>add_box_constraint(parameter_name, lower, upper, hard=True, weight=1.0)</code>","text":"<p>Add box constraint (bounds) for a parameter.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>Parameter name</p> required <code>lower</code> <code>float</code> <p>Lower bound</p> required <code>upper</code> <code>float</code> <p>Upper bound</p> required <code>hard</code> <code>bool</code> <p>Hard constraint (infinite penalty if violated)</p> <code>True</code> <code>weight</code> <code>float</code> <p>Penalty weight for soft constraints</p> <code>1.0</code> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def add_box_constraint(self, parameter_name: str, lower: float, upper: float, hard: bool = True, weight: float = 1.0):\n    \"\"\"\n    Add box constraint (bounds) for a parameter.\n\n    Args:\n        parameter_name: Parameter name\n        lower: Lower bound\n        upper: Upper bound\n        hard: Hard constraint (infinite penalty if violated)\n        weight: Penalty weight for soft constraints\n    \"\"\"\n    self.box_constraints[parameter_name] = BoxConstraint(parameter_name, lower, upper, hard)\n\n    if not hard:\n        self.penalty_weights[f\"box_{parameter_name}\"] = weight\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.add_linear_equality","title":"<code>add_linear_equality(coefficients, target, weight=1.0)</code>","text":"<p>Add linear equality constraint: sum(coefficients * parameters) == target</p> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>Dict[str, float]</code> <p>Coefficients for each parameter</p> required <code>target</code> <code>float</code> <p>Target value</p> required <code>weight</code> <code>float</code> <p>Penalty weight</p> <code>1.0</code> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def add_linear_equality(self, coefficients: Dict[str, float], target: float, weight: float = 1.0):\n    \"\"\"\n    Add linear equality constraint: sum(coefficients * parameters) == target\n\n    Args:\n        coefficients: Coefficients for each parameter\n        target: Target value\n        weight: Penalty weight\n    \"\"\"\n    constraint = LinearConstraint(coefficients, lower_bound=None, upper_bound=target, constraint_type=\"equality\")\n\n    self.linear_constraints.append(constraint)\n\n    constraint_id = f\"linear_eq_{len(self.linear_constraints)}\"\n    self.penalty_weights[constraint_id] = weight\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.add_linear_inequality","title":"<code>add_linear_inequality(coefficients, lower_bound=None, upper_bound=None, weight=1.0)</code>","text":"<p>Add linear inequality constraint.</p> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>Dict[str, float]</code> <p>Coefficients for each parameter</p> required <code>lower_bound</code> <code>Optional[float]</code> <p>Lower bound (sum &gt;= lower_bound)</p> <code>None</code> <code>upper_bound</code> <code>Optional[float]</code> <p>Upper bound (sum &lt;= upper_bound)</p> <code>None</code> <code>weight</code> <code>float</code> <p>Penalty weight</p> <code>1.0</code> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def add_linear_inequality(\n    self,\n    coefficients: Dict[str, float],\n    lower_bound: Optional[float] = None,\n    upper_bound: Optional[float] = None,\n    weight: float = 1.0,\n):\n    \"\"\"\n    Add linear inequality constraint.\n\n    Args:\n        coefficients: Coefficients for each parameter\n        lower_bound: Lower bound (sum &gt;= lower_bound)\n        upper_bound: Upper bound (sum &lt;= upper_bound)\n        weight: Penalty weight\n    \"\"\"\n    constraint = LinearConstraint(coefficients, lower_bound, upper_bound, constraint_type=\"inequality\")\n\n    self.linear_constraints.append(constraint)\n\n    # Store penalty weight\n    constraint_id = f\"linear_{len(self.linear_constraints)}\"\n    self.penalty_weights[constraint_id] = weight\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.add_nonlinear_constraint","title":"<code>add_nonlinear_constraint(name, function, constraint_type='inequality', weight=1.0)</code>","text":"<p>Add nonlinear constraint.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Constraint name</p> required <code>function</code> <code>Callable[[Dict[str, float]], float]</code> <p>Constraint function g(params) -&gt; float      For inequality: g(x) &lt;= 0      For equality: g(x) == 0</p> required <code>constraint_type</code> <code>str</code> <p>\"inequality\" or \"equality\"</p> <code>'inequality'</code> <code>weight</code> <code>float</code> <p>Penalty weight</p> <code>1.0</code> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def add_nonlinear_constraint(\n    self,\n    name: str,\n    function: Callable[[Dict[str, float]], float],\n    constraint_type: str = \"inequality\",\n    weight: float = 1.0,\n):\n    \"\"\"\n    Add nonlinear constraint.\n\n    Args:\n        name: Constraint name\n        function: Constraint function g(params) -&gt; float\n                 For inequality: g(x) &lt;= 0\n                 For equality: g(x) == 0\n        constraint_type: \"inequality\" or \"equality\"\n        weight: Penalty weight\n    \"\"\"\n    constraint = NonlinearConstraint(name, function, constraint_type)\n\n    self.nonlinear_constraints.append(constraint)\n\n    constraint_id = f\"nonlinear_{name}\"\n    self.penalty_weights[constraint_id] = weight\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.calculate_penalty","title":"<code>calculate_penalty(parameters)</code>","text":"<p>Calculate total penalty for constraint violations.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Parameter values</p> required <p>Returns:</p> Type Description <code>float</code> <p>Total penalty value</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def calculate_penalty(self, parameters: Dict[str, float]) -&gt; float:\n    \"\"\"\n    Calculate total penalty for constraint violations.\n\n    Args:\n        parameters: Parameter values\n\n    Returns:\n        Total penalty value\n    \"\"\"\n    total_penalty = 0.0\n\n    # Box constraint penalties\n    for name, constraint in self.box_constraints.items():\n        value = parameters.get(name, 0.0)\n        violation = constraint.violation(value)\n\n        if violation &gt; 0:\n            if constraint.hard:\n                return float(\"inf\")\n\n            weight = self.penalty_weights.get(f\"box_{name}\", 1.0)\n            penalty = self.penalty_function(violation, weight)\n            total_penalty += penalty\n\n    # Linear constraint penalties\n    for i, constraint in enumerate(self.linear_constraints, 1):\n        violation = constraint.violation(parameters)\n\n        if violation &gt; 0:\n            weight = self.penalty_weights.get(f\"linear_{i}\", 1.0)\n            penalty = self.penalty_function(violation, weight)\n            total_penalty += penalty\n\n    # Nonlinear constraint penalties\n    for constraint in self.nonlinear_constraints:\n        violation = constraint.violation(parameters)\n\n        if violation &gt; 0:\n            weight = self.penalty_weights.get(f\"nonlinear_{constraint.name}\", 1.0)\n            penalty = self.penalty_function(violation, weight)\n            total_penalty += penalty\n\n    return total_penalty\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.get_bounds_array","title":"<code>get_bounds_array(parameter_names)</code>","text":"<p>Get bounds as array for scipy optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_names</code> <code>List[str]</code> <p>Ordered list of parameter names</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_params, 2) with [lower, upper] for each parameter</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def get_bounds_array(self, parameter_names: List[str]) -&gt; np.ndarray:\n    \"\"\"\n    Get bounds as array for scipy optimizers.\n\n    Args:\n        parameter_names: Ordered list of parameter names\n\n    Returns:\n        Array of shape (n_params, 2) with [lower, upper] for each parameter\n    \"\"\"\n    bounds = []\n    for name in parameter_names:\n        if name in self.box_constraints:\n            constraint = self.box_constraints[name]\n            bounds.append([constraint.lower, constraint.upper])\n        else:\n            bounds.append([None, None])\n\n    return np.array(bounds)\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.get_scipy_constraints","title":"<code>get_scipy_constraints(parameter_names)</code>","text":"<p>Convert constraints to scipy format for constrained optimization.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_names</code> <code>List[str]</code> <p>Ordered list of parameter names</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of constraint dictionaries for scipy.optimize.minimize</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def get_scipy_constraints(self, parameter_names: List[str]) -&gt; List[Dict]:\n    \"\"\"\n    Convert constraints to scipy format for constrained optimization.\n\n    Args:\n        parameter_names: Ordered list of parameter names\n\n    Returns:\n        List of constraint dictionaries for scipy.optimize.minimize\n    \"\"\"\n    scipy_constraints = []\n\n    # Linear constraints\n    for constraint in self.linear_constraints:\n        # Build coefficient array\n        coef_array = np.array([constraint.coefficients.get(name, 0.0) for name in parameter_names])\n\n        if constraint.constraint_type == \"equality\":\n            # Equality: sum(coef * x) == target\n            scipy_constraints.append(\n                {\"type\": \"eq\", \"fun\": lambda x, c=coef_array, b=constraint.upper_bound: np.dot(c, x) - b}\n            )\n        else:\n            # Inequality constraints\n            if constraint.lower_bound is not None:\n                # sum(coef * x) &gt;= lower\n                scipy_constraints.append(\n                    {\"type\": \"ineq\", \"fun\": lambda x, c=coef_array, b=constraint.lower_bound: np.dot(c, x) - b}\n                )\n            if constraint.upper_bound is not None:\n                # sum(coef * x) &lt;= upper  =&gt;  upper - sum(coef * x) &gt;= 0\n                scipy_constraints.append(\n                    {\"type\": \"ineq\", \"fun\": lambda x, c=coef_array, b=constraint.upper_bound: b - np.dot(c, x)}\n                )\n\n    # Nonlinear constraints\n    for constraint in self.nonlinear_constraints:\n\n        def constraint_func(x, names=parameter_names, func=constraint.function):\n            params = {name: val for name, val in zip(names, x)}\n            return func(params)\n\n        if constraint.constraint_type == \"equality\":\n            scipy_constraints.append({\"type\": \"eq\", \"fun\": constraint_func})\n        else:\n            # g(x) &lt;= 0  =&gt;  -g(x) &gt;= 0\n            scipy_constraints.append({\"type\": \"ineq\", \"fun\": lambda x, f=constraint_func: -f(x)})\n\n    return scipy_constraints\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.is_feasible","title":"<code>is_feasible(parameters)</code>","text":"<p>Check if parameters satisfy all hard constraints.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Parameter values</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all hard constraints are satisfied</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def is_feasible(self, parameters: Dict[str, float]) -&gt; bool:\n    \"\"\"\n    Check if parameters satisfy all hard constraints.\n\n    Args:\n        parameters: Parameter values\n\n    Returns:\n        True if all hard constraints are satisfied\n    \"\"\"\n    # Check box constraints\n    for name, constraint in self.box_constraints.items():\n        if constraint.hard:\n            value = parameters.get(name, 0.0)\n            if not constraint.is_feasible(value):\n                return False\n\n    # Check linear constraints (all treated as hard for feasibility)\n    for constraint in self.linear_constraints:\n        if not constraint.is_feasible(parameters):\n            return False\n\n    # Check nonlinear constraints (all treated as hard)\n    for constraint in self.nonlinear_constraints:\n        if not constraint.is_feasible(parameters):\n            return False\n\n    return True\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.project_to_feasible","title":"<code>project_to_feasible(parameters)</code>","text":"<p>Project parameters to feasible region (box constraints only).</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Parameter values</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Projected parameters</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def project_to_feasible(self, parameters: Dict[str, float]) -&gt; Dict[str, float]:\n    \"\"\"\n    Project parameters to feasible region (box constraints only).\n\n    Args:\n        parameters: Parameter values\n\n    Returns:\n        Projected parameters\n    \"\"\"\n    projected = parameters.copy()\n\n    for name, constraint in self.box_constraints.items():\n        if name in projected:\n            projected[name] = constraint.project(projected[name])\n\n    return projected\n</code></pre>"},{"location":"api/optimization/#pyadm1ode_calibration.calibration.optimization.constraints.ParameterConstraints.validate_parameters","title":"<code>validate_parameters(parameters)</code>","text":"<p>Validate parameters and return detailed error messages.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Parameter values</p> required <p>Returns:</p> Type Description <code>Tuple[bool, List[str]]</code> <p>Tuple of (is_valid, error_messages)</p> Source code in <code>pyadm1ode_calibration/calibration/optimization/constraints.py</code> <pre><code>def validate_parameters(self, parameters: Dict[str, float]) -&gt; Tuple[bool, List[str]]:\n    \"\"\"\n    Validate parameters and return detailed error messages.\n\n    Args:\n        parameters: Parameter values\n\n    Returns:\n        Tuple of (is_valid, error_messages)\n    \"\"\"\n    errors = []\n\n    # Check box constraints\n    for name, constraint in self.box_constraints.items():\n        if name in parameters:\n            value = parameters[name]\n            if not constraint.is_feasible(value):\n                errors.append(\n                    f\"Parameter '{name}' = {value:.4f} violates bounds [{constraint.lower:.4f}, {constraint.upper:.4f}]\"\n                )\n\n    # Check linear constraints\n    for i, constraint in enumerate(self.linear_constraints, 1):\n        if not constraint.is_feasible(parameters):\n            value = constraint.evaluate(parameters)\n            if constraint.constraint_type == \"equality\":\n                errors.append(f\"Linear constraint {i}: {value:.4f} != {constraint.upper_bound:.4f}\")\n            else:\n                if constraint.lower_bound and value &lt; constraint.lower_bound:\n                    errors.append(f\"Linear constraint {i}: {value:.4f} &lt; {constraint.lower_bound:.4f}\")\n                if constraint.upper_bound and value &gt; constraint.upper_bound:\n                    errors.append(f\"Linear constraint {i}: {value:.4f} &gt; {constraint.upper_bound:.4f}\")\n\n    # Check nonlinear constraints\n    for constraint in self.nonlinear_constraints:\n        if not constraint.is_feasible(parameters):\n            value = constraint.evaluate(parameters)\n            errors.append(\n                f\"Nonlinear constraint '{constraint.name}': g(x) = {value:.4f} violates \"\n                f\"{constraint.constraint_type} constraint\"\n            )\n\n    return len(errors) == 0, errors\n</code></pre>"},{"location":"examples/","title":"Beispiele","text":"<p>Praktische Beispiele zur Verwendung der PyADM1ODE-Kalibrierung.</p>"},{"location":"examples/#workflow-beispiele","title":"Workflow-Beispiele","text":"<ul> <li>Kalibrierungs-Workflow: Schritt-f\u00fcr-Schritt-Kalibrierung eines Biogasanlagenmodells.</li> </ul>"},{"location":"examples/calibration_workflow/","title":"Kalibrierungs-Workflow","text":"<p>Dieser Workflow zeigt die schrittweise Kalibrierung eines Biogasanlagenmodells.</p>"},{"location":"examples/calibration_workflow/#schritte","title":"Schritte","text":"<ol> <li>Daten laden</li> <li>Vorverarbeitung</li> <li>Kalibrierung ausf\u00fchren</li> <li>Ergebnisse validieren</li> </ol>"},{"location":"user_guide/","title":"Benutzerhandbuch","text":"<p>Willkommen beim Benutzerhandbuch f\u00fcr die PyADM1ODE-Kalibrierung.</p>"},{"location":"user_guide/#themen","title":"Themen","text":"<ul> <li>Modellkalibrierung: Umfassendes Handbuch zur Parameterkalibrierung.</li> </ul>"},{"location":"user_guide/calibration/","title":"Handbuch zur Modellkalibrierung","text":"<p>Dieses Handbuch behandelt die Parameterkalibrierung in PyADM1ODE, einschlie\u00dflich der Erstkalibrierung aus historischen Daten und der Online-Rekalibrierung w\u00e4hrend des Anlagenbetriebs.</p>"},{"location":"user_guide/calibration/#ubersicht","title":"\u00dcbersicht","text":"<p>Die Modellkalibrierung ist f\u00fcr eine genaue Biogasanlagensimulation unerl\u00e4sslich. PyADM1ODE bietet:</p> <ul> <li>Erstkalibrierung: Batch-Optimierung aus historischen Messdaten</li> <li>Online-Rekalibrierung: Adaptive Parameteranpassung w\u00e4hrend des Betriebs</li> <li>Sensitivit\u00e4tsanalyse: Identifizierung einflussreicher Parameter</li> <li>Identifizierbarkeitsbewertung: Erkennung von \u00dcberparametrisierung</li> <li>Validierungswerkzeuge: G\u00fctekriterien und Residualanalyse</li> </ul>"},{"location":"en/","title":"PyADM1ODE_calibration","text":"<p>Note: This is a calibration-focused package that only works together with PyADM1ODE.</p> <p>Advanced parameter calibration framework for PyADM1ODE biogas plant models</p> <p>Automated calibration and re-calibration of Anaerobic Digestion Model No. 1 (ADM1) parameters using real plant measurement data with multiple optimization algorithms, comprehensive validation, and online adaptation capabilities.</p>"},{"location":"en/#overview","title":"Overview","text":"<p>PyADM1ODE_calibration provides a complete calibration framework for PyADM1ODE biogas plant models:</p> <ul> <li>Initial Calibration: Batch optimization from historical measurement data</li> <li>Online Re-Calibration: Real-time parameter adjustment during plant operation</li> <li>Multiple Optimization Algorithms: Differential Evolution, Nelder-Mead, L-BFGS-B, Particle Swarm</li> <li>Multi-Objective Optimization: Balance multiple outputs (CH\u2084, pH, VFA) with weighted objectives</li> <li>Comprehensive Validation: Goodness-of-fit metrics, residual analysis, cross-validation</li> <li>Parameter Identifiability: Sensitivity analysis and correlation detection</li> <li>Data Management: CSV/database import, validation, outlier detection, gap filling</li> </ul>"},{"location":"en/examples/","title":"Examples","text":"<p>Practical examples of using PyADM1ODE Calibration.</p>"},{"location":"en/examples/#workflow-examples","title":"Workflow Examples","text":"<ul> <li>Calibration Workflow: Step-by-step calibration of a biogas plant model.</li> </ul>"},{"location":"en/examples/calibration_workflow/","title":"TODO: Generate this file","text":"<ul> <li>Complete step-by-step guide</li> <li>Real data example</li> <li>Troubleshooting tips</li> </ul>"},{"location":"en/user_guide/","title":"User Guide","text":"<p>Welcome to the PyADM1ODE Calibration user guide.</p>"},{"location":"en/user_guide/#topics","title":"Topics","text":"<ul> <li>Model Calibration: Comprehensive guide to parameter calibration.</li> </ul>"},{"location":"en/user_guide/calibration/","title":"Model Calibration Guide","text":"<p>This guide covers parameter calibration in PyADM1ODE, including initial calibration from historical data and online re-calibration during plant operation.</p>"},{"location":"en/user_guide/calibration/#overview","title":"Overview","text":"<p>Model calibration is essential for accurate biogas plant simulation. PyADM1ODE provides:</p> <ul> <li>Initial Calibration: Batch optimization from historical measurement data</li> <li>Online Re-calibration: Adaptive parameter adjustment during operation</li> <li>Sensitivity Analysis: Identification of influential parameters</li> <li>Identifiability Assessment: Detection of over-parameterization</li> <li>Validation Tools: Goodness-of-fit metrics and residual analysis</li> </ul>"},{"location":"en/user_guide/calibration/#quick-start","title":"Quick Start","text":"<pre><code>from pyadm1.calibration import Calibrator\nfrom pyadm1.io import MeasurementData\nfrom pyadm1.configurator import BiogasPlant\nfrom pyadm1.substrates import Feedstock\n\n# Load plant and measurements\nfeedstock = Feedstock(feeding_freq=48)\nplant = BiogasPlant.from_json(\"plant.json\", feedstock)\nmeasurements = MeasurementData.from_csv(\"plant_data.csv\")\n\n# Create calibrator\ncalibrator = Calibrator(plant)\n\n# Calibrate parameters\nresult = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"k_hyd_ch\", \"Y_su\"],\n    objectives=[\"Q_ch4\", \"pH\"],\n    weights={\"Q_ch4\": 0.8, \"pH\": 0.2}\n)\n\n# Apply calibrated parameters\nif result.success:\n    calibrator.apply_calibration(result)\n</code></pre>"},{"location":"en/user_guide/calibration/#initial-calibration","title":"Initial Calibration","text":""},{"location":"en/user_guide/calibration/#when-to-use-initial-calibration","title":"When to Use Initial Calibration","text":"<p>Use initial calibration when you have: - Historical measurement data (\u22652 weeks recommended) - Stable plant operation during measurement period - Reliable measurements of key outputs (gas production, pH, VFA) - Known substrate feed rates and composition</p>"},{"location":"en/user_guide/calibration/#parameter-selection","title":"Parameter Selection","text":"<p>Choose parameters based on:</p> <p>High Priority (most sensitive to calibration): - <code>k_dis</code>: Disintegration rate [1/d] - <code>k_hyd_ch</code>, <code>k_hyd_pr</code>, <code>k_hyd_li</code>: Hydrolysis rates [1/d] - <code>Y_su</code>, <code>Y_aa</code>: Yield coefficients [kg COD/kg COD]</p> <p>Medium Priority: - <code>k_m_c4</code>, <code>k_m_pro</code>, <code>k_m_ac</code>, <code>k_m_h2</code>: Maximum uptake rates [1/d] - <code>K_S_su</code>, <code>K_S_aa</code>: Half-saturation constants [kg COD/m\u00b3]</p> <p>Low Priority (usually well-defined): - Stoichiometric coefficients (C, N content) - Physical-chemical constants (K_a, K_H)</p> <p>TODO: add publications that confirm which parameters have which priority</p>"},{"location":"en/user_guide/calibration/#example-single-parameter-calibration","title":"Example: Single-Parameter Calibration","text":"<pre><code># Calibrate disintegration rate only\nresult = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\"],\n    bounds={\"k_dis\": (0.3, 0.8)},\n    objectives=[\"Q_ch4\"],\n    max_iterations=50\n)\n\nprint(f\"Optimal k_dis: {result.parameters['k_dis']:.3f}\")\nprint(f\"Objective value: {result.objective_value:.4f}\")\n</code></pre>"},{"location":"en/user_guide/calibration/#example-multi-parameter-calibration","title":"Example: Multi-Parameter Calibration","text":"<pre><code># Calibrate multiple substrate-dependent parameters\nresult = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"k_hyd_ch\", \"k_hyd_pr\", \"Y_su\"],\n    bounds={\n        \"k_dis\": (0.3, 0.8),\n        \"k_hyd_ch\": (5.0, 15.0),\n        \"k_hyd_pr\": (5.0, 15.0),\n        \"Y_su\": (0.05, 0.15)\n    },\n    objectives=[\"Q_ch4\", \"pH\", \"VFA\"],\n    weights={\"Q_ch4\": 0.6, \"pH\": 0.2, \"VFA\": 0.2},\n    method=\"differential_evolution\",\n    validation_split=0.2,\n    max_iterations=100,\n    population_size=15\n)\n</code></pre>"},{"location":"en/user_guide/calibration/#optimization-methods","title":"Optimization Methods","text":""},{"location":"en/user_guide/calibration/#differential-evolution-default-recommended","title":"Differential Evolution (Default, Recommended)","text":"<p>Best for: Global optimization, multiple local minima</p> <pre><code>result = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"Y_su\"],\n    method=\"differential_evolution\",\n    max_iterations=100,\n    population_size=15,  # 15 \u00d7 n_parameters recommended\n    tolerance=1e-4\n)\n</code></pre> <p>Advantages: - Global optimization (avoids local minima) - Robust to parameter scaling - No gradient calculation needed</p> <p>Disadvantages: - Slower than local methods - Requires more function evaluations</p> <p>% TODO: add publication to DE</p>"},{"location":"en/user_guide/calibration/#nelder-mead-local","title":"Nelder-Mead (Local)","text":"<p>Best for: Fast refinement near known optimum</p> <pre><code>result = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"Y_su\"],\n    method=\"nelder_mead\",\n    max_iterations=50,\n    tolerance=1e-4\n)\n</code></pre> <p>Advantages: - Fast convergence - Simple implementation - No gradient required</p> <p>Disadvantages: - Local optimization only - May get stuck in local minima - Sensitive to initial guess</p>"},{"location":"en/user_guide/calibration/#l-bfgs-b-gradient-based","title":"L-BFGS-B (Gradient-Based)","text":"<p>Best for: When gradients are available, large-scale problems</p> <pre><code>result = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"Y_su\", \"k_hyd_ch\"],\n    method=\"lbfgsb\",\n    max_iterations=50,\n    tolerance=1e-6\n)\n</code></pre> <p>Advantages: - Very fast convergence - Handles box constraints - Memory efficient</p> <p>Disadvantages: - Local optimization - Requires numerical gradients (slower in practice)</p> <p>% TODO: add publication to L-BFGS-B</p>"},{"location":"en/user_guide/calibration/#multi-objective-calibration","title":"Multi-Objective Calibration","text":"<p>Balance multiple objectives with weighted sum:</p> <pre><code>result = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"Y_su\", \"k_hyd_ch\"],\n    objectives=[\"Q_ch4\", \"pH\", \"VFA\"],\n    weights={\n        \"Q_ch4\": 0.6,  # Most important\n        \"pH\": 0.2,     # Secondary\n        \"VFA\": 0.2     # Secondary\n    },\n    method=\"differential_evolution\"\n)\n</code></pre> <p>Weight Selection Guidelines: - Sum to 1.0 (automatically normalized) - Higher weight = more influence on optimization - Gas production typically most important (0.6-0.8) - Process stability (pH, VFA) secondary (0.2-0.4)</p>"},{"location":"en/user_guide/calibration/#validation-split","title":"Validation Split","text":"<p>Reserve data for validation:</p> <pre><code>result = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"Y_su\"],\n    validation_split=0.2,  # 20% for validation\n    objectives=[\"Q_ch4\", \"pH\"]\n)\n\n# Check validation metrics\nprint(f\"Training RMSE: {result.objective_value:.2f}\")\nprint(f\"Validation R\u00b2: {result.validation_metrics['Q_ch4_r2']:.3f}\")\n</code></pre> <p>Recommendations: - Use 20-30% for validation - Avoid over-fitting with small datasets - Check validation metrics match training metrics</p>"},{"location":"en/user_guide/calibration/#sensitivity-analysis","title":"Sensitivity Analysis","text":"<p>Identify influential parameters:</p> <pre><code># After calibration\nsensitivity = calibrator.sensitivity_analysis(\n    parameters=result.parameters,\n    measurements=measurements,\n    objectives=[\"Q_ch4\", \"pH\", \"VFA\"],\n    perturbation=0.01  # 1% parameter perturbation\n)\n\n# Analyze results\nfor param, sens_result in sensitivity.items():\n    print(f\"\\n{param}:\")\n    print(f\"  Base value: {sens_result.base_value:.4f}\")\n    print(\"  Sensitivity indices:\")\n    for obj, index in sens_result.sensitivity_indices.items():\n        print(f\"    {obj}: {index:+.4e}\")\n</code></pre> <p>Interpretation: - High sensitivity (|S| &gt; 0.1): Parameter strongly affects output - Low sensitivity (|S| &lt; 0.01): Parameter has minimal effect - Negative sensitivity: Inverse relationship</p>"},{"location":"en/user_guide/calibration/#identifiability-analysis","title":"Identifiability Analysis","text":"<p>Check if parameters can be reliably estimated:</p> <pre><code>identifiability = calibrator.identifiability_analysis(\n    parameters=result.parameters,\n    measurements=measurements,\n    confidence_level=0.95,\n    correlation_threshold=0.8\n)\n\n# Review identifiability\nfor param, ident in identifiability.items():\n    if not ident.is_identifiable:\n        print(f\"\u26a0 {param}: {ident.reason}\")\n\n        # Check correlations\n        for other, corr in ident.correlation_with.items():\n            if abs(corr) &gt; 0.8:\n                print(f\"  High correlation with {other}: {corr:.3f}\")\n</code></pre> <p>Common Issues:</p> <ol> <li>Low Sensitivity: Parameter doesn't affect measured outputs</li> <li> <p>Solution: Remove from calibration or get better measurements</p> </li> <li> <p>High Correlation: Two parameters compensate for each other</p> </li> <li> <p>Solution: Fix one parameter or add discriminating measurements</p> </li> <li> <p>Wide Confidence Interval: Insufficient data to estimate parameter</p> </li> <li>Solution: Collect more data or use prior knowledge</li> </ol>"},{"location":"en/user_guide/calibration/#online-re-calibration","title":"Online Re-Calibration","text":"<p>Adapt parameters during plant operation:</p> <pre><code># Configure trigger conditions\ncalibrator.set_trigger(\n    variance_threshold=0.15,      # Trigger when variance &gt; 15%\n    time_threshold=24.0,          # Minimum 24h between calibrations\n    consecutive_violations=3      # Require 3 consecutive violations\n)\n\n# Check if re-calibration needed\nshould_cal, reason = calibrator.should_recalibrate(recent_data)\n\nif should_cal:\n    print(f\"Re-calibration triggered: {reason}\")\n\n    # Re-calibrate with bounded changes\n    result = calibrator.calibrate_online(\n        measurements=recent_data,\n        parameters=[\"k_dis\", \"Y_su\"],\n        max_parameter_change=0.10,  # Max 10% change\n        time_window=7,               # Use last 7 days\n        method=\"nelder_mead\"         # Fast local optimization\n    )\n\n    if result.success:\n        calibrator.apply_calibration(result)\n</code></pre>"},{"location":"en/user_guide/calibration/#when-to-use-online-re-calibration","title":"When to Use Online Re-Calibration","text":"<p>Appropriate for: - Long-term operation (months) - Gradual substrate property changes - Seasonal variations - Equipment drift</p> <p>Not appropriate for: - Short-term disturbances (handle with control) - Sensor failures (fix measurements first) - Major process changes (use initial calibration)</p>"},{"location":"en/user_guide/calibration/#parameter-change-limits","title":"Parameter Change Limits","text":"<p>Prevent unrealistic drift:</p> <pre><code>result = calibrator.calibrate_online(\n    measurements=recent_data,\n    parameters=[\"k_dis\", \"Y_su\"],\n    max_parameter_change=0.10,  # \u00b110% maximum\n    time_window=7\n)\n\n# Check parameter changes\nfor param, new_val in result.parameters.items():\n    old_val = result.initial_parameters[param]\n    change_pct = ((new_val - old_val) / old_val) * 100\n    print(f\"{param}: {old_val:.3f} \u2192 {new_val:.3f} ({change_pct:+.1f}%)\")\n</code></pre> <p>Recommended Limits: - Conservative: 5-10% (most common) - Moderate: 10-20% (seasonal changes) - Aggressive: 20-30% (major changes, use carefully)</p>"},{"location":"en/user_guide/calibration/#validation-and-quality-checks","title":"Validation and Quality Checks","text":""},{"location":"en/user_guide/calibration/#goodness-of-fit-metrics","title":"Goodness-of-Fit Metrics","text":"<pre><code>from pyadm1.calibration.validation import CalibrationValidator\n\nvalidator = CalibrationValidator(plant)\n\nmetrics = validator.validate(\n    parameters=result.parameters,\n    measurements=validation_data,\n    objectives=[\"Q_ch4\", \"pH\", \"VFA\"]\n)\n\n# Review metrics\nfor obj, obj_metrics in metrics.items():\n    print(f\"\\n{obj}:\")\n    print(f\"  RMSE: {obj_metrics.rmse:.2f}\")\n    print(f\"  R\u00b2:   {obj_metrics.r2:.3f}\")\n    print(f\"  NSE:  {obj_metrics.nse:.3f}\")\n    print(f\"  PBIAS: {obj_metrics.pbias:.1f}%\")\n</code></pre> <p>Metric Interpretation:</p> Metric Excellent Good Fair Poor R\u00b2 &gt; 0.90 0.75-0.90 0.50-0.75 &lt; 0.50 NSE (Nash-Sutcliffe Efficiency) &gt; 0.90 0.70-0.90 0.50-0.70 &lt; 0.50 PBIAS (Percent Bias) &lt; \u00b15% \u00b15-\u00b110% \u00b110-\u00b125% &gt; \u00b125%"},{"location":"en/user_guide/calibration/#residual-analysis","title":"Residual Analysis","text":"<pre><code># Analyze residuals\nresiduals = validator.analyze_residuals(\n    measurements=validation_data,\n    simulated=simulated_outputs,\n    objectives=[\"Q_ch4\", \"pH\"]\n)\n\nfor obj, res_analysis in residuals.items():\n    print(f\"\\n{obj}:\")\n\n    # Check normality\n    if not res_analysis.is_normally_distributed():\n        print(\"  \u26a0 Residuals not normally distributed\")\n\n    # Check autocorrelation\n    if res_analysis.has_autocorrelation():\n        print(f\"  \u26a0 Significant autocorrelation: {res_analysis.autocorrelation:.3f}\")\n\n    # Check heteroscedasticity\n    if res_analysis.has_heteroscedasticity():\n        print(\"  \u26a0 Heteroscedasticity detected\")\n\n    # Report outliers\n    n_outliers = len(res_analysis.outlier_indices)\n    if n_outliers &gt; 0:\n        pct = n_outliers / len(res_analysis.residuals) * 100\n        print(f\"  \u26a0 {n_outliers} outliers ({pct:.1f}%)\")\n</code></pre>"},{"location":"en/user_guide/calibration/#cross-validation","title":"Cross-Validation","text":"<p>Test generalization with k-fold cross-validation:</p> <pre><code>cv_results = validator.cross_validate(\n    parameters=result.parameters,\n    measurements=full_dataset,\n    n_folds=5,\n    objectives=[\"Q_ch4\", \"pH\"]\n)\n\n# Calculate mean metrics\nfor obj in [\"Q_ch4\", \"pH\"]:\n    r2_values = [m.r2 for m in cv_results[obj]]\n    rmse_values = [m.rmse for m in cv_results[obj]]\n\n    print(f\"\\n{obj} (5-fold CV):\")\n    print(f\"  R\u00b2:   {np.mean(r2_values):.3f} \u00b1 {np.std(r2_values):.3f}\")\n    print(f\"  RMSE: {np.mean(rmse_values):.2f} \u00b1 {np.std(rmse_values):.2f}\")\n</code></pre>"},{"location":"en/user_guide/calibration/#best-practices","title":"Best Practices","text":""},{"location":"en/user_guide/calibration/#1-measurement-data-quality","title":"1. Measurement Data Quality","text":"<p>Essential Requirements: - \u2713 Hourly or daily measurements - \u2713 Minimum 2 weeks of data - \u2713 Stable operation period - \u2713 Accurate substrate feed records - \u2713 &lt; 10% missing data</p> <p>Data Preparation: <pre><code># Load and validate\nmeasurements = MeasurementData.from_csv(\"plant_data.csv\")\nvalidation = measurements.validate()\n\nif validation.quality_score &lt; 0.7:\n    print(\"\u26a0 Data quality issues - see report\")\n    validation.print_report()\n\n# Clean data\nmeasurements.remove_outliers(method=\"zscore\", threshold=3.0)\nmeasurements.fill_gaps(method=\"interpolate\", limit=3)\n</code></pre></p>"},{"location":"en/user_guide/calibration/#2-parameter-selection-strategy","title":"2. Parameter Selection Strategy","text":"<p>Start Small: 1. Begin with 1-2 most sensitive parameters 2. Add parameters incrementally 3. Check identifiability at each step 4. Limit to 3-5 parameters for typical datasets</p> <p>Example Progression: <pre><code># Step 1: Most sensitive parameter\nresult_1 = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\"],\n    objectives=[\"Q_ch4\"]\n)\n\n# Step 2: Add hydrolysis rates\nresult_2 = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"k_hyd_ch\"],\n    objectives=[\"Q_ch4\", \"pH\"]\n)\n\n# Step 3: Add yield coefficient\nresult_3 = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"k_hyd_ch\", \"Y_su\"],\n    objectives=[\"Q_ch4\", \"pH\", \"VFA\"]\n)\n</code></pre></p>"},{"location":"en/user_guide/calibration/#3-computational-efficiency","title":"3. Computational Efficiency","text":"<p>Quick Calibration (&lt; 5 min): <pre><code>result = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"Y_su\"],\n    method=\"nelder_mead\",\n    max_iterations=50\n)\n</code></pre></p> <p>Thorough Calibration (10-30 min): <pre><code>result = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"k_hyd_ch\", \"Y_su\"],\n    method=\"differential_evolution\",\n    max_iterations=100,\n    population_size=15,\n    sensitivity_analysis=True\n)\n</code></pre></p> <p>% TODO: actually measure the time needed for calibration</p>"},{"location":"en/user_guide/calibration/#4-result-interpretation","title":"4. Result Interpretation","text":"<p>Check Success Indicators: <pre><code>if result.success:\n    # 1. Reasonable objective value\n    if result.objective_value &lt; 1.0:  # Depends on scale\n        print(\"\u2713 Good fit achieved\")\n\n    # 2. Parameters within expected ranges\n    for param, value in result.parameters.items():\n        bounds = calibrator.parameter_bounds.get_bounds(param)\n        if bounds.lower &lt; value &lt; bounds.upper:\n            print(f\"\u2713 {param} within bounds\")\n\n    # 3. Validation metrics acceptable\n    if result.validation_metrics.get('Q_ch4_r2', 0) &gt; 0.75:\n        print(\"\u2713 Good validation performance\")\n</code></pre></p>"},{"location":"en/user_guide/calibration/#5-documentation","title":"5. Documentation","text":"<p>Keep calibration records: <pre><code># Save complete calibration result\nresult.to_json(\"calibration_result.json\")\n\n# Generate report\nreport = calibrator.generate_report(\"calibration_report.txt\")\n\n# Track calibration history\ncalibrator.save_history(\"calibration_history.json\")\n</code></pre></p>"},{"location":"en/user_guide/calibration/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"en/user_guide/calibration/#issue-1-calibration-fails-to-converge","title":"Issue 1: Calibration Fails to Converge","text":"<p>Symptoms: - <code>result.success = False</code> - High objective value - Maximum iterations reached</p> <p>Solutions: 1. Reduce number of parameters 2. Tighten parameter bounds 3. Use different initial guess 4. Increase max iterations 5. Try different optimization method</p> <pre><code># Solution: Tighter bounds and more iterations\nresult = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\"],  # Reduce to one parameter\n    bounds={\"k_dis\": (0.4, 0.6)},  # Tighter bounds\n    max_iterations=200,  # More iterations\n    method=\"differential_evolution\"\n)\n</code></pre>"},{"location":"en/user_guide/calibration/#issue-2-parameters-not-identifiable","title":"Issue 2: Parameters Not Identifiable","text":"<p>Symptoms: - High parameter correlation (&gt; 0.8) - Wide confidence intervals - Low sensitivity</p> <p>Solutions: 1. Remove highly correlated parameters 2. Collect more discriminating measurements 3. Fix one of the correlated parameters</p> <pre><code># Check correlations\nidentifiability = calibrator.identifiability_analysis(\n    parameters=result.parameters,\n    measurements=measurements\n)\n\n# Remove problematic parameter\nif not identifiability[\"Y_su\"].is_identifiable:\n    # Fix Y_su, calibrate only k_dis\n    result = calibrator.calibrate_initial(\n        measurements=measurements,\n        parameters=[\"k_dis\"],  # Remove Y_su\n        objectives=[\"Q_ch4\"]\n    )\n</code></pre>"},{"location":"en/user_guide/calibration/#issue-3-poor-validation-performance","title":"Issue 3: Poor Validation Performance","text":"<p>Symptoms: - Low validation R\u00b2 (&lt; 0.5) - High PBIAS (&gt; 25%) - Large validation/training gap</p> <p>Solutions: 1. Check for over-fitting (too many parameters) 2. Verify data quality 3. Ensure validation period is representative</p> <pre><code># Solution: Reduce parameters\nresult = calibrator.calibrate_initial(\n    measurements=measurements,\n    parameters=[\"k_dis\", \"k_hyd_ch\"],  # Reduced from 5 parameters\n    validation_split=0.3,  # More validation data\n    objectives=[\"Q_ch4\", \"pH\"]\n)\n\n# Check validation\nif result.validation_metrics['Q_ch4_r2'] &lt; 0.5:\n    print(\"\u26a0 Still poor - check data quality\")\n    validation = measurements.validate()\n    validation.print_report()\n</code></pre>"},{"location":"en/user_guide/calibration/#issue-4-unrealistic-parameter-values","title":"Issue 4: Unrealistic Parameter Values","text":"<p>Symptoms: - Parameters at bounds - Physically impossible values</p> <p>Solutions: 1. Check parameter bounds (too wide?) 2. Verify measurement data quality 3. Check model structure (missing processes?)</p> <pre><code># Check if parameters hit bounds\nfor param, value in result.parameters.items():\n    bounds = calibrator.parameter_bounds.get_bounds(param)\n\n    if value &lt;= bounds.lower + 0.01:\n        print(f\"\u26a0 {param} at lower bound: {value:.3f}\")\n    elif value &gt;= bounds.upper - 0.01:\n        print(f\"\u26a0 {param} at upper bound: {value:.3f}\")\n</code></pre>"},{"location":"en/user_guide/calibration/#example-workflows","title":"Example Workflows","text":""},{"location":"en/user_guide/calibration/#complete-initial-calibration","title":"Complete Initial Calibration","text":"<p>See <code>scripts/calibration_example.py</code> for a full working example with: - Plant setup - Measurement data preparation - Parameter calibration - Sensitivity analysis - Identifiability assessment - Validation - Application to plant model</p>"},{"location":"en/user_guide/calibration/#automated-online-monitoring","title":"Automated Online Monitoring","text":"<pre><code>import time\nfrom datetime import datetime\n\n# Setup\nplant = BiogasPlant.from_json(\"plant.json\", feedstock)\ncalibrator = Calibrator(plant)\n\n# Configure trigger\ncalibrator.set_trigger(\n    variance_threshold=0.15,\n    time_threshold=24.0,\n    consecutive_violations=3\n)\n\n# Monitoring loop (simplified)\nwhile True:\n    # Get recent measurements (last 7 days)\n    recent_data = get_recent_measurements(days=7)\n\n    # Check if re-calibration needed\n    should_cal, reason = calibrator.should_recalibrate(recent_data)\n\n    if should_cal:\n        print(f\"{datetime.now()}: Re-calibration triggered - {reason}\")\n\n        # Re-calibrate\n        result = calibrator.calibrate_online(\n            measurements=recent_data,\n            max_parameter_change=0.10,\n            time_window=7\n        )\n\n        if result.success:\n            calibrator.apply_calibration(result)\n            print(\"\u2713 Re-calibration applied\")\n        else:\n            print(f\"\u2717 Re-calibration failed: {result.message}\")\n\n    # Wait 1 hour\n    time.sleep(3600)\n</code></pre>"},{"location":"en/user_guide/calibration/#next-steps","title":"Next Steps","text":"<ul> <li>Components Guide: Learn about model components</li> <li>Parallel Simulation: Run parameter sweeps</li> <li>API Reference: Detailed API documentation</li> </ul>"},{"location":"en/user_guide/calibration/#references","title":"References","text":"<ol> <li>Batstone et al. (2002): Anaerobic Digestion Model No. 1 (ADM1). IWA Publishing.</li> <li>Gaida (2014): Dynamic real-time substrate feed optimization of anaerobic co-digestion plants. PhD thesis.</li> <li>Dochain &amp; Vanrolleghem (2001): Dynamical Modelling &amp; Estimation in Wastewater Treatment Processes.</li> </ol>"}]}